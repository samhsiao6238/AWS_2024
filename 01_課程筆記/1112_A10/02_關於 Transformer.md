# Transformer

_常被應用在 NLP 中的深度學習架構，相較於 `RNN` 和 `LSTM` 架構，`Transformer` 除擁有 `並行計算` 能力，最重要的是在捕捉 `長距離關係` 的處理上_

<br>

## 白話介紹

1. 相較於傳統的 RNN 或是 LSTM，Transformer 不依賴於數據序列關係，改以位置資訊為每個數據點產生多頭關係，計算本身與其他數據點間的向量，從而建立一個更縱觀的 NLP 模型，使數據不受距離影響，對於長距離關係的捕捉有相當補強效果；特別注意，這種拓展模型觀察範圍的模式與模型降維展開不同。

<br>

2. 以上這樣的模式就是 `自注意力機制（self-attention）`，這種多頭關係就是 `多頭注意力（multi-head attention）`

<br>

## 核心技術

_自注意力機制（Self-Attention Mechanism）_

<br>

1. 自注意力機制允許模型在處理序列的過程中，針對序列中每個詞，也就是每一個 `Token`  計算該詞與序列中所有其他詞的關聯性或`注意力權重`。

<br>

2. 如此將在模型中導入多組不同的權重組合，使模型可以捕捉到序列中的多層次關聯性。

<br>

3. 計算實務上，自注意力機制會根據每個詞的位置編碼及特徵，計算其與序列中其他詞的 `相似度` 或 `影響力`，生成 `注意力矩陣`。

<br>

## 架構要素

1. Transformer 包含多層的編碼器和解碼器（Encoder-Decoder），其中 `編碼器` 負責處理輸入序列的特徵表示，`解碼器` 負責根據輸入特徵生成 `輸出序列`。

<br>

2. 由於 Transformer _不具備序列順序傳遞的特性_，位置編碼為每個詞增加了 `位置信息`，使模型能識別各詞的相對位置。

<br>

## 差異性

_RNN 和 LSTM 因距離而產生的長期依賴衰減問題_

<br>

1. Transformer 與 RNN/LSTM 的對比。

    ![](images/img_01.png)

<br>

## 距離敏感性和位置編碼

1. `位置編碼` 賦予模型對序列中各詞的相對位置信息，使模型對詞之間的 `距離` 敏感，這種距離敏感性不是基於序列順序，而是由 `位置` 編碼加權的計算得出。

<br>

2. 在自注意力中，Transformer 的 `注意力矩陣` 基於每個詞的特徵和位置，計算每個詞對其他詞的影響力或關聯性。

<br>

___

_END_