{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 3.4 - 学员笔记本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "本实验是模块 3 引导式实验的延续。\n",
    "\n",
    "在本实验中，您会将数据拆分为三个独立的数据集：\n",
    "\n",
    "- *训练集* - 将用于训练模型。\n",
    "- *验证集* - 训练期间将使用它来验证模型。\n",
    "- *测试集* - 将保留该数据集，用于在训练模型后生成指标。您将在接下来的实验中使用此数据集。\n",
    "\n",
    "拆分数据后，您将使用 Amazon SageMaker 训练 XGBoost 模型。\n",
    "\n",
    "\n",
    "## 业务场景简介\n",
    "\n",
    "您在一家医疗保健服务提供商工作，并希望改善骨科患者的异常检测。\n",
    "\n",
    "您的任务是利用机器学习 (ML) 解决此问题。您可以使用包含六个生物力学特征且目标为*正常*或*异常*的数据集。您可以使用此数据集训练 ML 模型，以预测患者是否会出现异常。\n",
    "\n",
    "\n",
    "## 关于该数据集\n",
    "\n",
    "该生物医学数据集由 Henrique da Mota 博士在法国里昂 Médico-Chirurgical de Réadaptation des Massues 中心的整形外科应用研究组 (GARO) 实习期间创建。这些数据分到两个不同但相关的分类任务中。\n",
    "\n",
    "第一项任务是将患者归类为以下三类之一： \n",
    "\n",
    "- *正常*（100 名患者）\n",
    "- *椎间盘疝*（60 名患者）\n",
    "- *脊椎滑脱*（150 名患者）\n",
    "\n",
    "对于第二个任务，则是将*椎间盘疝*和*脊椎滑脱*合并为一个类别，标记为*异常*。因此，在第二个任务中，患者属于以下两个类别之一：*正常*（100 名患者）或*异常*（210 名患者）。\n",
    "\n",
    "\n",
    "## 属性信息：\n",
    "\n",
    "数据集中的每名患者都由六个生物力学属性表示，这些属性（顺序如下）是根据骨盆和腰椎的形状和方向得出的： \n",
    "\n",
    "- 骨盆入射角\n",
    "- 骨盆倾斜角\n",
    "- 腰椎前凸角\n",
    "- 骶骨倾斜角\n",
    "- 骨盆半径\n",
    "- 脊椎滑脱等级\n",
    "\n",
    "以下约定用于分类标签： \n",
    "- 椎间盘疝 (DH)\n",
    "- 脊椎滑脱 (SL)\n",
    "- 正常 (NO) \n",
    "- 异常 (AB)\n",
    "\n",
    "\n",
    "有关此数据集的更多信息，请参阅[脊柱数据集网页](http://archive.ics.uci.edu/ml/datasets/Vertebral+Column)。\n",
    "\n",
    "\n",
    "## 数据集属性\n",
    "\n",
    "该数据集来自：\n",
    "Dua, D. 和 Graff, C.（2019 年）。UCI 机器学习存储库 (http://archive.ics.uci.edu/ml)。加州尔湾市：加利福尼亚大学信息与计算机科学学院。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验设置\n",
    "由于此解决方案分散在模块中的多个实验中，因此您需要执行以下单元格中的内容，以便加载数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据\n",
    "\n",
    "通过执行以下单元格中的内容，将导入数据并让数据可供使用。\n",
    "\n",
    "**注意**：以下单元格中的内容代表以前的实验中的关键步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, requests, zipfile, io\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_zip = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00212/vertebral_column_data.zip'\n",
    "r = requests.get(f_zip, stream=True)\n",
    "Vertebral_zip = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "Vertebral_zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arff.loadarff('column_2C_weka.arff')\n",
    "df = pd.DataFrame(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapper = {b'Abnormal':1,b'Normal':0}\n",
    "df['class']=df['class'].replace(class_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 1：探索数据\n",
    "您首先将快速了解数据集中的数据。\n",
    "\n",
    "为了充分利用本实验，在执行单元格中的内容之前，请仔细阅读说明和代码。花点时间进行实验！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，使用 **shape** 来检查行数和列数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，获取列的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle',\n",
       "       'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以看到六个生物力学特征，目标列名为 *class*。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2：准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本实验中，您需要将数据拆分为三个数据集。\n",
    "\n",
    "通过在互联网上搜索，您可以找到许多不同的数据集拆分方式。您找到的许多代码範例可能会将数据集分为*目标*和*特征*。然后，这两个数据集又分别拆分为三个子集，从而总共产生了六个数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移动目标列位置\n",
    "\n",
    "XGBoost 要求训练数据存放在单个文件中。该文件的目标值必须在第一列。\n",
    "\n",
    "获取目标列并将其移到第一个位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您应该可以看到 **class** 现在是第一列了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class', 'pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle',\n",
       "       'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拆分数据\n",
    "\n",
    "首先将数据集分为两个数据集。您将使用其中一个数据集进行训练，并将再次拆分另一个数据集以用于验证和测试。\n",
    "\n",
    "您将使用 *scikit-learn 库*中的 *train_test_split 函数*，该库是免费的 Python 机器学习库。它拥有许多算法和有用的函数，包括您将使用的这个。\n",
    "\n",
    "- 有关该函数的更多信息，请参阅 [Train_test_split 文档](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)。\n",
    " - 有关 scikit-learn 的更多信息，请参阅 [scikit-learn 指南](https://scikit-learn.org/stable/)\n",
    "\n",
    "因为数据量并不多，所以您需要确保拆分的数据集包含每个类的代表性数量。因此，您将使用 *stratify* 开关。最后，您将使用一个随机数，以便可以重复进行拆分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test_and_validate = train_test_split(df, test_size=0.2, random_state=42, stratify=df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，将 *test_and_validate* 数据集拆分成两个相等的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, validate = train_test_split(test_and_validate, test_size=0.5, random_state=42, stratify=test_and_validate['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查三个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248, 7)\n",
      "(31, 7)\n",
      "(31, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，检查类的分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "1    168\n",
      "0     80\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    21\n",
      "0    10\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "1    21\n",
      "0    10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['class'].value_counts())\n",
    "print(test['class'].value_counts())\n",
    "print(validate['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据上传到 Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost 将从 Amazon Simple Storage Service (Amazon S3) 加载用于训练的数据。因此，您必须将数据写入逗号分隔值 (CSV) 文件，然后将文件上传到 Amazon S3。\n",
    "\n",
    "首先为 S3 存储桶设置一些变量，然后创建一个将 CSV 文件上传到 Amazon S3 的函数。您可以重复使用该函数。\n",
    "\n",
    "首先浏览函数。\n",
    "\n",
    "请注意以下这行：\n",
    "\n",
    "`dataframe.to_csv(csv_buffer, header=False, index=False)`\n",
    "\n",
    "此行将 pandas DataFrame（已传递到函数中）写入名为 *csv_buffer* 的 I/O 缓冲区。通过使用缓冲区，就可以避免在本地写入文件。\n",
    "\n",
    "要阻止列标题写出，请使用 `header=False`。要阻止输出 pandas 索引，请使用 `index=False`。\n",
    "\n",
    "要将 csv_buffer 作为对象写入 Amazon S3，请对 `object`（`bucket` 的一个属性）使用 `put` 操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c130335a3301600l8062774t1w632558395917-labbucket-uq8nhb2ic15j'\n",
    "\n",
    "prefix='lab3'\n",
    "\n",
    "train_file='vertebral_train.csv'\n",
    "test_file='vertebral_test.csv'\n",
    "validate_file='vertebral_validate.csv'\n",
    "\n",
    "import os\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False)\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用您刚才创建的函数上传三个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 3：训练模型\n",
    "\n",
    "现在，数据已经存储到了 Amazon S3 中，您可以开始训练模型了。\n",
    "\n",
    "第一步是获取 XGBoost 容器 URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，您需要为模型设置一些*超参数*。因为这是您第一次训练模型，所以开始时可以使用一些值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"auc\",\n",
    "             \"objective\": \"binary:logistic\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 **estimator** 函数设置模型。以下是一些需要关注的参数：\n",
    "\n",
    "- **instance_count** - 定义将用于训练的实例数。您将使用*一个*实例。\n",
    "- **instance_type** - 定义用于训练的实例类型。这里使用的是 *ml.m4.xlarge*。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                       sagemaker.get_execution_role(),\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "估算器需要*通道*才能将数据输入模型。对于训练，将使用 *train_channel* 和 *validate_channel*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行 **fit** 将训练模型。\n",
    "\n",
    "**注意**：这一过程耗时最多 5 分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-22-12-30-11-475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-10-22 12:30:14 Starting - Starting the training job..\n",
      "2024-10-22 12:30:28 Starting - Preparing the instances for training....\n",
      "2024-10-22 12:30:54 Downloading - Downloading input data.....\n",
      "2024-10-22 12:31:24 Downloading - Downloading the training image.........\n",
      "2024-10-22 12:32:15 Training - Training image download completed. Training in progress...\n",
      "2024-10-22 12:32:31 Uploading - Uploading generated training model..\n",
      "2024-10-22 12:32:44 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，您就可以测试和评估模型了。但我们在之后的实验中才会讨论测试和验证部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 恭喜！\n",
    "\n",
    "您已经完成了本实验，现在可以按照实验指南中的说明结束本实验。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAWS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
