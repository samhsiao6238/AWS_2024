# Lab 6.1 官方範例

_這個 Lab 使用 `Comprehend` 實作 `主題建模`，數據是從數據集 [20 個新聞組] (https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups) 中提取主題。_

![](images/img_01.png)

<br>

## 關於該數據集

1. 20 個新聞組數據集是約 20000 個新聞組文件的集合，這些文件平均分佈在 20 個不同的新聞組中。

<br>

2. 此數據集在對機器學習技術（例如文本分類和文本聚類）的文本應用進行實驗時非常受歡迎。在本實驗中，您將了解如何使用神經主題模型 (NTM) 演算法從此組文檔中學到主題。

<br>

3. 數據集來源：Tom Mitchell，*20 Newsgroups Data*，1999 年 9 月 9 日，由 UCI KDD Archive 分發。https://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html。

<br>

## 步驟簡介

1. 導入需求並建立會話變量。

<br>

2. 導入新聞組文件。

<br>

3. 檢查與預處理數據。

<br>

4. 分析 Amazon Comprehend 任務輸出。

<br>

## 準備工作

1. 建立專案資料夾。

    ```bash
    mkdir -p ~/Desktop/_test_ && cd ~/Desktop/_test_
    touch .env .gitignore ex01.ipynb
    echo ".env" >> .gitignore
    code .
    ```

<br>

2. 在 `.env` 貼上 Lab 的 AWS CLI 不包含 `[default]` 部分內容。

<br>

3. 緊接著在下方貼上以下內容。

    ```bash
    AWS_ACCESS_KEY_ID=${aws_access_key_id}
    AWS_SECRET_ACCESS_KEY=${aws_secret_access_key}
    AWS_SESSION_TOKEN=${aws_session_token}
    AWS_DEFAULT_REGION=us-east-1
    ```

<br>

4. 在 `ex01.ipynb` 中選擇適當核心如 `envAWS`，貼上並運行以下代碼，確認設置是否正確。

    ```python
    import boto3

    # 載入 .env 檔案中的環境變數
    from dotenv import load_dotenv
    import os
    load_dotenv()

    # 建立 STS 客戶端
    sts_client = boto3.client("sts")

    # 呼叫 get_caller_identity 以取得當前 AWS 帳號資訊
    response = sts_client.get_caller_identity()
    account_id = response["Account"]

    print(f"AWS Account ID: {account_id}")
    ```

<br>

5. 這個 Lab 提供的 Account ID 是固定的。

    ![](images/img_02.png)

<br>

## 開始運行專案

_導入需求和建立變數_

<br>

1. 建立相關變量。

    ```python
    import boto3
    import uuid
    
    # 建立 comprehend 客戶端
    comprehend_client = boto3.client(service_name='comprehend')

    # S3 bucket 和輸入數據文件的常量
    bucket = 'c133864a3391500l8216998t1w753251772626-labbucket-m0jhfwhqvfik'
    data_access_role_arn = 'arn:aws:iam::753251772626:role/service-role/c133864a3391500l8216998t1w-ComprehendDataAccessRole-BwjcVJXuvgaz'
    ```

<br>

2. 導入數據集中的 `新聞組` 文件，定義存放數據的文件夾，並清理其中可能存在的舊數據；新聞組文件作為單獨的文件存儲在 `.tar` 文件中。

    ```python
    import os
    import shutil

    data_dir = '20_newsgroups'
    # 清理現有的數據文件夾
    if os.path.exists(data_dir):
        shutil.rmtree(data_dir)
    ```

<br>

3. 可使用 `魔法指令` 進行解壓縮數據集並提取文件列表。

    ```python
    !tar -xzf ../s3/20_newsgroups.tar.gz
    !ls 20_newsgroups
    ```

<br>

4. 或使用 Python 的代碼玩成。

    ```python
    import tarfile
    import os

    # 解壓縮 .tar.gz 文件
    with tarfile.open('../s3/20_newsgroups.tar.gz', 'r:gz') as tar:
        tar.extractall(path='20_newsgroups')  # 將文件解壓到指定資料夾

    # 列出解壓縮後的文件夾內容
    print("20_newsgroups 文件夾內容:")
    for folder_name, subfolders, filenames in os.walk('20_newsgroups'):
        for filename in filenames:
            print(f"- {os.path.join(folder_name, filename)}")
    ```

    _輸出_

    ```bash
    alt.atheism		  rec.autos	      sci.space
    comp.graphics		  rec.motorcycles     soc.religion.christian
    comp.os.ms-windows.misc   rec.sport.baseball  talk.politics.guns
    comp.sys.ibm.pc.hardware  rec.sport.hockey    talk.politics.mideast
    comp.sys.mac.hardware	  sci.crypt	      talk.politics.misc
    comp.windows.x		  sci.electronics     talk.religion.misc
    misc.forsale		  sci.med
    ```

<br>

5. 獲取指定目錄下所有子文件夾內的文件列表，並統計總文件數量。

    ```python
    folders = [
        os.path.join(data_dir, f) for f in sorted(os.listdir(data_dir)) if os.path.isdir(os.path.join(data_dir, f))
    ]
    file_list = [
        os.path.join(d, f) for d in folders for f in os.listdir(d)
    ]
    print('文件數量:', len(file_list))
    ```

    _輸出_

    ```bash
    文件數量: 19997
    ```

<br>

## 數據清洗

_檢查數據並執行一些標準的自然語言處理 (NLP) 數據清洗任務。_

<br>

1. 使用魔法指令顯示指定文件的內容。

    ```python
    !cat 20_newsgroups/comp.graphics/37917
    ```

<br>

2. 同樣地，可使用代碼完成。

    ```python
    file_path = "20_newsgroups/comp.graphics/37917"

    # 打開文件並讀取內容
    with open(file_path, "r") as file:
        content = file.read()

    # 輸出文件內容
    print(content)
    ```

    _輸出_

    ```bash
    Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!usc!elroy.jpl.nasa.gov!nntp-server.caltech.edu!andrey
    From: andrey@cco.caltech.edu (Andre T. Yew)
    Newsgroups: comp.graphics
    Subject: Re: 16 million vs 65 thousand colors
    Date: 3 Apr 1993 19:51:06 GMT
    Organization: California Institute of Technology, Pasadena
    Lines: 28
    Message-ID: <1pkpraINNck9@gap.caltech.edu>
    References: <1993Mar26.210323.27802@midway.uchicago.edu> <dotzlaw-020493084300@murphy.biochem.umanitoba.ca> <d9hh.733845825@dtek.chalmers.se>
    NNTP-Posting-Host: punisher.caltech.edu
    
    d9hh@dtek.chalmers.se (Henrik Harmsen) writes:
    
    >1-4 bits per R/G/B gives horrible machbanding visible in almost any picture.
    
    >5 bits per R/G/B (32768, 65000 colors) gives visible machbanding
    
    >color-gradient picture has _almost_ no machbanding. This color-resolution is 
    
    >see some small machbanding on the smooth color-gradient picture, but all in all,
    >There _ARE_ situiations where you get visible mach-banding even in
    >a 24 bit card. If
    >you create a very smooth color gradient of dark-green-white-yellow
    >or something and turn
    >up the contrast on the monitor, you will probably see some mach-banding.
    
        While I don't mean to damn Henrik's attempt to be helpful here,
    he's using a common misconception that should be corrected.
    
        Mach banding will occur for any image.  It is not the color
    quantization you see when you don't have enough bits.  It is the
    human eye's response to transitions or edges between intensities.
    The result is that colors near the transistion look brighter on
    the brighter side and darker on the darker side.
    
    --Andre
    
    -- 
                Andre Yew andrey@cco.caltech.edu (131.215.139.2)
    ```

<br>

3. 每個新聞組文檔都可能包含以下部分；特別注意，*消息* 就是這個 Lab 希望提取主題的正文。

    ```bash
    頁眉 - 包含標準的新聞組頁眉信息，應將其移除。

    引用文本 - 上一條消息的文本，通常有 ">" 或 "|" 前綴，並且有時以 `writes*`、`wrote`、`said` 或 `says` 開頭。

    消息 - 這就是任務希望從中提取主題的消息正文。

    頁腳 – 通常是以簽名結尾的消息。
    ```

<br>

4. 移除 `頁眉`、`引用文本` 和 `頁腳`。

    ```python
    import re
    def strip_newsgroup_header(text):
        _before, _blankline, after = text.partition('\n\n')
        return after

    _QUOTE_RE = re.compile(
        r'(writes in|writes:|wrote:|says:|said:'
        r'|^In article|^Quoted from|^\||^>)'
    )

    def strip_newsgroup_quoting(text):
        good_lines = [
            line for line in text.split('\n') if not _QUOTE_RE.search(line)
        ]
        return '\n'.join(good_lines)

    def strip_newsgroup_footer(text):
        lines = text.strip().split('\n')
        for line_num in range(len(lines) - 1, -1, -1):
            line = lines[line_num]
            if line.strip().strip('-') == '':
                break

        if line_num > 0:
            return '\n'.join(lines[:line_num])
        else:
            return text
    ```

<br>

5. 現在使用這些函數清洗數據，以移除頁眉、頁腳和引號。

    ```python
    data = []
    for f in file_list:
        with open(f, 'rb') as fin:
            content = fin.read().decode('latin1')   
            content = strip_newsgroup_header(content)
            content = strip_newsgroup_quoting(content)
            content = strip_newsgroup_footer(content)
            data.append(content)
    ```

<br>

6. 接著將所有新聞組文檔保存到單個文件中，每行包含一個文檔。

    ```python
    with open('comprehend_input.txt','w', encoding='UTF-8') as cf:
        for line in data:
            line = line.strip()
            line = re.sub('\n',' ',line)
            line = re.sub('\r',' ',line)
            cf.write(line+'\n')
    ```

<br>

## 上傳數據

_需要將數據上傳到 `S3 Bucket`，這樣 `Comprehend` 才能使用數據。_

<br>

1. 將數據上傳到 `S3`。

    ```python
    s3 = boto3.resource('s3')
    s3.Bucket(bucket).upload_file(
        'comprehend_input.txt',
        'comprehend/newsgroups'
    )
    ```

<br>

## 進行 Comprehend

1. 設定 Comprehend 主題檢測工作的相關參數。

    ```python
    # 主題數量，表示 Comprehend 要識別的主題數目
    number_of_topics = 20

    # S3 路徑
    input_s3_url = f"s3://{bucket}/comprehend"

    # 輸入文件的格式，每行作為單個文件進行處理
    input_doc_format = "ONE_DOC_PER_LINE"

    # 輸入數據的配置字典，包括 S3 路徑和文件格式
    input_data_config = {
        "S3Uri": input_s3_url,
        "InputFormat": input_doc_format
    }

    # 輸出文件的 S3 路徑，主題檢測結果將儲存在此路徑下
    output_s3_url = f"s3://{bucket}/outputfolder/"

    # 輸出數據的配置字典，指定 S3 儲存位置
    output_data_config = {
        "S3Uri": output_s3_url
    }

    # 生成工作的唯一識別 ID
    job_uuid = uuid.uuid1()

    # 加入 UUID 組合主題檢測工作名稱
    job_name = f"top-job-{job_uuid}"

    # 輸出輸入的 S3 路徑
    print(input_s3_url)
    ```

    _輸出_

    ```bash
    s3://c133864a3391500l8216998t1w753251772626-labbucket-m0jhfwhqvfik/comprehend
    ```

<br>

2. 開始 `Comprehend` 主題檢測任務；特別注意這個函數中的 `DataAccessRoleArn` 參數，必須設定 Lab 提供的角色來獲取權限，這在後續 Lab 中會提供角色 ARN，但不會再交代這個函數設定。

    ```python
    # 啟動主題檢測工作
    start_topics_detection_job_result = comprehend_client.start_topics_detection_job(
        # 指定要識別的主題數量
        NumberOfTopics=number_of_topics,
        # 提供輸入數據的配置
        InputDataConfig=input_data_config,
        # 設定工作名稱
        JobName=job_name,
        # 輸出數據的配置
        OutputDataConfig=output_data_config,
        # 設定資料訪問的角色 ARN，用於允許 Comprehend 存取 S3 上的數據
        DataAccessRoleArn=data_access_role_arn
    )
    ```

<br>

3. 獲取任務狀態並等待任務完成，這個過程相當耗時，在 Sagemaker 中需 `25–35` 分鐘的時間；本機尚未實測。

    ```python
    # 因為這段代碼會運行很久，透過代碼讓程式暫停指定的秒數以顯示進行狀態
    from time import sleep

    # 透過 JobId 確認目前的工作狀態
    job = comprehend_client.describe_topics_detection_job(JobId=start_topics_detection_job_result['JobId'])

    # 初始化等待時間計數，單位為 `秒`
    waited = 0

    # 設定超時時間，單位為 `分鐘`
    timeout_minutes = 40

    # 透過 while 檢查工作的狀態是否完成
    while job['TopicsDetectionJobProperties']['JobStatus'] != 'COMPLETED':
        # 每次迴圈暫停 60 秒再進行下一次狀態檢查，沒耐心的話可自訂短一點
        sleep(60)
        # 累計已等待的時間，每次 60 秒
        waited += 60

        # 檢查是否已超過自訂的上限時間
        # 若超時則透過語法 assert 終止程式並提示錯誤
        assert waited // 60 < timeout_minutes, "Job timed out after %d seconds." % waited
        
        # 印出 `.` 表示程式正在等待完成
        print('.', end='')

        # 再次查詢工作狀態，更新 job 資訊
        job = comprehend_client.describe_topics_detection_job(
            JobId=start_topics_detection_job_result['JobId']
        )

    # 工作完成
    print('Ready')
    ```

<br>

4. 任務完成，下載結果。

    ```python
    # 取得 Comprehend 工作輸出文件的 S3 URI
    topic_comprehend_output_file = job['TopicsDetectionJobProperties']['OutputDataConfig']['S3Uri']

    # 輸出文件的名稱
    print(f'output filename: {topic_comprehend_output_file}')

    # 移除 S3 URI 的開頭，並拆分出 bucket 名稱和檔案路徑
    topics_comprehend_bucket, topics_comprehend_key = topic_comprehend_output_file.replace("s3://", "").split("/", 1)

    # 顯示從 S3 路徑解析出的檔案路徑（key）
    print(topics_comprehend_key)

    # 建立 S3 資源物件
    s3r = boto3.resource('s3')

    # 從指定的 S3 bucket 下載主題檢測輸出文件到本地，並命名為 'output.tar.gz'
    s3r.meta.client.download_file(
        topics_comprehend_bucket, 
        topics_comprehend_key, 
        'output.tar.gz'
    )
    ```

    _輸出_

    ```bash
    output filename: s3://c133864a3391500l8216998t1w753251772626-labbucket-m0jhfwhqvfik/outputfolder/753251772626-TOPICS-5199ae6b337b522fab6a70844842b5ea/output/output.tar.gz
    outputfolder/753251772626-TOPICS-5199ae6b337b522fab6a70844842b5ea/output/output.tar.gz
    ```

<br>

5. 利用 tarfile 進行解壓縮。

    ```python
    import tarfile
    tf = tarfile.open('output.tar.gz')
    tf.extractall()
    ```

<br>

## 分析 Comprehend 輸出

_Comprehend 下載的壓縮文件包含兩個文件_

<br>

1. `topic-terms.csv` 列出集合中的主題。對於每個主題，此列表包含前 10 個詞語（按權重）；`doc-topics.csv` 列出與主題關聯的文件及其比例。每個文件可以與多個主題關聯。

<br>

2. 將 `topic-terms.csv` 文件加載到 pandas DataFrame 中。

    ```python
    import pandas as pd
    dftopicterms = pd.read_csv("topic-terms.csv")
    ```

<br>

3. 查看前幾行。

    ```python
    dftopicterms.head()
    ```

<br>

4. 輸出每個主題及其熱門詞語。

    ```python
    for t in range(0, number_of_topics):
        rslt_df = dftopicterms.loc[dftopicterms['topic'] == t]
        topic_list = rslt_df['term'].values.tolist()
        print(f'Topic {t:2} - {topic_list}')
    ```

    _輸出_

    ```bash
    Topic  0 - [
        'people', 'thing', 'government', 'force', 'kill', 'live', 'associate', 'gun', 'law', 'moral'
    ]
    Topic  1 - [
        'find', 'source', 'information', 'stuff', 'code', 'delete', 'site', 'ftp', 'image', 'advance'
    ]
    # 以下省略
    ```

<br>

5. 在上一個 CELL 的輸出中選擇一個描述每個主題的詞語，在以下 CELL 中將佔位符文本替換為決定用來描述每個主題的詞語。

    ```python
    colnames = pd.DataFrame({
        'topics': [
            'topic 0', 'topic 1', 'topic 2', 'topic 3', 'topic 4', 
            'topic 5', 'topic 6', 'topic 7', 'topic 8', 'topic 9',
            'topic 10', 'topic 11', 'topic 12', 'topic 13', 
            'topic 14', 'topic 15', 'topic 16', 'topic 17', 
            'topic 18', 'topic 19'
        ]
    })
    ```

<br>

6. 接著，讀取 `doc-topics.csv` 文件。

    ```python
    dfdoctopics = pd.read_csv("doc-topics.csv")
    dfdoctopics.head()
    ```

<br>

7. 為了簡化可視化，選擇 `5` 篇文章進行繪製。

    ```python
    to_chart = dfdoctopics.loc[dfdoctopics['docname'].isin([
        'newsgroups:1000', 'newsgroups:2000', 
        'newsgroups:3000', 'newsgroups:4000', 
        'newsgroups:5000'
    ])]
    ```

<br>

8. 然後，使用 `pivot_table` 函數將 **docname** 列中的值映射到實際列。

    ```python
    to_chart = to_chart.pivot_table(
        values='proportion', 
        index='topic', 
        columns='docname'
    )
    to_chart.head()
    ```

<br>

9. 最後，繪製新聞組文件的主題。某些文檔具有單一主題，而其他文檔則具有多個主題的組合；加入魔法指令 `%matplotlib inline` 可讓 `Matplotlib` 繪製的圖表直接在 Notebook 的輸出區域中顯示。

    ```python
    import matplotlib.pyplot as plt
    %matplotlib inline

    fs = 12
    to_chart.plot(kind='bar', figsize=(16, 4), fontsize=fs)
    plt.ylabel('Topic assignment', fontsize=fs+2)
    plt.xlabel('Topic ID', fontsize=fs+2)
    ```

<br>

___

_END_