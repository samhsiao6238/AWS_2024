{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (24.3.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: nltk in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: seaborn in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade nltk\n",
    "!pip install --upgrade seaborn\n",
    "!pip install --upgrade matplotlib  pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samhsiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samhsiao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samhsiao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samhsiao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 混淆矩陣可視化函數\n",
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    # 計算混淆矩陣\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    # 將混淆矩陣轉為 DataFrame 以便於繪圖\n",
    "    df_confusion = pd.DataFrame(matrix)\n",
    "    # 設定顏色板\n",
    "    colormap = sns.color_palette(\"BrBG\", 10)\n",
    "    # 使用 seaborn 的 heatmap 繪製混淆矩陣\n",
    "    sns.heatmap(df_confusion, annot=True, fmt=\".2f\", cbar=None, cmap=colormap)\n",
    "    plt.title(\"Confusion Matrix\")  # 設置標題\n",
    "    plt.tight_layout()  # 自動調整佈局\n",
    "    plt.ylabel(\"True Class\")  # 設置 y 軸標籤\n",
    "    plt.xlabel(\"Predicted Class\")  # 設置 x 軸標籤\n",
    "    plt.show()  # 顯示圖表\n",
    "\n",
    "\n",
    "# 評估指標輸出函數\n",
    "def print_metrics(test_labels, target_predicted_binary):\n",
    "    # 計算混淆矩陣，並解構為四個部分\n",
    "    TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()\n",
    "\n",
    "    # 各種評估指標計算\n",
    "    Sensitivity = float(TP) / (TP + FN) * 100  # 敏感度 (TPR)\n",
    "    Specificity = float(TN) / (TN + FP) * 100  # 特異性 (TNR)\n",
    "    Precision = float(TP) / (TP + FP) * 100  # 精確度\n",
    "    NPV = float(TN) / (TN + FN) * 100  # 負預測值\n",
    "    FPR = float(FP) / (FP + TN) * 100  # 假陽性率\n",
    "    FNR = float(FN) / (TP + FN) * 100  # 假陰性率\n",
    "    FDR = float(FP) / (TP + FP) * 100  # 假發現率\n",
    "    ACC = float(TP + TN) / (TP + FP + FN + TN) * 100  # 總體準確率\n",
    "\n",
    "    # 輸出各項指標\n",
    "    print(f\"Sensitivity or TPR: {Sensitivity}%\")\n",
    "    print(f\"Specificity or TNR: {Specificity}%\")\n",
    "    print(f\"Precision: {Precision}%\")\n",
    "    print(f\"Negative Predictive Value: {NPV}%\")\n",
    "    print(f\"False Positive Rate: {FPR}%\")\n",
    "    print(f\"False Negative Rate: {FNR}%\")\n",
    "    print(f\"False Discovery Rate: {FDR}%\")\n",
    "    print(f\"Accuracy: {ACC}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/imdb.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What I hoped for (or even expected) was the we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Garden State must rate amongst the most contri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is a lot wrong with this film. I will no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To qualify my use of \"realistic\" in the summar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty War is absolutely one of the best politi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  What I hoped for (or even expected) was the we...      0\n",
       "1  Garden State must rate amongst the most contri...      0\n",
       "2  There is a lot wrong with this film. I will no...      1\n",
       "3  To qualify my use of \"realistic\" in the summar...      1\n",
       "4  Dirty War is absolutely one of the best politi...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    25000\n",
       "1    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df):\n",
    "\n",
    "    train, test_and_validate = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2, \n",
    "        shuffle=True, \n",
    "        random_state=324\n",
    "    )\n",
    "    test, validate = train_test_split(\n",
    "        test_and_validate, \n",
    "        test_size=0.5, \n",
    "        shuffle=True, \n",
    "        random_state=324\n",
    "    )\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n",
      "(5000, 2)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = split_data(df)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shapes before processing:  (40000, 2) (5000, 2) (5000, 2)\n",
      "Datasets shapes after processing:  (40000, 500) (5000, 500) (5000, 500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "text_features = ['text']\n",
    "model_target = 'label'\n",
    "\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(max_features=500))\n",
    "])\n",
    "\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_pre_0', text_processor_0, text_features[0])\n",
    "])\n",
    "\n",
    "print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)\n",
    "train_matrix = data_preprocessor.fit_transform(train)\n",
    "test_matrix = data_preprocessor.transform(test)\n",
    "validate_matrix = data_preprocessor.transform(validate)\n",
    "print('Datasets shapes after processing: ', train_matrix.shape, validate_matrix.shape, test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已儲存至本地：./batch-in/batch-in.csv\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 將數據儲存到本地 CSV 文件，而非上傳至 S3\n",
    "def save_to_local_csv(filename, folder, X_train, y_train, is_test=False):\n",
    "    # 建立資料夾（若不存在）\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    # 使用 StringIO 作為緩衝區，準備將數據寫入 CSV\n",
    "    csv_buffer = io.StringIO()\n",
    "    \n",
    "    # 將特徵矩陣轉為 float32 格式並展開為列表\n",
    "    features = [\n",
    "        t.toarray().astype('float32').flatten().tolist() for t in X_train\n",
    "    ]\n",
    "    \n",
    "    # 如果是測試數據，僅包含特徵\n",
    "    if is_test:\n",
    "        temp_list = features\n",
    "    else:\n",
    "        # 非測試數據則在最前列插入標籤\n",
    "        temp_list = np.insert(features, 0, y_train['label'], axis=1)\n",
    "    \n",
    "    # 將數據寫入緩衝區並指定分隔符為逗號\n",
    "    np.savetxt(csv_buffer, temp_list, delimiter=',')\n",
    "    \n",
    "    # 將緩衝區的內容寫入指定的本地文件\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(csv_buffer.getvalue())\n",
    "    \n",
    "    print(f\"文件已儲存至本地：{file_path}\")\n",
    "\n",
    "# 使用範例\n",
    "save_to_local_csv(\n",
    "    'batch-in.csv', \n",
    "    './batch-in', \n",
    "    X_train=test_matrix, \n",
    "    y_train=test, \n",
    "    is_test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定本地路徑和文件名稱\n",
    "# 使用本地資料夾作為儲存目標\n",
    "bucket = './local_data'\n",
    "prefix = 'lab41'\n",
    "train_file = 'train-pass1.csv'\n",
    "validate_file = 'validate-pass1.csv'\n",
    "test_file = 'test-pass1.csv'\n",
    "\n",
    "# 完整的文件路徑範例\n",
    "train_path = os.path.join(bucket, prefix, train_file)\n",
    "validate_path = os.path.join(bucket, prefix, validate_file)\n",
    "test_path = os.path.join(bucket, prefix, test_file)\n",
    "\n",
    "# 確保資料夾存在\n",
    "os.makedirs(os.path.join(bucket, prefix), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已儲存至本地：train/train-pass1.csv\n",
      "文件已儲存至本地：validate/validate-pass1.csv\n",
      "文件已儲存至本地：test/test-pass1.csv\n"
     ]
    }
   ],
   "source": [
    "# 調用本地存儲函數來保存數據集\n",
    "save_to_local_csv(train_file, 'train', train_matrix, train)\n",
    "save_to_local_csv(validate_file, 'validate', validate_matrix, validate)\n",
    "save_to_local_csv(test_file, 'test', test_matrix, test, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檢測到 '/Volumes/SSD_01/00_課程講義專用/AWS_2024/01_課程筆記/00_Lab/94020/Lab 4.1_Implementing Sentiment Analysis/02_exLab41_完整/output' 存在為文件，刪除該文件...\n",
      "目錄 '/Volumes/SSD_01/00_課程講義專用/AWS_2024/01_課程筆記/00_Lab/94020/Lab 4.1_Implementing Sentiment Analysis/02_exLab41_完整/output' 已成功建立，準備儲存文件。\n",
      "[0]\ttrain-error:0.28790\tvalidate-error:0.28940\n",
      "[1]\ttrain-error:0.26698\tvalidate-error:0.27240\n",
      "[2]\ttrain-error:0.25250\tvalidate-error:0.26080\n",
      "[3]\ttrain-error:0.24463\tvalidate-error:0.25120\n",
      "[4]\ttrain-error:0.23950\tvalidate-error:0.24480\n",
      "[5]\ttrain-error:0.22737\tvalidate-error:0.23740\n",
      "[6]\ttrain-error:0.22428\tvalidate-error:0.23580\n",
      "[7]\ttrain-error:0.21752\tvalidate-error:0.23060\n",
      "[8]\ttrain-error:0.20877\tvalidate-error:0.22200\n",
      "[9]\ttrain-error:0.20563\tvalidate-error:0.22080\n",
      "[10]\ttrain-error:0.20128\tvalidate-error:0.21760\n",
      "[11]\ttrain-error:0.19578\tvalidate-error:0.21600\n",
      "[12]\ttrain-error:0.19207\tvalidate-error:0.21120\n",
      "[13]\ttrain-error:0.18707\tvalidate-error:0.20660\n",
      "[14]\ttrain-error:0.18495\tvalidate-error:0.20540\n",
      "[15]\ttrain-error:0.18165\tvalidate-error:0.20240\n",
      "[16]\ttrain-error:0.17822\tvalidate-error:0.20300\n",
      "[17]\ttrain-error:0.17492\tvalidate-error:0.19920\n",
      "[18]\ttrain-error:0.17185\tvalidate-error:0.19840\n",
      "[19]\ttrain-error:0.16895\tvalidate-error:0.19480\n",
      "[20]\ttrain-error:0.16605\tvalidate-error:0.19320\n",
      "[21]\ttrain-error:0.16178\tvalidate-error:0.19320\n",
      "[22]\ttrain-error:0.15962\tvalidate-error:0.19180\n",
      "[23]\ttrain-error:0.15840\tvalidate-error:0.18860\n",
      "[24]\ttrain-error:0.15527\tvalidate-error:0.19180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:35:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"num_round\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\ttrain-error:0.15302\tvalidate-error:0.18960\n",
      "[26]\ttrain-error:0.15093\tvalidate-error:0.18780\n",
      "[27]\ttrain-error:0.14868\tvalidate-error:0.18620\n",
      "[28]\ttrain-error:0.14647\tvalidate-error:0.18620\n",
      "[29]\ttrain-error:0.14402\tvalidate-error:0.18740\n",
      "[30]\ttrain-error:0.14142\tvalidate-error:0.18880\n",
      "[31]\ttrain-error:0.14030\tvalidate-error:0.18580\n",
      "[32]\ttrain-error:0.13905\tvalidate-error:0.18580\n",
      "[33]\ttrain-error:0.13787\tvalidate-error:0.18620\n",
      "[34]\ttrain-error:0.13650\tvalidate-error:0.18680\n",
      "[35]\ttrain-error:0.13448\tvalidate-error:0.18540\n",
      "[36]\ttrain-error:0.13230\tvalidate-error:0.18560\n",
      "[37]\ttrain-error:0.13065\tvalidate-error:0.18420\n",
      "[38]\ttrain-error:0.12905\tvalidate-error:0.18440\n",
      "[39]\ttrain-error:0.12850\tvalidate-error:0.18340\n",
      "[40]\ttrain-error:0.12773\tvalidate-error:0.18260\n",
      "[41]\ttrain-error:0.12652\tvalidate-error:0.18220\n",
      "模型已儲存至 /Volumes/SSD_01/00_課程講義專用/AWS_2024/01_課程筆記/00_Lab/94020/Lab 4.1_Implementing Sentiment Analysis/02_exLab41_完整/output/xgboost_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:35:25] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# 設置本地輸出路徑\n",
    "output_dir = os.path.join(os.getcwd(), 'output')\n",
    "\n",
    "# 檢查 output_dir 是否存在，如果存在則刪除該目錄或文件\n",
    "if os.path.exists(output_dir):\n",
    "    # 如果存在，且是文件，則刪除該文件\n",
    "    if os.path.isfile(output_dir):\n",
    "        print(f\"檢測到 '{output_dir}' 存在為文件，刪除該文件...\")\n",
    "        os.remove(output_dir)\n",
    "    # 如果存在，且是目錄，則刪除該目錄及其內容\n",
    "    elif os.path.isdir(output_dir):\n",
    "        print(f\"檢測到 '{output_dir}' 存在為目錄，刪除該目錄及其內容...\")\n",
    "        import shutil\n",
    "        shutil.rmtree(output_dir)\n",
    "\n",
    "# 建立 output 目錄\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"目錄 '{output_dir}' 已成功建立，準備儲存文件。\")\n",
    "\n",
    "# 設定超參數\n",
    "hyperparams = {\n",
    "    \"num_round\": 42,\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"silent\": 1\n",
    "}\n",
    "\n",
    "# 準備訓練數據和驗證數據\n",
    "# 假設 train_matrix, validate_matrix 和 train, validate 中包含了訓練和驗證數據\n",
    "dtrain = xgb.DMatrix(train_matrix, label=train['label'])\n",
    "dvalidate = xgb.DMatrix(validate_matrix, label=validate['label'])\n",
    "\n",
    "# 執行 XGBoost 訓練並使用早停法則\n",
    "evals = [(dtrain, 'train'), (dvalidate, 'validate')]\n",
    "xgb_model = xgb.train(\n",
    "    params=hyperparams,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=hyperparams[\"num_round\"],\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# 儲存模型到本地\n",
    "model_path = os.path.join(output_dir, 'xgboost_model.bin')\n",
    "xgb_model.save_model(model_path)\n",
    "print(f\"模型已儲存至 {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本地訓練數據路徑: /Volumes/SSD_01/00_課程講義專用/AWS_2024/01_課程筆記/00_Lab/94020/Lab 4.1_Implementing Sentiment Analysis/02_exLab41_完整/data/train/train-pass1.csv\n",
      "本地驗證數據路徑: /Volumes/SSD_01/00_課程講義專用/AWS_2024/01_課程筆記/00_Lab/94020/Lab 4.1_Implementing Sentiment Analysis/02_exLab41_完整/data/validate/validate-pass1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 設定本地數據路徑\n",
    "local_data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# 確認路徑中訓練和驗證數據是否存在\n",
    "train_file_path = os.path.join(local_data_dir, 'train', train_file)\n",
    "validate_file_path = os.path.join(local_data_dir, 'validate', validate_file)\n",
    "\n",
    "# 建立訓練和驗證數據通道的字典，指向本地路徑\n",
    "data_channels = {\n",
    "    'train': train_file_path,\n",
    "    'validation': validate_file_path\n",
    "}\n",
    "\n",
    "print(f\"本地訓練數據路徑: {train_file_path}\")\n",
    "print(f\"本地驗證數據路徑: {validate_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數據已儲存至 /Volumes/SSD_01/00_課程講義專用/AWS_2024/01_課程筆記/00_Lab/94020/Lab 4.1_Implementing Sentiment Analysis/02_exLab41_完整/output/train-pass1.csv\n",
      "數據已儲存至 /Volumes/SSD_01/00_課程講義專用/AWS_2024/01_課程筆記/00_Lab/94020/Lab 4.1_Implementing Sentiment Analysis/02_exLab41_完整/output/validate-pass1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_to_csv_local(filename, data_matrix, labels, output_dir='output'):\n",
    "    \"\"\"\n",
    "    儲存數據矩陣和標籤至本地的 CSV 文件。\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    # 將數據和標籤合併成 DataFrame\n",
    "    data_df = pd.DataFrame(data_matrix)\n",
    "    data_df['label'] = labels\n",
    "    # 將 DataFrame 存儲為 CSV\n",
    "    data_df.to_csv(filepath, index=False)\n",
    "    print(f\"數據已儲存至 {filepath}\")\n",
    "\n",
    "# 使用上述函數保存數據\n",
    "save_to_csv_local('train-pass1.csv', train_matrix, train['label'], output_dir)\n",
    "save_to_csv_local('validate-pass1.csv', validate_matrix, validate['label'], output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練...\n",
      "[0]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[1]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[2]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[3]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[4]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[5]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[6]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[7]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[8]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[9]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "訓練完成\n",
      "模型已儲存至 output/xgboost_model_11-14-2024-17-41-03.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:41:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"num_round\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:41:03] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 設定文件路徑\n",
    "output_dir = 'output'\n",
    "train_file = 'train-pass1.csv'\n",
    "validate_file = 'validate-pass1.csv'\n",
    "\n",
    "# 設定超參數\n",
    "hyperparams = {\n",
    "    \"num_round\": 42,\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"silent\": 1\n",
    "}\n",
    "\n",
    "# 讀取數據\n",
    "train_df = pd.read_csv(os.path.join(output_dir, train_file))\n",
    "validate_df = pd.read_csv(os.path.join(output_dir, validate_file))\n",
    "\n",
    "# 確保所有特徵欄位都是數值型\n",
    "train_df = train_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "validate_df = validate_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# 將數據轉換為 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(train_df.drop('label', axis=1), label=train_df['label'])\n",
    "dvalidate = xgb.DMatrix(validate_df.drop('label', axis=1), label=validate_df['label'])\n",
    "\n",
    "# 設置評估數據\n",
    "evals = [(dtrain, 'train'), (dvalidate, 'validate')]\n",
    "\n",
    "# 執行訓練並使用早停法則\n",
    "print(\"開始訓練...\")\n",
    "xgb_model = xgb.train(\n",
    "    params=hyperparams,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=hyperparams[\"num_round\"],\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "print(\"訓練完成\")\n",
    "\n",
    "# 保存模型至本地\n",
    "model_path = os.path.join(output_dir, f'xgboost_model_{datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")}.bin')\n",
    "xgb_model.save_model(model_path)\n",
    "print(f\"模型已儲存至 {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練...\n",
      "[0]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[1]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[2]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[3]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[4]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[5]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[6]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[7]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[8]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[9]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[10]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "訓練完成\n",
      "訓練時間: 0.01 秒\n",
      "模型已儲存至 output/xgboost_model_11-14-2024-17-42-37.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"num_round\", \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 設定文件路徑\n",
    "output_dir = 'output'\n",
    "train_file = 'train-pass1.csv'\n",
    "validate_file = 'validate-pass1.csv'\n",
    "\n",
    "# 設定超參數\n",
    "hyperparams = {\n",
    "    \"num_round\": 42,\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"silent\": 1\n",
    "}\n",
    "\n",
    "# 讀取數據\n",
    "train_df = pd.read_csv(os.path.join(output_dir, train_file))\n",
    "validate_df = pd.read_csv(os.path.join(output_dir, validate_file))\n",
    "\n",
    "# 確保所有特徵欄位都是數值型\n",
    "train_df = train_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "validate_df = validate_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# 將數據轉換為 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(train_df.drop('label', axis=1), label=train_df['label'])\n",
    "dvalidate = xgb.DMatrix(validate_df.drop('label', axis=1), label=validate_df['label'])\n",
    "\n",
    "# 設置評估數據\n",
    "evals = [(dtrain, 'train'), (dvalidate, 'validate')]\n",
    "\n",
    "# 執行訓練並使用早停法則，並計時\n",
    "print(\"開始訓練...\")\n",
    "start_time = time.time()\n",
    "xgb_model = xgb.train(\n",
    "    params=hyperparams,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=hyperparams[\"num_round\"],\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"訓練完成\")\n",
    "\n",
    "# 計算和顯示訓練時間\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"訓練時間: {elapsed_time:.2f} 秒\")\n",
    "\n",
    "# 保存模型至本地\n",
    "model_path = os.path.join(output_dir, f'xgboost_model_{datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")}.bin')\n",
    "xgb_model.save_model(model_path)\n",
    "print(f\"模型已儲存至 {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練...\n",
      "[0]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[1]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[2]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[3]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[4]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[5]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[6]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[7]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[8]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[9]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[10]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "訓練完成\n",
      "訓練和驗證錯誤率分析：\n",
      "    timestamp       metric_name     value\n",
      "0           0       train:error  0.401875\n",
      "1           1       train:error  0.401875\n",
      "2           2       train:error  0.401875\n",
      "3           3       train:error  0.401875\n",
      "4           4       train:error  0.401875\n",
      "5           5       train:error  0.401875\n",
      "6           6       train:error  0.401875\n",
      "7           7       train:error  0.401875\n",
      "8           8       train:error  0.401875\n",
      "9           9       train:error  0.401875\n",
      "10         10       train:error  0.401875\n",
      "11          0  validation:error  0.052200\n",
      "12          1  validation:error  0.052200\n",
      "13          2  validation:error  0.052200\n",
      "14          3  validation:error  0.052200\n",
      "15          4  validation:error  0.052200\n",
      "16          5  validation:error  0.052200\n",
      "17          6  validation:error  0.052200\n",
      "18          7  validation:error  0.052200\n",
      "19          8  validation:error  0.052200\n",
      "20          9  validation:error  0.052200\n",
      "21         10  validation:error  0.052200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:46:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# 設定文件路徑\n",
    "output_dir = 'output'\n",
    "train_file = 'train-pass1.csv'\n",
    "validate_file = 'validate-pass1.csv'\n",
    "\n",
    "# 設定超參數\n",
    "hyperparams = {\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"silent\": 1\n",
    "}\n",
    "num_boost_round = 42  # 設定迭代次數\n",
    "\n",
    "# 讀取數據\n",
    "train_df = pd.read_csv(os.path.join(output_dir, train_file))\n",
    "validate_df = pd.read_csv(os.path.join(output_dir, validate_file))\n",
    "\n",
    "# 確保所有特徵欄位都是數值型\n",
    "train_df = train_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "validate_df = validate_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# 將數據轉換為 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(train_df.drop('label', axis=1), label=train_df['label'])\n",
    "dvalidate = xgb.DMatrix(validate_df.drop('label', axis=1), label=validate_df['label'])\n",
    "\n",
    "# 設置評估數據\n",
    "evals = [(dtrain, 'train'), (dvalidate, 'validate')]\n",
    "\n",
    "# 訓練過程中的評估結果將儲存在 `eval_result` 字典中\n",
    "eval_result = {}\n",
    "\n",
    "# 執行訓練並使用早停法則\n",
    "print(\"開始訓練...\")\n",
    "xgb_model = xgb.train(\n",
    "    params=hyperparams,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10,\n",
    "    evals_result=eval_result\n",
    ")\n",
    "print(\"訓練完成\")\n",
    "\n",
    "# 提取訓練和驗證的錯誤率數據\n",
    "train_error = eval_result['train']['error']\n",
    "validate_error = eval_result['validate']['error']\n",
    "\n",
    "# 調整 timestamp 和 metric_name 列以匹配數據長度\n",
    "df_train = pd.DataFrame({\n",
    "    'timestamp': range(len(train_error)),\n",
    "    'metric_name': ['train:error'] * len(train_error),\n",
    "    'value': train_error\n",
    "})\n",
    "\n",
    "df_validate = pd.DataFrame({\n",
    "    'timestamp': range(len(validate_error)),\n",
    "    'metric_name': ['validation:error'] * len(validate_error),\n",
    "    'value': validate_error\n",
    "})\n",
    "\n",
    "# 合併訓練和驗證的數據\n",
    "df_analytics = pd.concat([df_train, df_validate], ignore_index=True)\n",
    "\n",
    "# 顯示 DataFrame 中的錯誤率，方便進行分析\n",
    "print(\"訓練和驗證錯誤率分析：\")\n",
    "print(df_analytics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練...\n",
      "[0]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[1]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[2]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[3]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[4]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[5]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[6]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[7]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[8]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[9]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "[10]\ttrain-error:0.40187\tvalidate-error:0.05220\n",
      "訓練完成\n",
      "最終訓練和驗證錯誤率分析：\n",
      "   timestamp       metric_name     value\n",
      "0        0.0       train:error  0.401875\n",
      "1        0.0  validation:error  0.052200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhsiao/Documents/PythonVenv/envAWS2/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [17:47:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# 設定文件路徑\n",
    "output_dir = 'output'\n",
    "train_file = 'train-pass1.csv'\n",
    "validate_file = 'validate-pass1.csv'\n",
    "\n",
    "# 設定超參數\n",
    "hyperparams = {\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"silent\": 1\n",
    "}\n",
    "num_boost_round = 42  # 設定迭代次數\n",
    "\n",
    "# 讀取數據\n",
    "train_df = pd.read_csv(os.path.join(output_dir, train_file))\n",
    "validate_df = pd.read_csv(os.path.join(output_dir, validate_file))\n",
    "\n",
    "# 確保所有特徵欄位都是數值型\n",
    "train_df = train_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "validate_df = validate_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# 將數據轉換為 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(train_df.drop('label', axis=1), label=train_df['label'])\n",
    "dvalidate = xgb.DMatrix(validate_df.drop('label', axis=1), label=validate_df['label'])\n",
    "\n",
    "# 設置評估數據\n",
    "evals = [(dtrain, 'train'), (dvalidate, 'validate')]\n",
    "\n",
    "# 訓練過程中的評估結果將儲存在 `eval_result` 字典中\n",
    "eval_result = {}\n",
    "\n",
    "# 執行訓練並使用早停法則\n",
    "print(\"開始訓練...\")\n",
    "xgb_model = xgb.train(\n",
    "    params=hyperparams,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10,\n",
    "    evals_result=eval_result\n",
    ")\n",
    "print(\"訓練完成\")\n",
    "\n",
    "# 提取最終的訓練和驗證錯誤率數據\n",
    "final_train_error = eval_result['train']['error'][-1]\n",
    "final_validate_error = eval_result['validate']['error'][-1]\n",
    "\n",
    "# 建立 DataFrame 以符合期望的格式\n",
    "df_analytics = pd.DataFrame({\n",
    "    'timestamp': [0.0, 0.0],\n",
    "    'metric_name': ['train:error', 'validation:error'],\n",
    "    'value': [final_train_error, final_validate_error]\n",
    "})\n",
    "\n",
    "# 顯示 DataFrame 中的最終錯誤率，方便進行分析\n",
    "print(\"最終訓練和驗證錯誤率分析：\")\n",
    "print(df_analytics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAWS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
