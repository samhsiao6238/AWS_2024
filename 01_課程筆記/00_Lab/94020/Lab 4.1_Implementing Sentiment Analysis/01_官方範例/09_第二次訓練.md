#

## 第二次传递：标准化文本

1. 可删除所有非索引字，但可能想保留可能影响情绪的非索引字，例如 __not__ 或 __don't__。


```python
# Get a list of stopwords from the NLTK library
stop = stopwords.words('english')

def remove_stopwords(stopwords):
    # Implement this function
    excluding = ['against', 'not', 'don', 'don\'t','ain', 'are', 'aren\'t']
    ### BEGIN_SOLUTION
    excluding = ['against', 'not', 'don', 'don\'t','ain', 'are', 'aren\'t', 'could', 'couldn\'t',
             'did', 'didn\'t', 'does', 'doesn\'t', 'had', 'hadn\'t', 'has', 'hasn\'t', 
             'have', 'haven\'t', 'is', 'isn\'t', 'might', 'mightn\'t', 'must', 'mustn\'t',
             'need', 'needn\'t','should', 'shouldn\'t', 'was', 'wasn\'t', 'were', 
             'weren\'t', 'won\'t', 'would', 'wouldn\'t']
    ### END_SOLUTION
    return [word for word in stop if word not in excluding]

# New stopword list
stopwords = remove_stopwords(stop)

```

## 添加清理步骤

1. 更新以下 `clean` 函数，以完成删除前导空格和尾随空格、删除任何 HTML 标记。

```python
snow = SnowballStemmer('english')
def clean(sent):
    # Implement this function
    sent = sent.lower()
    sent = re.sub('\s+', ' ', sent)
    ### BEGIN_SOLUTION
    sent = sent.strip()
    sent = re.compile('<.*?>').sub('',sent)
    ### END_SOLUTION
    filtered_sentence = []
    
    for w in word_tokenize(sent):
        # You are applying custom filtering here. Feel free to try different things.
        # Check if it is not numeric, its length > 2, and it is not in stopwords
        if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  
            # Stem and add to filtered list
            filtered_sentence.append(snow.stem(w))
    final_string = " ".join(filtered_sentence) #final string of cleaned words
    return final_string
```

2. 使用之前创建的函数创建新的测试、验证和测试 DataFrame。

```python
# Uncomment the next line and implement the function call to split_data
#train, validate, test = 

### BEGIN_SOLUTION
train, validate, test = split_data(df)
### END_SOLUTION

print(train.shape)
print(test.shape)
print(validate.shape)
```

3. 管道已更新，以包含对之前定义的来自 `CountVectorizer`的 `clean`函数的调用。此函数需要更长的运行时间。


```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

text_features = ['text']
model_target = 'label'

text_processor_0 = Pipeline([
    ('text_vect_0', CountVectorizer(preprocessor=clean, max_features=500))
])

data_preprocessor = ColumnTransformer([
    ('text_pre_0', text_processor_0, text_features[0])
])

print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)
train_matrix = data_preprocessor.fit_transform(train)
test_matrix = data_preprocessor.transform(test)
validate_matrix = data_preprocessor.transform(validate)
print('Datasets shapes after processing: ', train_matrix.shape, validate_matrix.shape, test_matrix.shape)
```

4. 设置此传递的文件名。


```python
prefix='lab41'
train_file='train_pass2.csv'
validate_file='validate_pass2.csv'
test_file='test_pass2.csv'
```

## 将文件上载到 S3

1. 使用之前的代码将新文件上载到 Amazon S3。

```python
upload_s3_csv(train_file, 'train', train_matrix, train)
upload_s3_csv(validate_file, 'validate', validate_matrix, validate)
upload_s3_csv(test_file, 'test', test_matrix, test, True)
```

## 创建估算器并设置数据通道

1. 使用之前的代码设置估算器和数据通道。

```python
container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')

hyperparams={
    "num_round":"42",
    "eval_metric": "error",
    "objective": "binary:logistic",
    "silent" : 1
}

xgb_model=sagemaker.estimator.Estimator(
    container,
    sagemaker.get_execution_role(),
    instance_count=1,
    instance_type='ml.m5.2xlarge',
    output_path=s3_output_location,
    hyperparameters = hyperparams,
    sagemaker_session=sagemaker.Session()
)

train_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/train/{train_file}',
    content_type='text/csv'
)

validate_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/validate/{validate_file}',
    content_type='text/csv'
)

data_channels = {
    'train': train_channel, 
    'validation': validate_channel
}

xgb_model.fit(
    inputs=data_channels, 
    logs=False, 
    job_name='xgb-pass2-'+datetime.now()\
        .strftime("%m-%d-%Y-%H-%M-%S")
)
```

2. 後補。

```python
sagemaker.analytics.TrainingJobAnalytics(
    xgb_model._current_job_name, 
    metric_names = ['train:error','validation:error']
).dataframe()
```

## 创建批处理转换器任务

1. 使用之前的代码创建一个转换器任务。

```python
xgb_transformer = xgb_model.transformer(
    instance_count=1,
    instance_type='ml.m5.2xlarge',
    strategy='MultiRecord',
    assemble_with='Line',
    output_path=batch_output
)

xgb_transformer.transform(
    data=batch_input,
    data_type='S3Prefix',
    content_type='text/csv',
    split_type='Line')

xgb_transformer.wait(logs=False)
```

2. 後補。

```python
s3 = boto3.client('s3')
obj = s3.get_object(
    Bucket=bucket, 
    Key="{}/batch-out/{}".\
        format(prefix,'batch-in.csv.out')
)
target_predicted = pd.read_csv(
    io.BytesIO(obj['Body'].read()),
    sep=',',
    names=['class']
)

def binary_convert(x):
    threshold = 0.5
    if x > threshold:
        return 1
    else:
        return 0

target_predicted_binary = target_predicted['class'].apply(binary_convert)
```

3. 後補。

```python
plot_confusion_matrix(test['label'], target_predicted_binary)
```

4. 後補。

```python
print_metrics(test['label'], target_predicted_binary)
```

5. 将创建一个超参数优化任务来优化模型。

## 创建估算器用于优化

1. 第一步是创建一个估算器用于优化。取消注释并完成以下估算器代码。

```python
# xgb = sagemaker.estimator.Estimator(....)
xgb = sagemaker.estimator.Estimator(
    container,
    role=sagemaker.get_execution_role(), 
    # make sure you have limit set for these instances
    instance_count= 1,
    instance_type='ml.m5.2xlarge', 
    output_path=f's3://{bucket}/{prefix}/output',
    sagemaker_session=sagemaker.Session()
)
```

2. 後補。

```python
xgb.set_hyperparameters(
    eval_metric='error',
    objective='binary:logistic',
    num_round=42,
    silent=1
)
```

## 创建超参数范围

1. 使用 [XGBoost 优化文档](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html)，将超参数范围添加到以下单元格中。


```python
from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner

hyperparameter_ranges = {'alpha': ContinuousParameter(0,1000)}

hyperparameter_ranges = {
    'alpha': ContinuousParameter(0, 1000),
    'min_child_weight': ContinuousParameter(0, 120),
    'subsample': ContinuousParameter(0.5, 1),
    'eta': ContinuousParameter(0.1, 0.5),  
    'num_round': IntegerParameter(1,4000)
}
```

## 指定目标指标

1. 针对二元分类问题将 `objective_metric_name`和 `objective_type`更新为适当的值。

```python
objective_metric_name = '<INSERT_VALUE_HERE>'
objective_type = '<INSERT_VALUE_HERE>'

### BEGIN_SOLUTION
objective_metric_name = 'validation:error'
objective_type = 'Minimize'
### END_SOLUTION
```

2. 创建超参数优化任务。

```python
tuner = HyperparameterTuner(
    xgb,
    objective_metric_name,
    hyperparameter_ranges,
    # Set this to 10 or above depending upon budget & available time.
    max_jobs=10,
    max_parallel_jobs=1,
    objective_type=objective_type,
    early_stopping_type='Auto',
)
```

3. 运行优化任务。请注意，此任务可能需要大约 60 分钟的时间。

```python
tuner.fit(
    inputs=data_channels, 
    include_cls_metadata=False, 
    wait=False
)
```

4. 如果想在等待期间尝试第 7 节，请不要运行下一个单元格，而是转到第 7 节。

```python
tuner.wait()
```

5. 优化任务完成后，您可以查看来自优化任务的指标。

```python
from pprint import pprint
from sagemaker.analytics import HyperparameterTuningJobAnalytics

tuner_analytics = HyperparameterTuningJobAnalytics(
    tuner.latest_tuning_job.name,
    sagemaker_session=sagemaker.Session()
)

df_tuning_job_analytics = tuner_analytics.dataframe()

# Sort the tuning job analytics by the final metrics value
df_tuning_job_analytics.sort_values(
    by=['FinalObjectiveValue'],
    inplace=True,
    ascending=False if tuner.objective_type == "Maximize" else True
)

# Show detailed analytics for the top 20 models
df_tuning_job_analytics.head(20)
```
## 使用最佳超参数任务

1. 优化任务完成后，您可以从 實作HyperparameterTuner實作 对象中找到最佳优化任务。

```python
attached_tuner = HyperparameterTuner.attach(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())
best_training_job = attached_tuner.best_training_job()
```

2. 後補。

```python
from sagemaker.estimator import Estimator
algo_estimator = Estimator.attach(best_training_job)

best_algo_model = algo_estimator.create_model(
    env={'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT':"text/csv"}
)
```

3. 通过数据处理管道运行测试数据，以测试模型。

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

text_features = ['text']
model_target = 'label'

text_processor_0 = Pipeline([(
    'text_vect_0', 
    CountVectorizer(
        preprocessor=clean, max_features=500
    )
)])

data_preprocessor = ColumnTransformer([(
    'text_pre_0', 
    text_processor_0, 
    text_features[0]
)])

print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)
train_matrix = data_preprocessor.fit_transform(train)
test_matrix = data_preprocessor.transform(test)
validate_matrix = data_preprocessor.transform(validate)
print(
    'Datasets shapes after processing: ', 
    train_matrix.shape, 
    validate_matrix.shape, 
    test_matrix.shape
)
```

4. 使用来自超参数优化任务的最佳算法，以使用批处理转换的测试数据。

```python
upload_s3_csv('batch-in.csv', 'batch-in', test_matrix, test, True)

batch_output = f's3://{bucket}/{prefix}/batch-out/'
batch_input = f's3://{bucket}/{prefix}/batch-in/{batch_X_file}'

xgb_transformer = best_algo_model.transformer(
    instance_count=1,
    instance_type='ml.m5.2xlarge',
    strategy='MultiRecord',
    assemble_with='Line',
    output_path=batch_output
)
xgb_transformer.transform(
    data=batch_input,
    data_type='S3Prefix',
    content_type='text/csv',
    split_type='Line'
)
xgb_transformer.wait(logs=False)
```

5. 处理结果以计算类。

```python
s3 = boto3.client('s3')
obj = s3.get_object(
    Bucket=bucket, 
    Key=f'{prefix}/batch-out/batch-in.csv.out'
)
target_predicted = pd.read_csv(
    io.BytesIO(obj['Body'].read()),
    sep=',',
    names=['class']
)

def binary_convert(x):
    threshold = 0.5
    if x > threshold:
        return 1
    else:
        return 0

target_predicted_binary = target_predicted['class'].apply(binary_convert)

```

6. 绘制混淆矩阵并打印指标。

```python
plot_confusion_matrix(test['label'], target_predicted_binary)
```

7. 後補。

```python
print_metrics(test['label'], target_predicted_binary)
```

## 使用 BlazingText

_将使用 BlazingText，这是一种内置的 Amazon SageMaker 算法。BlazingText 可以在不做修改的情况下执行分类。将为 BlazingText 重新格式化数据。然后，将使用数据训练算法并将结果与之前的模型进行比较。_


1. 首先，获取算法容器。


```python
import sagemaker
from sagemaker.image_uris import retrieve

container = retrieve(
    'blazingtext',
    boto3.Session().region_name,"latest"
)
```

2. 为训练、验证和测试数据配置前缀。

```python
import io
    
prefix='lab41'
train_file='blazing_train.txt'
validate_file='blazing_validate.txt'
test_file='blazing_test.txt'
```

3. 提醒自己数据是什么样的。

```python
train.head()
```


4. BlazingText 需要采用以下格式的数据：

```bash
`__label__1` Caught this movie on the tube on a Sunday...
```

5. 以下两个单元格将 DataFrame 转换为正确的格式，然后将它们上载到 Amazon S3。

```python
blazing_text_buffer = io.StringIO()
train.to_string(buf=blazing_text_buffer, columns=['label','text'], header=False, index=False, formatters=
                         {'label': '__label__{}'.format})
s3r = boto3.resource('s3')
s3r.Bucket(bucket).Object(os.path.join(prefix, 'blazing', train_file)).put(Body=blazing_text_buffer.getvalue())
```

6. 後補。

```python
blazing_text_buffer = io.StringIO()
validate.to_string(
    buf=blazing_text_buffer, 
    columns=['label','text'], 
    header=False, 
    index=False, 
    formatters={'label': '__label__{}'.format}
)
s3r.Bucket(bucket).Object(
    os.path.join(prefix, 'blazing', validate_file)
).put(
    Body=blazing_text_buffer.getvalue()
)
```

## BlazingText 估算器

1. 通过指定缺失值来取消注释并完成估算器代码。

```python
bt_model = sagemaker.estimator.Estimator(
    container,
    sagemaker.get_execution_role(), 
    instance_count=1, 
    instance_type='ml.c4.4xlarge',
    volume_size = 30,
    max_run = 360000,
    input_mode= 'File',
    output_path=s3_output_location,
    sagemaker_session=sagemaker.Session()
)

```

2. 使用以下超参数。

```python
bt_model.set_hyperparameters(
    mode="supervised",
    epochs=10,
    min_count=2,
    learning_rate=0.05,
    vector_dim=10,
    early_stopping=True,
    patience=4,
    min_epochs=5,
    word_ngrams=2
)
```

3. 设置训练通道和验证通道。

```python
train_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/blazing/{train_file}',
    content_type='text/csv')

validate_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/blazing/{validate_file}',
    content_type='text/csv')

data_channels_3 = {
    'train': train_channel, 
    'validation': validate_channel
}
```

## 开始训练任务

1. 输入以下代码开始训练任务。（这个步骤可能需要花几分钟的时间。）

```python
bt_model.fit(inputs=data_channels_3, logs=False)
```

2. 训练任务完成后，请查看训练指标。

```python
sagemaker.analytics.TrainingJobAnalytics(
    bt_model._current_job_name, 
    metric_names = ['train:accuracy','validation:accuracy']
).dataframe()
```

3.  後補。

```python
pd.options.display.max_rows
pd.set_option('display.max_colwidth', None)
```

4. 後補。


```python
bt_test = test.copy()
bt_test.head()
```

5. 将数据集格式化为 BlazingText 所需的格式。

```python
# bt_test['text'].str.strip()
bt_test.replace(r'\\n','', regex=True, inplace = True)
bt_test.rename(columns={'text':'source'}, inplace=True)
bt_test.drop(columns='label', inplace=True)
```

6. 後補。

```python
print(bt_test.head().to_json(orient="records", lines=True))
```

7. 将文件上载到 Amazon S3。

```python
bt_file = 'bt_input.json'
blazing_text_buffer = io.StringIO()
bt_test.to_json(path_or_buf=blazing_text_buffer, orient="records", lines=True)
```

8. 後補。

```python
s3r.Bucket(bucket).Object(os.path.join(prefix, 'blazing', bt_file)).put(Body=blazing_text_buffer.getvalue())

```

9. 後補。


```python
batch_output = f's3://{bucket}/{prefix}/blazing/'
batch_input = f's3://{bucket}/{prefix}/blazing/{bt_file}'
```

10. 对测试数据使用批处理转换器。

```python
bt_transformer = bt_model.transformer(
    instance_count=1,
    instance_type='ml.m5.2xlarge',
   strategy='MultiRecord',
   assemble_with='Line',
   output_path=batch_output
)

bt_transformer.transform(
    data=batch_input,
    data_type='S3Prefix',
    content_type='application/jsonlines',
    split_type='Line'
)

bt_transformer.wait(logs=True)
```

11. 检索来自 Amazon S3 的结果。

```python
obj = s3.get_object(
    Bucket=bucket, 
    Key=f'{prefix}/blazing/bt_input.json.out'
)
```

12. 後補。

```python
target_predicted = pd.read_json(
    io.BytesIO(obj['Body'].read()),
    lines=True
)
```

13. 後補。

```python
target_predicted.head()
```

14. 重新格式化结果，以便计算混淆矩阵和指标。

```python
def binary_convert(label):
    label = label[0].replace('__label__','')
    return int(label)

target_predicted_binary = target_predicted['label'].apply(binary_convert)
```

15. 後補。

```python
plot_confusion_matrix(test['label'], target_predicted_binary)
```

16. 後補。

```python
print_metrics(test['label'], target_predicted_binary)
```

## 使用Comprehend


在本节中，您将使用 Amazon Comprehend 来计算情绪。Amazon Comprehend 为您提供了正面和负面的结果，还显示了中立和喜忧参半的结果。Amazon Comprehend 是一项托管的服务，在使用它之前需要较少的文本处理。您无需处理本节中的任何文本。

查看  `test` DataFrame 中的数据是什么样的。


```python
test.head()
```

Amazon Comprehend 的使用可以像 API 调用一样简单直接。

以下单元格输出了来自 Amazon Comprehend 的前五个结果。


```python
import boto3
import json

comprehend = boto3.client(service_name='comprehend')
for n in range(5):
    text = test.iloc[n]['text']
    response = comprehend.detect_sentiment(Text=text, LanguageCode='en')
    sentiment = response['Sentiment']
    print(f'{sentiment} - {text}')

```

您可以启动预测任务来处理多个项目。必须将输入格式化为每行的单个输入，然后上载到 Amazon S3。文本的最大大小为 5120，因此 `str.slice(0,5000)`函数用于修剪长文本。


```python
# Upload test file minus label to S3
def upload_comprehend_s3_csv(filename, folder, dataframe):
    csv_buffer = io.StringIO()
    
    dataframe.to_csv(csv_buffer, header=False, index=False )
    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())

comprehend_file = 'comprehend_input.csv'
upload_comprehend_s3_csv(comprehend_file, 'comprehend', test['text'].str.slice(0,5000))
test_url = f's3://{bucket}/{prefix}/comprehend/{comprehend_file}'
print(f'Uploaded input to {test_url}')
```

数据上载到 Amazon S3 后，您可以使用 `start_sentiment_detection_jon`函数开始任务。



### 挑战：配置 Amazon Comprehend 任务参数

在下一个单元格中，配置 Amazon Comprehend 任务参数。
– 在__input_data_config__中 - 
  –實作S3Uri實作：将 *`<S3_INPUT_GOES_HERE> `* 替换为之前定义的 `test_uri`
  –實作InputFormat實作：将 *`<INPUT_FORMAT_GOES_HERE> `* 替换为 `ONE_DOC_PER_LINE`
– 在__output_data config__ 中-  
  –實作S3Uri實作：将 *`<S3_OUTPUT_GOES_HERE> `* 替换为 `s3_output_location`
  –實作data_access_role_arn實作：将 *`arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW `* 替换为*Lab详细信息*文件中的 Amazon Resource Name (ARN)


```python
input_data_config={
    'S3Uri': 'S3_INPUT_GOES_HERE',
    'InputFormat': 'INPUT_FORMAT_GOES_HERE'
},

output_data_config={
    'S3Uri': 'S3_OUTPUT_GOES_HERE'
},
data_access_role_arn = 'arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW'

### BEGIN_SOLUTION
input_data_config={
    'S3Uri': test_url,
    'InputFormat': 'ONE_DOC_PER_LINE'
}
output_data_config={
    'S3Uri': s3_output_location
}
data_access_role_arn = 'arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW'
### END_SOLUTION
```

现在，您已定义了任务参数，可以开始情绪检测任务。


```python
response = comprehend.start_sentiment_detection_job(
    InputDataConfig=input_data_config,
    OutputDataConfig=output_data_config,
    DataAccessRoleArn=data_access_role_arn,
    JobName='movie_sentiment',
    LanguageCode='en'
)

print(response['JobStatus'])
```

以下单元格将循环进行，直到任务结束。（这个步骤可能需要花几分钟的时间。）


```python
import time
job_id = response['JobId']
while True:
    job_status=(comprehend.describe_sentiment_detection_job(JobId=job_id))
    if job_status['SentimentDetectionJobProperties']['JobStatus'] in ['COMPLETED','FAILED']:
        break            
    else:
        print('.', end='')
    time.sleep(15)
print((comprehend.describe_sentiment_detection_job(JobId=job_id))['SentimentDetectionJobProperties']['JobStatus'])
```

任务完成后，您可以通过调用 `describe_sentiment_detection_job`函数返回任务的详细信息。


```python
output=(comprehend.describe_sentiment_detection_job(JobId=job_id))
print(output)
```

在 實作OutputDataConfig實作 部分，您应该会看到 `S3Uri`。提取该 URI 将为您提供必须从 Amazon S3 下载的文件。您可以使用结果来计算指标，方式与使用算法计算批处理转换结果的方式相同。


```python
comprehend_output_file = output['SentimentDetectionJobProperties']['OutputDataConfig']['S3Uri']
comprehend_bucket, comprehend_key = comprehend_output_file.replace("s3://", "").split("/", 1)

s3r = boto3.resource('s3')
s3r.meta.client.download_file(comprehend_bucket, comprehend_key, 'output.tar.gz')

# Extract the tar file
import tarfile
tf = tarfile.open('output.tar.gz')
tf.extractall()
```

应将提取的文件命名为 __output__。阅读此文件中的行。


```python
import json
data = ''
with open ('output', "r") as myfile:
    data = myfile.readlines()
```

将这些行添加到数组中。


```python
results = []
for line in data:
    json_data = json.loads(line)
    results.append([json_data['Line'],json_data['Sentiment']])
```

将数组转换为 Pandas DataFrame。


```python
c = pd.DataFrame.from_records(results, index='index', columns=['index','sentiment'])
c.head()
```

结果包含 實作NEGATIVE實作（负面）、實作POSITIVE實作（正面）、實作NEUTRAL實作（中立）和 實作MIXED實作（喜忧参半），而不是数值。要将这些结果与测试数据进行比较，可将它们映射到数值，如以下单元格所示。返回结果中的索引也是无序的。 `sort_index`函数应解决这个问题。


```python
class_mapper = {'NEGATIVE':0, 'POSITIVE':1, 'NEUTRAL':2, 'MIXED':3}
c['sentiment']=c['sentiment'].replace(class_mapper)
c = c.sort_index()
c.head()
```


```python
# Build list to compare for Amazon Comprehend
test_2 = test.reset_index()
test_3 = test_2.sort_index()
test_labels = test_3.iloc[:,2]
```

您可以使用 `plot_confusion_matrix`函数显示混淆矩阵。由于 Amazon Comprehend 的结果还包含__mixed__ 和 __neutral__，因此图表会有所不同。


```python
plot_confusion_matrix(test_labels, c['sentiment'])
```

用于打印指标的现有函数无法正常工作，因为您的数据维度太多。以下代码单元格将计算相同的值。


```python
cm = confusion_matrix(test_labels, c['sentiment'])

TN = cm[0,0]
FP = cm[0,1]
FN = cm[1,0]
TP = cm[1,1]

Sensitivity  = float(TP)/(TP+FN)*100
# Specificity or true negative rate
Specificity  = float(TN)/(TN+FP)*100
# Precision or positive predictive value
Precision = float(TP)/(TP+FP)*100
# Negative predictive value
NPV = float(TN)/(TN+FN)*100
# Fall out or false positive rate
FPR = float(FP)/(FP+TN)*100
# False negative rate
FNR = float(FN)/(TP+FN)*100
# False discovery rate
FDR = float(FP)/(TP+FP)*100
# Overall accuracy
ACC = float(TP+TN)/(TP+FP+FN+TN)*100

print(f"Sensitivity or TPR: {Sensitivity}%")    
print(f"Specificity or TNR: {Specificity}%") 
print(f"Precision: {Precision}%")   
print(f"Negative Predictive Value: {NPV}%")  
print( f"False Positive Rate: {FPR}%") 
print(f"False Negative Rate: {FNR}%")  
print(f"False Discovery Rate: {FDR}%" )
print(f"Accuracy: {ACC}%") 
```

# 恭喜！

您已经完成了本Lab内容，现在可以按照Lab指南中的说明结束本Lab内容。

*©2023 Amazon Web Services, Inc. 或其联属公司。保留所有权利。未经 Amazon Web Services, Inc. 事先书面许可，不得复制或转载本文的部分或全部内容。禁止因商业目的复制、出借或出售本文。所有商标均为各自所有者的财产。*

