# Lab 4.1ï¼šå¯¦ä½œæƒ…æ„Ÿåˆ†æ Sentiment Analysis

_Version: 02.14.2023_

<br>

## èªªæ˜

1. é€™å€‹ Lab ä½¿ç”¨ `IMDB` è³‡æ–™é›†é€²è¡Œæƒ…ç·’åˆ†æï¼Œç›®çš„æ˜¯è¨“ç·´ä¸€å€‹æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼Œç”¨æ–¼åˆ†æé›»å½±è©•è«–çš„æƒ…ç·’ï¼Œä¸¦èƒ½å¤ å°è©•è«–é€²è¡Œæ¨ç†ï¼Œåˆ¤æ–·å…¶ç‚ºæ­£é¢é‚„æ˜¯è² é¢ã€‚

<br>

2. è³‡æ–™é›†ä¸­åŒ…å« `50,000` æ¢å·²æ¨™è¨˜ç‚ºæ­£é¢æˆ–è² é¢çš„é›»å½±è©•è«–çš„æ–‡æœ¬ï¼Œç™¼å¸ƒæ–¼ `2011/06`ï¼Œè©³ç´°è³‡è¨Šå¯åƒè€ƒ [é€£çµ](http://ai.stanford.edu/~amaas/data/sentiment/)ï¼›é»æ“Šç•«é¢ä¸­çš„é€£çµå¯ä¸‹è¼‰ã€‚

    ![](images/img_32.png)

<br>

3. å¦å¤–ï¼Œæ­¤è³‡æ–™é›†çš„æƒ…ç·’æ¨™ç±¤ï¼Œ`1` è¡¨ç¤ºæ­£é¢ã€`0` è¡¨ç¤ºè² é¢ã€‚

<br>

## æ­¥éª¤æ¦‚è¿°

1. å®‰è£å¥—ä»¶ã€‚

<br>

2. è®€å–è³‡æ–™é›†ã€‚

<br>

3. åŸ·è¡Œæ¢ç´¢æ€§è³‡æ–™åˆ†æã€‚

<br>

4. åŸ·è¡Œç¬¬ä¸€æ¬¡å‚³éï¼šæœ€ä½ç¨‹åº¦çš„è™•ç†ã€‚

<br>

5. åŸ·è¡Œç¬¬äºŒæ¬¡å‚³éï¼šæ¨™æº–åŒ–æ–‡æœ¬ã€‚

<br>

6. å„ªåŒ–è¶…åƒæ•¸ã€‚

<br>

7. ä½¿ç”¨ BlazingTextã€‚

<br>

8. ä½¿ç”¨ Amazon Comprehendã€‚

<br>

## é–‹å§‹

_ä¸»è¦åœ¨æœ¬åœ°é€²è¡Œé–‹ç™¼ï¼Œéƒ¨åˆ†ä½œæ¥­å¿…é ˆåœ¨é›²ç«¯æ“ä½œ_

<br>

1. é¿å…ç‰ˆæœ¬è¡çªï¼Œå»ºç«‹æ–°çš„è™›æ“¬ç’°å¢ƒï¼›ä¸¦å•Ÿå‹• VSCode ç·¨è¼¯ç’°å¢ƒè®Šæ•¸ã€‚

    ```bash
    cd ~/Documents/PythonVenv && python -m venv envAWS2
    code ~/.zshrc 
    ```

<br>

2. å¯«å…¥ä¸¦å„²å­˜è·¯å¾‘å¾Œï¼Œå¥—ç”¨è®Šæ›´ã€‚

    ```bash
    source ~/.zshrc
    ```

<br>

3. åœ¨æ¡Œé¢å»ºç«‹å°ˆæ¡ˆè³‡æ–™å¤¾ `_test_`ï¼Œä¸¦æ·»åŠ é è¨­æ–‡ä»¶å¾Œï¼Œå•Ÿå‹• VSCodeã€‚

    ```bash
    mkdir -p ~/Desktop/_test_ && cd ~/Desktop/_test_
    touch ex01.ipynb .env .gitignore
    echo ".env" >> .gitignore
    code .
    ```

<br>

4. åœ¨ `.env` è²¼ä¸Š Lab é¦–é çš„ `AWS CLI` å…§å®¹ä¸åŒ…å« `[default]` éƒ¨åˆ†ã€‚

    ![](images/img_33.png)

<br>

5. ç·Šæ¥è‘—åœ¨ `.env` ä¸‹æ–¹è²¼ä¸Šä»¥ä¸‹å…§å®¹ï¼Œå®Œæˆ AWS CLI è¨­ç½®ã€‚

    ```bash
    AWS_ACCESS_KEY_ID=${aws_access_key_id}
    AWS_SECRET_ACCESS_KEY=${aws_secret_access_key}
    AWS_SESSION_TOKEN=${aws_session_token}
    AWS_DEFAULT_REGION=us-east-1
    ```

<br>

6. å‡å¦‚åœ¨çµ‚ç«¯æ©Ÿä¸­é‹è¡Œï¼Œå‰‡åœ¨è²¼ä¸Š AWS CIL ç›¸é—œè³‡æ–™å¾Œï¼Œé‹è¡Œä»¥ä¸‹æŒ‡ä»¤ã€‚

    ```bash
    export AWS_ACCESS_KEY_ID=$aws_access_key_id
    export AWS_SECRET_ACCESS_KEY=$aws_secret_access_key
    export AWS_SESSION_TOKEN=$aws_session_token
    ```

<br>

## å®‰è£…å¥—ä»¶

_å¯åœ¨ç­†è¨˜æœ¬æˆ–æ˜¯çµ‚ç«¯æ©Ÿä¸­åŸ·è¡Œ_

<br>

1. åœ¨ç­†è¨˜æœ¬ä¸­å¯ä½¿ç”¨é­”æ³•æ–¹æ³•å®‰è£å¥—ä»¶ã€‚

    ```python
    !pip install --upgrade pip
    !pip install --upgrade boto3
    !pip install --upgrade scikit-learn
    !pip install --upgrade sagemaker
    !pip install --upgrade nltk
    !pip install --upgrade seaborn
    ```

<br>

2. åœ¨æœ¬åœ°é‹è¡Œå‰‡å»ºè­°åœ¨çµ‚ç«¯æ©Ÿä¸­å®‰è£ã€‚

    ```bash
    python -m pip install --upgrade pip awscli boto3 scikit-learn  nltk seaborn python-dotenv
    ```

<br>

## æº–å‚™å·¥ä½œ

_ç·¨è¼¯è…³æœ¬_

<br>

1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸ã€‚

    ```python
    from dotenv import load_dotenv
    import os

    # è¼‰å…¥ .env æ–‡ä»¶
    load_dotenv()
    ```

    ![](images/img_13.png)

<br>

2. é©—è­‰å¸³è™Ÿï¼Œé€™å€‹å¸³è™Ÿæ¯æ¬¡é‡å•Ÿ Lab ä¹‹å¾Œæ˜¯æœƒè®Šå‹•çš„ï¼Œå‹™å¿…ç¢ºä¿è¼¸å‡ºçš„å¸³è™Ÿèˆ‡ Lab å–å¾—ç›¸åŒï¼Œè—‰æ­¤ç¢ºèª `.env` è¨­ç½®æ­£ç¢ºã€‚

    ```python
    import boto3

    # åˆå§‹åŒ– STS å®¢æˆ¶ç«¯
    sts_client = boto3.client('sts')

    # ç²å–ç•¶å‰å¸³æˆ¶çš„ Account ID
    account_id = sts_client.get_caller_identity()["Account"]
    print("ç•¶å‰çš„ Account ID:", account_id)
    ```

    ![](images/img_14.png)

<br>

## åœ¨æœ¬åœ°é‹ä½œ Sagemaker

_ç‰¹åˆ¥æ³¨æ„ï¼Œåœ¨æœ¬åœ°é‹è¡Œ `Sagemaker` æœƒæœ‰è«¸å¤šé™åˆ¶ï¼Œä»¥ä¸‹æœƒè©¦åœ–ç¹éé™åˆ¶ä¾†é€²è¡Œé‹ä½œã€‚_

<br>

1. å®‰è£å¥—ä»¶ã€‚

    ```bash
    python -m pip install sagemaker
    ```

<br>

2. å°å…¥ `sagemaker` èˆ‡ `Lab` æ“ä½œæœ‰é—œçš„åº«ï¼›å¾è¼¸å‡ºçš„è¨Šæ¯å¯çŸ¥ï¼Œ `SDK` æ²’æœ‰å¾é€™äº›ä½ç½®æ‡‰ç”¨é è¨­é…ç½®ï¼Œä½†é€™ä¸å½±éŸ¿ `SDK` çš„æ­£å¸¸é‹è¡Œï¼Œå¯ä»¥å¿½ç•¥é€™äº›è³‡è¨Šï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œé€™åœ¨ Lab é‹è¡Œä¹Ÿæœƒé¡¯ç¤ºç›¸åŒè¨Šæ¯ã€‚

    ```python
    import sagemaker
    from sagemaker.estimator import Estimator
    from sagemaker import get_execution_role
    ```

    ![](images/img_15.png)

<br>

## æŒ‡å®šè§’è‰²

1. åœ¨é›²ç«¯é‹ä½œæ™‚ï¼Œå¯ç›´æ¥æŒ‡å®šè§’è‰²ï¼Œå°‡ä»¥ä¸‹ä»£ç¢¼æ›¿ç‚ºæŒ‡å®šè§’è‰²ä¸¦å¯«åœ¨ CELL ä¸­å³å¯ã€‚

    ```python
    role = "<arn:aws:iam::XX-å…±12ç¢¼-XXX:role/è‡ªå·±çš„-SageMaker-åŸ·è¡Œè§’è‰²>"
    ```

<br>

2. åŸºæ–¼è‡ªå‹•åŒ–çš„è€ƒé‡ï¼Œåœ¨é›²ç«¯å¯é€éå‡½æ•¸å–å¾—è§’è‰²ä¸¦å­˜å…¥è®Šæ•¸ä¸­ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œé€™åªåœ¨é›²ç«¯ SageMaker notebook ç’°å¢ƒä¸­é©ç”¨ï¼Œæœ¬åœ°èª¿ç”¨é€™å€‹å‡½æ•¸å¾—åˆ°çš„æœƒæ˜¯ Lab è§’è‰²ï¼Œè€Œä¸æ˜¯é‡å° SageMaker è³¦äºˆçš„è§’è‰²ã€‚

    ```python
    role = get_execution_role()
    role
    ```

<br>

3. åœ¨æœ¬åœ°é‹è¡Œæœƒé¡¯ç¤ºè­¦å‘Šï¼Œé›–ç„¶ä¾èˆŠè¿”å›è§’è‰²çš„ ARNï¼Œä½†é€™ä¸¦é Lab è³¦äºˆæ¬Šé™çš„ Roleï¼›æ›å¥è©±èªªï¼Œç•¶å‰å–å¾—çš„è§’è‰²ä¸¦éé‹è¡Œæœ¬è¡Œæ¡ˆæ‰€è™›è¦çš„ï¼Œä¹‹å¾Œæœƒå¦åšè™•ç†ï¼Œé€™è£¡å…ˆèªªæ˜é€™å€‹ç‹€æ³ã€‚

    ```bash
    role_local = get_execution_role()
    role_local
    ```

    ![](images/img_16.png)

<br>

## æŸ¥è©¢ä¸¦å–å¾—è¨­ç½®

_S3 Bucketã€IAM Roles_

<br>

1. å–å¾—åç¨±ä¸­åŒ…å« `labbucket` çš„ S3 bucketï¼Œä¸¦å­˜å…¥è®Šæ•¸ `bucket_name` ä¸­ï¼Œé€™å°‡æ‡‰ç”¨æ–¼å¾ŒçºŒçš„ä»£ç¢¼ä¸­ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œå› ç‚ºå®˜æ–¹ç¯„ä¾‹ä½¿ç”¨çš„è®Šæ•¸åç¨±æ˜¯ `bucket`ï¼Œé€™è£¡åŒæ™‚ä¿ç•™äº† `bucket_name` ä¸¦å­˜å…¥ `bucket`ï¼Œä¸€æ–¹é¢æ˜¯å°æ‡‰å®˜æ–¹ç¯„ä¾‹ï¼ŒäºŒæ–¹é¢æ˜¯é¿å…éŒ¯èª¤è¦†è“‹æ™‚å¯é€²è¡Œæ¢å¾©ã€‚

    ```python
    import boto3

    # åˆå§‹åŒ– S3 å®¢æˆ¶ç«¯
    s3_client = boto3.client('s3')

    # åˆ—å‡ºæ‰€æœ‰ S3 buckets ä¸¦ç¯©é¸åç¨±åŒ…å« 'labbucket' çš„
    try:
        response = s3_client.list_buckets()
        # åªå–å‡ºç¬¬ä¸€å€‹åç¨±åŒ…å« 'labbucket' çš„ S3 bucketï¼Œè‹¥ç„¡å‰‡è¿”å› None
        bucket_name = next((bucket['Name'] for bucket in response['Buckets'] if 'labbucket' in bucket['Name']), None)

        # é¡¯ç¤ºçµæœ
        if bucket_name:
            print("åŒ…å« 'labbucket' çš„ S3 Bucketï¼š", bucket_name)
        else:
            print("æ‰¾ä¸åˆ°åŒ…å« 'labbucket' çš„ S3 Bucketã€‚")
    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")

    # é…åˆå®˜æ–¹ç¯„ä¾‹ï¼Œå°‡å‘½åç‚º `bucket`
    bucket = bucket_name
    bucket
    ```

    ![](images/img_17.png)

<br>

2. é—œæ–¼é€™å€‹ Bucketï¼Œå¯é€²å…¥ S3 ä¸»æ§å°æŸ¥çœ‹ã€‚

    ![](images/img_50.png)

<br>

3. æŸ¥è©¢æœ‰å“ªäº› Rolesï¼Œå‰é¢æ­¥é©Ÿæ›¾å–å¾—çš„å°±æ˜¯å…¶ä¸­çš„ `voclabs`ã€‚

    ```python
    # åˆå§‹åŒ– IAM å®¢æˆ¶ç«¯
    iam_client = boto3.client('iam')

    # åˆ—å‡ºæ‰€æœ‰ IAM è§’è‰²çš„åç¨±
    try:
        roles = iam_client.list_roles()
        print("å¸³æˆ¶ä¸­çš„ IAM è§’è‰²åç¨±ï¼š")
        for role in roles['Roles']:
            print(role['RoleName'])
    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
    ```

    ![](images/img_18.png)

<br>

4. åŒæ¨£åœ¨ IAM ä¸»æ§å°ä¸­ä¹Ÿå¯æŸ¥çœ‹åˆ°é€™äº› Rolesã€‚

    ![](images/img_51.png)

<br>

5. æª¢æŸ¥æŒ‡å®š Bucket ä¸­æ˜¯å¦æœ‰è³‡æ–™å¤¾æˆ–æ–‡ä»¶ã€‚

    ```python
    def list_s3_structure(bucket_name, prefix='', level=0):
        s3_client = boto3.client('s3')
        result = s3_client.list_objects_v2(
            Bucket=bucket_name, Prefix=prefix, Delimiter='/'
        )
        # æ ¹æ“šå±¤ç´šç¸®é€²
        indent = '    ' * level
        has_content = False

        # åˆ—å‡ºè³‡æ–™å¤¾
        if 'CommonPrefixes' in result:
            has_content = True
            for folder in result['CommonPrefixes']:
                # é¡¯ç¤ºè³‡æ–™å¤¾åç¨±
                print(
                    f"{indent}ğŸ“ {folder['Prefix'].split('/')[-2]}"
                )
                # éæ­¸åˆ—å‡ºå­è³‡æ–™å¤¾
                list_s3_structure(
                    bucket_name, 
                    prefix=folder['Prefix'], 
                    level=level + 1
                )

        # åˆ—å‡ºæ–‡ä»¶
        if 'Contents' in result:
            has_content = True
            for file in result['Contents']:
                # é¿å…é‡è¤‡é¡¯ç¤ºè³‡æ–™å¤¾çš„ key
                if file['Key'] != prefix:
                    # é¡¯ç¤ºæ–‡ä»¶åç¨±
                    print(f"{indent}ğŸ“„ {file['Key'].split('/')[-1]}")

        # å¦‚æœæ²’æœ‰å…§å®¹ï¼Œå‰‡è¼¸å‡ºé€šçŸ¥
        if not has_content and level == 0:
            print(f"Bucket '{bucket_name}' ä¸­æ²’æœ‰ä»»ä½•å…§å®¹ã€‚")

    # èª¿ç”¨
    list_s3_structure(bucket_name)
    ```

    ![](images/img_57.png)

<br>

## æ’°å¯«è…³æœ¬

1. å°å…¥ä¸»è¦åº«ä¸¦ä¸‹è¼‰ NLTKã€‚

    ```python
    # æ–‡ä»¶ã€è¼¸å…¥è¼¸å‡ºå’Œæ•¸æ“šçµæ§‹
    import os, io, struct
    import numpy as np
    import pandas as pd
    from sklearn.metrics import (
        # è¨ˆç®— ROC AUC åˆ†æ•¸
        roc_auc_score,
        # è¨ˆç®— ROC æ›²ç·šçš„åº§æ¨™
        roc_curve,
        # è¨ˆç®— AUC å€¼
        auc,
        # è¨ˆç®—æ··æ·†çŸ©é™£
        confusion_matrix
    )
    # ç”¨æ–¼æ•¸æ“šè¦–è¦ºåŒ–çš„åº«
    import seaborn as sns
    import matplotlib.pyplot as plt
    # ç”¨æ–¼è™•ç†æ—¥æœŸå’Œæ™‚é–“çš„åº«
    from datetime import datetime

    # åŒ¯å…¥ NLTK åº«ä»¥é€²è¡Œè‡ªç„¶èªè¨€è™•ç†
    import nltk

    '''ä¸‹è¼‰ NLTK è³‡æ–™é›†'''
    # ä¸‹è¼‰å¥å­åˆ†å‰²æ‰€éœ€çš„è³‡æ–™é›†
    nltk.download('punkt')
    # ä¸‹è¼‰åœç”¨è©åˆ—è¡¨
    nltk.download('stopwords')
    # ä¸‹è¼‰è©æ€§æ¨™è¨»å™¨
    nltk.download('averaged_perceptron_tagger')
    # ä¸‹è¼‰ WordNet è©å…¸
    nltk.download('wordnet')
    ```

    ![](images/img_01.png)

<br>

2. å¯é€²å…¥æª”æ¡ˆç³»çµ±ä¸­æŸ¥çœ‹ï¼Œä½ç½®åœ¨ `~/nltk_data`ï¼›`corpora` ä¸­å­˜æ”¾å„ç¨®èªæ–™åº«ï¼Œä¹Ÿå°±æ˜¯å¤§é‡çš„æ–‡æœ¬æ•¸æ“šé›†ï¼›`sentiment` å­˜æ”¾èˆ‡æƒ…æ„Ÿåˆ†æç›¸é—œçš„æ¨¡å‹æˆ–è³‡æ–™é›†ï¼›`taggers` åŒ…å«å„ç¨®æ¨™è¨»å™¨æ¨¡å‹ï¼Œä¾‹å¦‚è©æ€§æ¨™è¨»å™¨ï¼›`tokenizers` åŒ…å«å„ç¨®åˆ†è©å™¨ï¼Œé€™äº›å·¥å…·å¯å°‡æ–‡æœ¬åˆ‡åˆ†ç‚ºè©å½™æˆ–å¥å­ã€‚

    ![](images/img_52.png)

<br>

## è‡ªè¨‚è¼”åŠ©å‡½æ•¸

_é€™åœ¨ä¹‹å¾Œæœƒèª¿ç”¨_

<br>

1. ç¹ªè£½æ··æ·†çŸ©é™£çš„ç†±åŠ›åœ–ã€‚

    ```python
    def plot_confusion_matrix(test_labels, target_predicted):
        # è¨ˆç®—æ··æ·†çŸ©é™£
        matrix = confusion_matrix(test_labels, target_predicted)
        # å°‡æ··æ·†çŸ©é™£è½‰æ›ç‚º DataFrame æ ¼å¼
        df_confusion = pd.DataFrame(matrix)
        # è¨­å®šé¡è‰²åœ–çš„é…è‰²æ–¹æ¡ˆ
        colormap = sns.color_palette("BrBG", 10)
        # ç¹ªè£½ç†±åŠ›åœ–ï¼Œä¸¦é¡¯ç¤ºæ•¸å€¼
        sns.heatmap(df_confusion, annot=True, fmt='.2f', cbar=None, cmap=colormap)
        # è¨­å®šåœ–è¡¨çš„æ¨™é¡Œ
        plt.title("Confusion Matrix")
        # è‡ªå‹•èª¿æ•´ä½ˆå±€
        plt.tight_layout()
        # è¨­å®š y è»¸æ¨™ç±¤ç‚º "True Class"ï¼ˆçœŸå¯¦é¡åˆ¥ï¼‰
        plt.ylabel("True Class")
        # è¨­å®š x è»¸æ¨™ç±¤ç‚º "Predicted Class"ï¼ˆé æ¸¬é¡åˆ¥ï¼‰
        plt.xlabel("Predicted Class")
        # é¡¯ç¤ºåœ–è¡¨
        plt.show()
    ```

<br>

2. è¨ˆç®—å’Œè¼¸å‡ºæ¨¡å‹çš„æ€§èƒ½æŒ‡æ¨™ã€‚

    ```python
    def print_metrics(test_labels, target_predicted_binary):
        # è¨ˆç®—æ··æ·†çŸ©é™£ä¸¦è§£å£“ç¸®æˆ TN, FP, FN, TP
        TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()
        # è¨ˆç®—éˆæ•åº¦ã€å‘½ä¸­ç‡ã€å¬å›ç‡æˆ–çœŸé™½æ€§ç‡
        Sensitivity = float(TP)/(TP+FN)*100
        # è¨ˆç®—ç‰¹ç•°æ€§æˆ–çœŸé™°æ€§ç‡
        Specificity = float(TN)/(TN+FP)*100
        # è¨ˆç®—ç²¾ç¢ºåº¦æˆ–æ­£é™½æ€§é æ¸¬å€¼
        Precision = float(TP)/(TP+FP)*100
        # è¨ˆç®—é™°æ€§é æ¸¬å€¼
        NPV = float(TN)/(TN+FN)*100
        # è¨ˆç®—éŒ¯èª¤ç‡æˆ–å‡é™½æ€§ç‡
        FPR = float(FP)/(FP+TN)*100
        # è¨ˆç®—å‡é™°æ€§ç‡
        FNR = float(FN)/(TP+FN)*100
        # è¨ˆç®—éŒ¯èª¤ç™¼ç¾ç‡
        FDR = float(FP)/(TP+FP)*100
        # è¨ˆç®—æ•´é«”æº–ç¢ºç‡
        ACC = float(TP+TN)/(TP+FP+FN+TN)*100

        '''å€‹ç›¸è¼¸å‡º'''
        # éˆæ•åº¦æˆ– TPR
        print(f"Sensitivity or TPR: {Sensitivity}%")    
        # ç‰¹ç•°æ€§æˆ– TNR
        print(f"Specificity or TNR: {Specificity}%") 
        # ç²¾ç¢ºåº¦
        print(f"Precision: {Precision}%")   
        # é™°æ€§é æ¸¬å€¼
        print(f"Negative Predictive Value: {NPV}%")  
        # å‡é™½æ€§ç‡
        print(f"False Positive Rate: {FPR}%") 
        # å‡é™°æ€§ç‡
        print(f"False Negative Rate: {FNR}%")  
        # éŒ¯èª¤ç™¼ç¾ç‡
        print(f"False Discovery Rate: {FDR}%" )
        # æ•´é«”æº–ç¢ºç‡
        print(f"Accuracy: {ACC}%")
    ```

<br>

## æ‰‹å‹•ä¸‹è¼‰æ•¸æ“šé›†

_IMDB çš„æ•¸æ“šé›†ä¸¦éä»¥ `imdb.csv` çš„å½¢å¼ç›´æ¥æä¾›ï¼Œå®˜æ–¹ç¯„ä¾‹æ˜¯ç›´æ¥é å…ˆæº–å‚™å¥½çš„ï¼Œé€™è£¡æ‰‹å‹•é€²è¡Œè¼‰ä¸¦åŠ ä»¥è™•ç†ã€‚_

<br>

1. ä¸‹è¼‰ä¸¦å°‡è³‡æ–™çµ„åˆè½‰æ›æˆ `CSV` æ ¼å¼ï¼›ç¨‹å¼ç¢¼ç”Ÿæˆçš„ `imdb.csv` åŒ…å«å…©åˆ— `review` å’Œ `sentiment`ï¼Œåˆ†åˆ¥è¨˜éŒ„ `è©•è«–æ–‡æœ¬` èˆ‡ `æ­£é¢æˆ–è² é¢çš„æ¨™è¨˜`ï¼Œé€™æ¨£çš„æ ¼å¼å¯ç”¨æ–¼æƒ…ç·’åˆ†æä»»å‹™ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œæ‰‹å‹•ä¸‹è¼‰å¾Œçš„åŸå§‹æ•¸æ“šé›†åœ¨æ¬„ä½åç¨±ä¸Šèˆ‡ Lab æä¾›çš„ä¸åŒï¼Œé€™åœ¨å¾ŒçºŒæ­¥é©Ÿæœƒé€²è¡Œè½‰æ›ã€‚

    ```python
    import urllib.request
    import tarfile

    # å®šç¾©ä¸‹è¼‰ URL å’Œè³‡æ–™å¤¾åç¨±
    url = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
    data_dir = "aclImdb"

    # ä¸‹è¼‰ IMDB è³‡æ–™é›†
    if not os.path.exists("aclImdb_v1.tar.gz"):
        print("Downloading IMDB dataset...")
        urllib.request.urlretrieve(url, "aclImdb_v1.tar.gz")
        print("Download complete.")

    # è§£å£“ç¸®è³‡æ–™é›†
    if not os.path.exists(data_dir):
        print("Extracting IMDB dataset...")
        with tarfile.open("aclImdb_v1.tar.gz", "r:gz") as tar_ref:
            tar_ref.extractall(".")
        print("Extraction complete.")

    # æº–å‚™è³‡æ–™ä¸¦è½‰æ›ç‚º CSV æ ¼å¼
    data = {"review": [], "sentiment": []}

    # è®€å–è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆä¸¦æ¨™è¨˜æ­£é¢/è² é¢
    for split in ["train", "test"]:
        for sentiment in ["pos", "neg"]:
            folder_path = os.path.join(data_dir, split, sentiment)
            # æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
            if os.path.exists(folder_path):
                for filename in os.listdir(folder_path):
                    file_path = os.path.join(folder_path, filename)
                    with open(file_path, "r", encoding="utf-8") as file:
                        review = file.read()
                        # æ­£é¢ç‚º 1ï¼Œè² é¢ç‚º 0
                        label = 1 if sentiment == "pos" else 0 
                        data["review"].append(review)
                        data["sentiment"].append(label)

    # è½‰æ›ç‚º DataFrame ä¸¦å­˜æˆ CSV
    df = pd.DataFrame(data)
    df.to_csv("imdb.csv", index=False)
    print("CSV file 'imdb.csv' created successfully.")
    ```

    ![](images/img_02.png)

<br>

2. åœ¨æœ¬åœ°å°ˆæ¡ˆè·¯å¾‘ä¸­å¯çœ‹åˆ°æ·»åŠ ä¸€å€‹ç›®éŒ„ã€ä¸‰å€‹æ–‡ä»¶ï¼Œå…¶ä¸­ `imdb.csv` ä¾¿æ˜¯ç¶“éè½‰æ›å¾Œçš„æ–°è³‡æ–™é›†ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œåœ¨é›²ç«¯ç’°å¢ƒä¸­ï¼Œè³‡æ–™é›†è¢«å­˜æ”¾åœ¨ `'../data/imdb.csv'` è·¯å¾‘ä¸­ã€‚

    ![](images/img_19.png)

<br>

## è®€å–æ•¸æ“šé›†

1. è®€å–ä¸¦é¡¯ç¤ºè³‡æ–™ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œåœ¨é€™è£¡å·²ç¶“å°‡æ•¸æ“šå­˜å…¥ `df`ï¼Œè³‡æ–™è¡¨æœ‰å…©å€‹æ¬„ä½ï¼Œåˆ†åˆ¥æ˜¯ `review` åŠ `sentiment`ã€‚

    ```python
    df = pd.read_csv('imdb.csv', header=0)
    df
    ```

    ![](images/img_03.png)

<br>

## æ¯”è¼ƒä¸¦è§€å¯Ÿæ•¸æ“š

_èˆ‡ç¯„ä¾‹æ•¸æ“šé›†æ¯”è¼ƒ_

<br>

1. å¯è‡ªå‹•åŒ–ç”Ÿæˆå®˜æ–¹ç¯„ä¾‹æ–‡ä»¶çš„é€£çµã€‚

    ```python
    # ç²å– SageMaker å®¢æˆ¶ç«¯
    sagemaker_client = boto3.client('sagemaker')

    # ç²å–æ‰€æœ‰ Notebook å¯¦ä¾‹
    notebook_instances = sagemaker_client.list_notebook_instances()

    # ç²å–ç•¶å‰ Notebook å¯¦ä¾‹åç¨±
    # å‡è¨­åªå­˜åœ¨ä¸€å€‹ Notebook å¯¦ä¾‹ï¼Œæ‚¨å¯ä»¥æ ¹æ“šéœ€è¦é€²è¡Œä¿®æ”¹
    notebook_instance_name = \
        notebook_instances['NotebookInstances'][0]['NotebookInstanceName']

    # è¨­å®š AWS å€åŸŸ
    aws_region = boto3.Session().region_name

    # è¨­å®šè·¯å¾‘å’Œæª”æ¡ˆåç¨±
    folder_path = 'data'
    file_name = 'imdb.csv'

    # è‡ªå‹•ç”Ÿæˆç¶²å€
    s3_url = \
        f"https://{notebook_instance_name}.notebook."\
        f"{aws_region}.sagemaker.aws/lab/tree/{folder_path}/{file_name}"

    # è¼¸å‡ºç”Ÿæˆçš„ URL
    print("ç”Ÿæˆçš„ CSV æ–‡ä»¶ç¶²å€:", s3_url)
    ```

    ![](images/img_47.png)

<br>

2. é»æ“Šé€£çµå¯é€²å…¥ Lab æŸ¥çœ‹æ•¸æ“šé›†ï¼Œé»æ“Šå³éµä¸‹è¼‰åˆ°æœ¬åœ°ï¼›è£œå……èªªæ˜ï¼Œé€™ä¹Ÿæ˜¯å¿«é€Ÿé€²å…¥ Sagemaker çš„è·¯å¾‘ã€‚

    ![](images/img_34.png)

<br>

3. ç‚ºäº†æ¯”å°å®˜æ–¹æ•¸æ“šèˆ‡è‡ªè¡Œä¸‹è¼‰æ•¸æ“šæ˜¯å¦å­˜åœ¨å·®ç•°ï¼Œæ¥è‘—å°‡ä¸‹è¼‰çš„æ–‡ä»¶æ›´åç‚º `imdb_0.csv` å¾Œæ‹–æ›³é€²å…¥å°ˆæ¡ˆè³‡æ–™å¤¾ä¸­ã€‚

    ![](images/img_24.png)

<br>

4. é€éä»£ç¢¼è§€å¯Ÿï¼Œçµæœé¡¯ç¤ºæ•¸æ“šçš„å…§å®¹èˆ‡æ¬„ä½ä¸ä¸€æ¨£ï¼Œä½†æ ¼å¼æ˜¯ç›¸åŒçš„ï¼Œä¸¦ä¸”è³‡æ–™ç­†æ•¸ `50,000` ä¹Ÿç›¸åŒã€‚

    ```python
    df_0 = pd.read_csv('imdb_0.csv', header=0)
    df_0
    ```

    ![](images/img_04.png)

<br>

5. åˆ—å‡ºå…©å€‹æ•¸æ“šçš„æ¬„ä½åç¨±ï¼Œçµæœé¡¯ç¤ºå®˜æ–¹æ•¸æ“šé›†çš„æ¬„ä½åç¨±æ˜¯ `text` å’Œ `label`ï¼Œè€Œä¸‹è¼‰æ•¸æ“šé›†çš„æ¬„ä½æ˜¯ `review`ã€`sentiment`ã€‚

    ```python
    print(df.head())
    print(df_0.head())
    ```

    ![](images/img_05.png)

<br>

6. å°‡ä¸‹è¼‰æ•¸æ“šé›† `imdb.csv` çš„æ¬„ä½åç¨±ä¿®æ”¹èˆ‡å®˜æ–¹è³‡æ–™é›†ä¸€è‡´ã€‚

    ```python
    # å°‡æ¬„ä½åç¨±é‡å‘½åç‚ºå®˜æ–¹æ ¼å¼
    df.rename(
        columns={"review": "text", "sentiment": "label"},
        inplace=True
    )

    # å°‡æ›´æ”¹å¾Œçš„ DataFrame å„²å­˜å› CSV
    df.to_csv("imdb.csv", index=False)

    print("æ¬„ä½åç¨±å·²ä¿®æ”¹ç‚ºèˆ‡å®˜æ–¹ä¸€è‡´ï¼Œä¸¦é‡æ–°å„²å­˜ç‚º 'imdb.csv'ã€‚")

    # é‡æ–°è®€å–æ›´æ–°å¾Œçš„æ–‡ä»¶
    df = pd.read_csv("imdb.csv")
    print("é‡æ–°è®€å–å¾Œçš„æ¬„ä½åç¨±ï¼š", df.columns.tolist())
    ```

    ![](images/img_20.png)

<br>

7. æ¯”å°æ•¸æ“šï¼Œæå–è‡ªè¡Œä¸‹è¼‰çš„æ•¸æ“šé›†å‰äº”ç­†é€²è¡ŒæŸ¥è©¢ï¼Œæ¯”å°è©²ç­†æ•¸æ“šæ˜¯å¦å­˜åœ¨æ–¼å®˜æ–¹è³‡æ–™é›†ä¸­ï¼Œä¸¦æ˜ç¢ºæŒ‡å‡ºæ˜¯è³‡æ–™é›†çš„å“ªä¸€ç­†ï¼›æ­¤æ­¥é©Ÿç”¨å·²ç¢ºèªæ˜¯å¦åƒ…åƒ…æ˜¯æ’åºå•é¡Œã€‚

    ```python
    # é‡æ–°è®€å–æ›´æ–°å¾Œçš„æ•¸æ“šé›†
    df = pd.read_csv("imdb.csv")

    # å–å‡ºè‡ªè¡Œä¸‹è¼‰çš„æ•¸æ“šé›†å‰äº”ç­†è³‡æ–™
    first_five_rows = df.head(5)

    # å»ºç«‹ä¸€å€‹ç©ºçš„åˆ—è¡¨ä¾†å„²å­˜åŒ¹é…çµæœ
    matches = []

    # éæ­·å‰äº”ç­†è³‡æ–™ï¼Œé€ç­†èˆ‡å®˜æ–¹æ•¸æ“šé›†æ¯”å°
    for index, row in first_five_rows.iterrows():
        match = df_0[
            (df_0['text'] == row['text']) & 
            (df_0['label'] == row['label'])
        ]
        # å¦‚æœåŒ¹é…åˆ°æ•¸æ“šï¼Œå°‡çµæœåŠ å…¥åˆ—è¡¨
        if not match.empty:
            matches.append(match)

    # é¡¯ç¤ºåŒ¹é…çš„åˆ—
    if matches:
        matched_df = pd.concat(matches)
        print("å‰äº”ç­†æ•¸æ“šåœ¨å®˜æ–¹æ•¸æ“šé›†ä¸­çš„åˆ—ï¼š")
        print(matched_df)
    else:
        print("åœ¨å®˜æ–¹æ•¸æ“šé›†ä¸­æ‰¾ä¸åˆ°èˆ‡å‰äº”ç­†è³‡æ–™åŒ¹é…çš„åˆ—ã€‚")
    ```

    ![](images/img_06.png)

<br>

## æ¢ç´¢æ•¸æ“š

_ä»¥ä¸‹æ˜¯å®˜æ–¹ç¯„ä¾‹ä¸­æ¢ç´¢æ•¸æ“šçš„ä»£ç¢¼ï¼Œå¯é‹è¡Œæ“ä½œã€‚_

<br>

1. æŸ¥çœ‹å‰é¢ `8` ç­†ã€‚

    ```python
    def show_eight_rows(df):
        return df.head(8)    

    print(show_eight_rows(df))
    ```

    ![](images/img_07.png)

<br>

2. æŸ¥çœ‹æ•¸æ“šçµæ§‹ã€‚

    ```python
    def show_data_shape(df):
        return df.shape

    print(show_data_shape(df))
    ```

    ![](images/img_08.png)

<br>

3. è³‡æ–™ä¸­æ­£é¢å’Œè² é¢å¯¦ä¾‹æ•¸é‡ï¼›å¯è‡ªè¡Œæ›¿æ›æ–‡ä»¶åç¨±ä¾†æŸ¥çœ‹ç¯„ä¾‹æ–‡ä»¶çš„å…§å®¹ã€‚

    ```python
    def show_data_instances(df):
        return df['label'].value_counts()

    print(show_data_instances(df))
    ```

    ![](images/img_09.png)

<br>

4. æª¢æŸ¥éºæ¼ç¼ºå¤±å€¼ã€‚

    ```python
    def show_missing_values(df):
        return df.isna().sum()
        

    print(show_missing_values(df))
    ```

    ![](images/img_10.png)

<br>

## é‹è¡Œç¬¬ä¸€æ¬¡å‚³é

_æœ€ä½ç¨‹åº¦çš„è™•ç†_

<br>

1. å°å…¥ `NLTK` å’Œ `re`ã€‚

    ```python
    import re
    from nltk.corpus import stopwords
    from nltk.stem import SnowballStemmer
    from nltk.tokenize import word_tokenize
    ```

<br>

2. æ‹†åˆ†è³‡æ–™é›†ï¼Œå°‡è³‡æ–™é›†ä¸­ `80ï¼…` æ•¸æ“šç”¨æ–¼è¨“ç·´ï¼Œå¦å¤–ç”¨æ–¼é©—è­‰èˆ‡æ¸¬è©¦çš„å„ä½” `10%`ã€‚

    ```python
    # train_test_split ç”¨æ–¼è³‡æ–™é›†çš„åˆ†å‰²
    from sklearn.model_selection import train_test_split

    def split_data(df):

        # å°‡è³‡æ–™é›†åˆ†å‰²ç‚ºè¨“ç·´é›†ã€æ¸¬è©¦ã€é©—è­‰é›†
        train, test_and_validate = train_test_split(
            # è¦åˆ†å‰²çš„è³‡æ–™é›†
            df,
            # é€™æ˜¯ `æ¸¬è©¦+é©—è­‰` çš„æ¯”ä¾‹
            test_size=0.2,
            # åˆ†å‰²å‰æ˜¯å¦éš¨æ©Ÿæ‰“äº‚è³‡æ–™
            shuffle=True,
            # è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œé€™å¯ç”¨æ–¼é‡ç¾åˆ†å‰²çµæœ
            random_state=324
        )
        
        # å°‡ `æ¸¬è©¦é›†` å†åˆ†å‰²ç‚º `æ¸¬è©¦é›†` å’Œ `é©—è­‰é›†`
        test, validate = train_test_split(
            # è¦åˆ†å‰²çš„è³‡æ–™é›†
            test_and_validate,
            # å°‡é›†åˆåˆ†å‰²å…©éƒ¨åˆ†å„ä½” `50%`ï¼Œå³æ¸¬è©¦å’Œé©—è­‰å„ä½”æ•´é«” `10%`
            test_size=0.5,
            shuffle=True,
            random_state=324
        )
        # è¿”å›ä¸‰å€‹å­é›†
        return train, validate, test
    ```

<br>

3. èª¿ç”¨å‰ä¸€å€‹æ­¥é©Ÿå»ºç«‹çš„å‡½æ•¸ï¼Œå°‡æ•¸æ“šæ‹†åˆ†ç‚ºä¸‰å€‹è³‡æ–™å­é›† `train`ã€`validate`ã€`test`ï¼Œä¸¦é€é `shape` å±¬æ€§æŸ¥çœ‹æ‹†åˆ†å¾Œçš„æ•¸æ“šçµæ§‹ã€‚

    ```python
    train, validate, test = split_data(df)
    print(train.shape)
    print(test.shape)
    print(validate.shape)
    ```

    ![](images/img_11.png)

<br>

4. æª¢æŸ¥æ¬„ä½å­˜åœ¨æ€§ï¼›åŸºæ–¼æ•¸æ“šæ˜¯ç¶“éè½‰æ›çš„ï¼Œé€™è£¡åšä¸€æ¬¡æª¢æŸ¥ç¢ºä¿è³‡æ–™ç„¡èª¤ã€‚

    ```python
    # å®šç¾©è¦æª¢æŸ¥çš„æ¬„ä½åç¨±
    required_columns = ['text', 'label']

    # æª¢æŸ¥æ¯å€‹è³‡æ–™é›†æ˜¯å¦åŒ…å«æ‰€éœ€çš„æ¬„ä½
    for name, dataset in zip(
        ['Train', 'Validate', 'Test'], 
        [train, validate, test]
    ):
        missing_columns = [
            col for col in required_columns if col not in dataset.columns
        ]
        
        if not missing_columns:
            print(f"{name} è³‡æ–™é›†åŒ…å«æ‰€æœ‰æ‰€éœ€çš„æ¬„ä½: {required_columns}")
        else:
            print(f"{name} è³‡æ–™é›†ç¼ºå°‘ä»¥ä¸‹æ¬„ä½: {missing_columns}")
    ```

    ![](images/img_21.png)

<br>

## é—œæ–¼é­”è¡“å‘½ä»¤ `%%time`

_ä»¥ä¸‹æ­¥é©Ÿé–‹å§‹æœƒä½¿ç”¨åˆ°ï¼Œåœ¨æ­¤å…ˆåšå€‹è£œå……èªªæ˜_

<br>

1. åœ¨ `Jupyter Notebook` ä¸­å¯é€éè©²æŒ‡ä»¤å°è©² cell çš„åŸ·è¡Œé€²è¡Œè¨ˆæ™‚ï¼Œé™¤å¯æ¸¬é‡æ•´å€‹ cell çš„åŸ·è¡Œæ™‚é–“ï¼Œä¹Ÿæœƒé¡¯ç¤º `CPU time` è™•ç†å™¨èŠ±è²»çš„æ™‚é–“ï¼Œå¦å¤– `Wall time` å‰‡æ˜¯å¾ cell é–‹å§‹åŸ·è¡Œåˆ°çµæŸæ‰€ç¶“éçš„å¯¦éš›æ™‚é–“ï¼ŒåŒ…å«æ‰€æœ‰çš„ç­‰å¾…æ™‚é–“ã€‚

    ```python
    %%time
    # é€™è£¡æ˜¯ä¸€äº›ç¨‹å¼ç¢¼
    ```

<br>

2. åŸ·è¡Œå®Œæ­¤ cell å¾Œï¼ŒJupyter Notebook æœƒé¡¯ç¤ºé€™æ®µç¨‹å¼ç¢¼çš„åŸ·è¡Œæ™‚é–“ã€‚`%%time` åªèƒ½ç”¨æ–¼ cell çš„ç¬¬ä¸€è¡Œï¼Œä¸¦ä¸”æœƒè¨ˆæ™‚æ•´å€‹ cell çš„åŸ·è¡Œã€‚å¦‚æœåªæƒ³è¨ˆæ™‚å–®è¡Œç¨‹å¼ç¢¼ï¼Œå¯ä»¥ä½¿ç”¨ `%time`ã€‚

<br>

## çµ„è£ `processing pipeline`

_`è™•ç†ç®¡é“` å°±æ˜¯å°‡æ•¸æ“šè™•ç†æ­¥é©Ÿé€£æ¥åœ¨ä¸€èµ·çš„æ–¹å¼ï¼Œä½¿å…¶å¯ç°¡åŒ–æ•¸æ“šæµçš„ç®¡ç†å’Œæ“ä½œï¼›å…¶ä¸­åŒ…å«å¤šå€‹è™•ç†æ­¥é©Ÿå¦‚æ•¸æ“šæ¸…æ´—ã€è½‰æ›ã€ç‰¹å¾µæå–ç­‰ï¼Œç„¶å¾Œå°‡å…¶éˆæ¥èµ·ä¾†å½¢æˆä¸€å€‹æ•´é«”æµç¨‹ã€‚_

<br>

1. å°æ–‡æœ¬æ•¸æ“šé€²è¡Œç‰¹å¾µæå–å’Œé è™•ç†ï¼Œä»¥ä¾¿å°‡å…¶è½‰æ›ç‚ºæ©Ÿå™¨å­¸ç¿’æ¨¡å‹å¯ä»¥æ¥å—çš„æ ¼å¼ã€‚

    ```python
    %%time

    # å°å…¥æ–‡æœ¬ç‰¹å¾µæå–çš„ CountVectorizer
    from sklearn.feature_extraction.text import CountVectorizer
    # å°å…¥ Pipelineï¼Œç”¨æ–¼å°‡å¤šå€‹è™•ç†æ­¥é©Ÿä¸²è¯
    from sklearn.pipeline import Pipeline
    # å°å…¥ ColumnTransformerï¼Œç”¨æ–¼é¸æ“‡å’Œè½‰æ›ç‰¹å®šçš„åˆ—
    from sklearn.compose import ColumnTransformer

    # å®šç¾©æ–‡æœ¬ç‰¹å¾µçš„åç¨±
    text_features = ['text']
    # å®šç¾©æ¨¡å‹çš„ç›®æ¨™è®Šé‡åç¨±
    model_target = 'label'

    # å»ºç«‹æ–‡æœ¬è™•ç†ç®¡é“ï¼ŒåŒ…æ‹¬ CountVectorizer ä»¥æå–æ–‡æœ¬ç‰¹å¾µ
    text_processor_0 = Pipeline([(
        'text_vect_0', 
        # è¨­ç½®æœ€å¤§ç‰¹å¾µæ•¸ç‚º 500
        CountVectorizer(max_features=500)
    )])

    # å®šç¾©æ•¸æ“šé è™•ç†å™¨ï¼Œå°‡æ–‡æœ¬è™•ç†ç®¡é“æ‡‰ç”¨æ–¼æŒ‡å®šçš„æ–‡æœ¬ç‰¹å¾µåˆ—
    data_preprocessor = ColumnTransformer([(
        'text_pre_0', 
        text_processor_0, 
        # æ‡‰ç”¨æ–¼ 'text' æ¬„ä½
        text_features[0]
    )])

    # è¼¸å‡ºè™•ç†å‰çš„æ•¸æ“šé›†å½¢ç‹€
    print(
        'Datasets shapes before processing: ', 
        train.shape, 
        validate.shape, 
        test.shape
    )

    # æ“¬åˆè¨“ç·´æ•¸æ“šä¸¦è½‰æ›æˆæ•¸å€¼çŸ©é™£
    train_matrix = data_preprocessor.fit_transform(train)
    # å°æ¸¬è©¦æ•¸æ“šé€²è¡Œè½‰æ›
    test_matrix = data_preprocessor.transform(test)
    # å°é©—è­‰æ•¸æ“šé€²è¡Œè½‰æ›
    validate_matrix = data_preprocessor.transform(validate)

    # è¼¸å‡ºè™•ç†å¾Œçš„æ•¸æ“šé›†å½¢ç‹€
    print(
        'Datasets shapes after processing: ', 
        train_matrix.shape, 
        validate_matrix.shape, 
        test_matrix.shape
    )
    ```

    ![](images/img_12.png)

<br>

## åœ¨é›²ç«¯è‡ªè¨‚ä¸Šå‚³å‡½æ•¸

_é€™æ˜¯åœ¨é›²ç«¯æ‰€ä½¿ç”¨çš„å‡½æ•¸_

<br>

1. å› ç‚ºè¦è¨“ç·´æ¨¡å‹ï¼Œå¿…é ˆä»¥æ­£ç¢ºçš„æ ¼å¼å°‡è³‡æ–™ä¸Šè¼‰åˆ° S3ï¼ŒXGBoost ä½¿ç”¨ `CSV` æª”æ¡ˆã€‚

    ```python
    # åˆå§‹åŒ– S3 è³‡æº
    s3_resource = boto3.Session().resource('s3')

    # å®šç¾©ä¸Šå‚³ CSV æ–‡ä»¶çš„å‡½æ•¸
    def upload_s3_csv(
        filename, folder, X_train, y_train, is_test=False
    ):
        # å»ºç«‹ä¸€å€‹ StringIO å°è±¡ï¼Œä½œç‚º CSV çš„ç·©è¡å€
        csv_buffer = io.StringIO()
        
        # å°‡ç‰¹å¾µçŸ©é™£è½‰æ›ç‚º float32 é¡å‹ï¼Œä¸¦å±•å¹³ç‚ºåˆ—è¡¨æ ¼å¼
        features = [
            t.toarray().astype('float32').flatten().tolist() 
            for t in X_train
        ]
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¸¬è©¦é›†
        if is_test:
            # å¦‚æœæ˜¯æ¸¬è©¦é›†ï¼Œç›´æ¥ä½¿ç”¨ç‰¹å¾µ
            temp_list = features
        else:
            # å¦‚æœä¸æ˜¯æ¸¬è©¦é›†ï¼Œå°‡æ¨™ç±¤æ·»åŠ åˆ°ç‰¹å¾µçš„ç¬¬ä¸€åˆ—
            temp_list = np.insert(
                features, 0, y_train['label'], axis=1
            )
        
        # å°‡æ•¸æ“šä¿å­˜ç‚º CSV æ ¼å¼åˆ°ç·©è¡å€
        np.savetxt(csv_buffer, temp_list, delimiter=',')
        
        # ä½¿ç”¨ S3 è³‡æºå°‡ CSV ä¸Šå‚³è‡³æŒ‡å®šçš„ S3 Bucket å’Œæ–‡ä»¶è·¯å¾‘
        s3_resource.Bucket(bucket).Object(
            # å°‡æ–‡ä»¶è·¯å¾‘çµ„åˆ
            os.path.join(prefix, folder, filename)
        # å°‡ CSV æ•¸æ“šä½œç‚ºæ–‡ä»¶å…§å®¹ä¸Šå‚³
        ).put(Body=csv_buffer.getvalue())
    ```

<br>

2. åœ¨å®˜æ–¹ç¯„ä¾‹ä¸­ï¼Œå°æ–¼ä¸Šå‚³ä½¿ç”¨ç¡¬ç·¨ç¢¼æŒ‡å®šäº† Bucketï¼Œä½†é€™æ˜¯æœƒè®Šå‹•çš„ï¼Œæ¯æ¬¡å•Ÿå‹• Lab éƒ½æœƒæœ‰æ–°çš„å€¼ã€‚

    ```python
    bucket = 'c133864a3391494l8261467t1w637423426529-labbucket-hcjcbnnncwhe'
    ```

<br>

3. æ‰€ä»¥å°‡å‰é¢æ­¥é©Ÿè‡ªå‹•åŒ–æ­¥é©Ÿå–å¾— Bucket åç¨± `bucket_name` å‚³çµ¦ `bucket`ï¼›é€™åœ¨ä¹‹å‰å·²ç¶“åšéä¸€æ¬¡ï¼Œä½†é¿å…å› ç‚ºé‹è¡Œä¸Šä¸€å€‹ä»£ç¢¼è€Œè¦†è“‹äº†è³‡æ–™ï¼›é€™è£¡å¯è§€å¯Ÿè¼¸å‡º `bucket` çš„æ˜¯å¦æ­£ç¢ºã€‚

    ```python
    bucket = bucket_name
    bucket
    ```

<br>

4. è¨­ç½®é€™æ¬¡å‚³éçš„æ–‡ä»¶åç¨±ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œé€™æ˜¯åœ¨ Lab æ“ä½œæ™‚æ‰€åšçš„è¨­ç½®ï¼Œåœ¨æœ¬åœ°é‹è¡Œæ™‚å°‡ç•¥ä½œä¿®æ”¹ä»¥ä¾¿å€åˆ†ï¼Œé€™æœƒåœ¨å¾ŒçºŒæ­¥é©Ÿé€²è¡Œã€‚

    ```python
    prefix='lab41'
    train_file='train-pass1.csv'
    validate_file='validate-pass1.csv'
    test_file='test-pass1.csv'
    ```

<br>

5. å‡å¦‚åœ¨é›²ç«¯é‹è¡Œï¼Œå¯èª¿ç”¨å‡½æ•¸å°‡æ•¸æ“šä¸Šå‚³åˆ° S3ã€‚

    ```python
    upload_s3_csv(train_file, 'train', train_matrix, train)
    upload_s3_csv(validate_file, 'validate', validate_matrix, validate)
    upload_s3_csv(test_file, 'test', test_matrix, test, True)
    ```

<br>

## åœ¨æœ¬åœ°é‹è¡Œ

_é¦–å…ˆé€²è¡Œä¸€å€‹ç°¡å–®çš„æ¸¬è©¦ï¼Œç¢ºèªé€™æ¨£çš„æ–¹å¼å¯ä»¥åœ¨ç•¶å‰æ¬Šé™é™åˆ¶ä¸‹å¯«å…¥ S3_

<br>

1. è‹¥åœ¨æœ¬åœ°é‹è¡Œï¼Œå°è¨­å®šç•¥ä½œä¿®æ”¹å¦‚ä¸‹ï¼›æª”åä¸è®Šï¼Œ`prefix` æ›´æ”¹ç‚º `lab41_local`ã€‚

    ```python
    prefix='lab41_local'
    train_file='train-pass1.csv'
    validate_file='validate-pass1.csv'
    test_file='test-pass1.csv'
    ```

<br>

## é—œæ–¼æ¬Šé™èªªæ˜

1. ç‰¹åˆ¥æ³¨æ„ï¼ŒLab æ‰€è³¦äºˆçš„è§’è‰²ä¸å…·å‚™ç›´æ¥å¯«å…¥æ•¸æ“šè‡³ S3 çš„æ¬Šé™ï¼Œè¦ç²å¾—å¯«å…¥æ¬Šé™ï¼Œå¿…é ˆé€é `SageMaker` çš„ `get_execution_role` å‡½æ•¸ä¾†å–å¾—è©²è§’è‰²çš„ ARNï¼Œä¸¦åœ¨ `SageMaker` ç’°å¢ƒä¸­é‹è¡Œè©²å‡½æ•¸ï¼›è‹¥åœ¨æœ¬åœ°ç’°å¢ƒåŸ·è¡Œå°‡è¿”å› Lab è§’è‰²çš„ ARNï¼Œè€Œä¸æ˜¯ SageMaker çš„ ARNï¼Œé€™å°‡é™åˆ¶æœ¬åœ°ç’°å¢ƒå° S3 çš„å­˜å–æ¬Šé™ã€‚

<br>

2. é‡å°ä¸Šè¿°æƒ…æ³ï¼Œæ¥ä¸‹ä¾†å°‡ä½¿ç”¨ `Presigned URL` ä¾†è™•ç†æª”æ¡ˆä¸Šå‚³ï¼Œæ­¤æ–¹æ³•éœ€é€éåœ¨ `SageMaker` ç’°å¢ƒä¸­ä½¿ç”¨å…·æœ‰å¿…è¦æ¬Šé™çš„ ARN æ‰€ç”Ÿæˆçš„ `Presigned URL` ä¾†å®Œæˆä¸Šå‚³ï¼Œé€™æ¨£åšå¯ä»¥åœ¨ç„¡æ³•ä¿®æ”¹ IAM è¨­ç½®ä¸”ç„¡æ³•ç²å¾— `PutObject` æ¬Šé™çš„æƒ…æ³ä¸‹ï¼Œé€éæœ¬åœ°çš„ Jupyter Notebook æˆåŠŸä¸Šå‚³æª”æ¡ˆè‡³ S3ï¼›é€™å€‹è®Šé€šçš„è§£æ±ºæ–¹æ¡ˆè®“ä½¿ç”¨è€…èƒ½å¤ åœ¨æœ‰é™çš„æ¬Šé™ç’°å¢ƒä¸­é€²è¡Œæª”æ¡ˆæ“ä½œã€‚

<br>

3. é€™å€‹ç”Ÿæˆçš„ `Presigned URL` åœ¨ç‰¹å®šæ™‚é–“å…§æœ‰æ•ˆï¼Œå¯ä¾æ“šéœ€æ±‚è¨­å®šçš„æœ‰æ•ˆæœŸé™ç‚ºå¹¾åˆ†é˜è‡³å¹¾å°æ™‚ã€‚

<br>

## æ¸¬è©¦ä¸Šå‚³æ•¸æ“šåˆ° S3

_åœ¨é›²ç«¯æ“ä½œ_

<br>

1. åœ¨ _é›²ç«¯_ é–‹å•Ÿæ–°çš„ç­†è¨˜æœ¬ï¼Œåœ¨å½ˆçª—ä¸­é¸æ“‡æ ¸å¿ƒç‚º `conda_python3`ã€‚

    ![](images/img_26.png)

<br>

2. å»ºç«‹é ç°½å URLï¼Œç¬¬ä¸€æ­¥å…ˆå–å¾— bucket åç¨±ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œé€™æ˜¯åœ¨é›²ç«¯é‹è¡Œã€‚

    ```python
    import boto3

    # åˆå§‹åŒ– S3 å®¢æˆ¶ç«¯
    s3_client = boto3.client('s3')

    # åˆ—å‡ºæ‰€æœ‰ S3 buckets ä¸¦ç¯©é¸åç¨±åŒ…å« 'labbucket' çš„
    try:
        response = s3_client.list_buckets()
        # åªå–å‡ºç¬¬ä¸€å€‹åç¨±åŒ…å« 'labbucket' çš„ S3 bucketï¼Œè‹¥ç„¡å‰‡è¿”å› None
        bucket_name = next((bucket['Name'] for bucket in response['Buckets'] if 'labbucket' in bucket['Name']), None)

        # é¡¯ç¤ºçµæœ
        if bucket_name:
            print("åŒ…å« 'labbucket' çš„ S3 Bucketï¼š", bucket_name)
        else:
            print("æ‰¾ä¸åˆ°åŒ…å« 'labbucket' çš„ S3 Bucketã€‚")
    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤: {e}")

    # é…åˆå®˜æ–¹ç¯„ä¾‹ï¼Œå°‡å‘½åç‚º `bucket`
    bucket = bucket_name
    bucket
    ```

    ![](images/img_25.png)

<br>

3. è‡ªå‹•å°‡å‰ä¸€å€‹æ­¥é©Ÿå–å¾—çš„ bucket åç¨±ç”Ÿæˆé ç°½å URLï¼Œå¦å¤– `key` è¨­ç½®ç‚ºæ–‡ä»¶åç¨± `test.txt`ï¼Œä»£è¡¨ä¸Šå‚³å¾Œçš„æ–‡ä»¶å°‡ä»¥è©²åç¨±å­˜æ”¾åœ¨ Bucket çš„æ ¹ç›®éŒ„ä¸­ã€‚

    ```python
    # æŒ‡å®šæ–‡ä»¶åç¨±
    s3_file_path = "test.txt"

    # ç”Ÿæˆé ç°½å URLï¼Œæœ‰æ•ˆæœŸè¨­å®šç‚º 1 å°æ™‚
    presigned_url = s3_client.generate_presigned_url(
        'put_object',
        Params={
            'Bucket': bucket_name, 
            'Key': s3_file_path
        },
        # è¨­ç½® URL æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        ExpiresIn=3600
    )

    print("é ç°½å URL:", presigned_url)
    ```

    ![](images/img_27.png)

<br>

4. å°‡é ç°½å URL å¯«å…¥æ–‡ä»¶ pre_url.txtã€‚

    ```python
    with open("pre_url.txt", "w") as file:
        file.write(presigned_url)

    print("å·²ç”Ÿæˆé ç°½å URL ä¸¦å„²å­˜è‡³ pre_url.txt æ–‡ä»¶ã€‚")
    ```

    ![](images/img_28.png)

<br>

5. ä¸Šå‚³ pre_url.txt è‡³ S3ï¼Œé€™æ˜¯ç‚ºäº†è¦è®“æœ¬åœ°é€²è¡Œè®€å–æˆ–ä¸‹è¼‰ã€‚

    ```python
    with open("pre_url.txt", "rb") as file_data:
        s3_client.upload_fileobj(
            file_data, 
            bucket_name, 
            "pre_url.txt"
        )

    print("å·²æˆåŠŸå°‡ pre_url.txt ä¸Šå‚³è‡³ S3ã€‚")
    ```

    ![](images/img_48.png)

<br>

6. é€²å…¥ Bucket ä¸­ç¢ºèªæ–‡ä»¶å·²ç¶“å¯«å…¥ã€‚

    ![](images/img_35.png)

<br>

7. åœ¨æœ¬åœ°èª¿ç”¨ä¹‹å‰å»ºç«‹çš„å‡½æ•¸ `list_s3_structure()` æŸ¥è©¢ã€‚

    ![](images/img_58.png)

<br>

## å–å¾— S3 çš„é ç°½å

_å›åˆ°æœ¬åœ°ç¹¼çºŒæ“ä½œ_

<br>

1. åœ¨æœ¬æ©ŸæŸ¥çœ‹ S3 ä¸­æ‰€å„²å­˜çš„ `Presigned URL`ã€‚

    ```python
    import requests

    pre_url_file_key = 'pre_url.txt'

    # å–å¾— S3 ä¸­æŒ‡å®šæ–‡ä»¶ç‰©ä»¶
    response = s3_client.get_object(
        Bucket=bucket_name, 
        Key=pre_url_file_key
    )
    # è®€å‡ºå…§å®¹ä¸¦å„²å­˜ç‚ºè®Šæ•¸ presigned_url
    presigned_url = response['Body'].read().decode('utf-8')

    # è¼¸å‡ºæŸ¥çœ‹
    print(presigned_url)
    ```

    ![](images/img_36.png)

<br>

2. åœ¨ç•¶å‰è·¯å¾‘ä¸­å»ºç«‹ `test.txt` æ–‡ä»¶ï¼Œä¸¦å°‡å­—ä¸² `æ¸¬è©¦é ç°½å URL` å¯«å…¥å…¶ä¸­ã€‚

    ```python
    with open("test.txt", "w", encoding="utf-8") as file:
        file.write("æ¸¬è©¦é ç°½å URL")

    print("æ–‡ä»¶ test.txt å·²æˆåŠŸå‰µå»ºä¸¦å¯«å…¥å…§å®¹ã€‚")
    ```

    ![](images/img_37.png)

<br>

3. ä½¿ç”¨ `requests` åº«ï¼Œé€šé `Presigned URL` å°‡æ–‡ä»¶ä¸Šå‚³è‡³ S3ã€‚

    ```python
    # æœ¬åœ°æ–‡ä»¶è·¯å¾‘
    local_file_path = "test.txt"

    # ä½¿ç”¨é ç°½å URL ä¸Šå‚³æ–‡ä»¶
    with open(local_file_path, 'rb') as file_data:
        response = requests.put(
            presigned_url,
            data=file_data
        )

    # ç¢ºèªä¸Šå‚³çµæœ
    if response.status_code == 200:
        print("æ–‡ä»¶æˆåŠŸä¸Šå‚³è‡³ S3")
    else:
        print("ä¸Šå‚³å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š", response.status_code)
        print("éŒ¯èª¤å…§å®¹ï¼š", upload_response.text)
    ```

    ![](images/img_38.png)

<br>

4. é€²å…¥ S3 æŸ¥çœ‹æ˜¯å¦ç¢ºå¯¦å¯«å…¥ï¼Œç¢ºèªå®Œæˆä»£è¡¨ä»¥ä¸Šç¨‹åºæ˜¯æ­£ç¢ºçš„ã€‚

    ![](images/img_39.png)

<br>

5. å¯é€éè…³æœ¬æŸ¥è©¢ç•¶å‰ S3 ä¸­å®Œæ•´çš„æ¨¹ç‹€çµæ§‹ï¼›èˆ‡ä¹‹å‰æŒ‡å®š Bucket ä¸åŒï¼Œé€™æ˜¯æŸ¥è©¢æ•´å€‹ S3 çš„çµæ§‹ã€‚

    ```python
    def list_s3_bucket_structure(bucket_name):
        s3 = boto3.resource('s3')
        bucket = s3.Bucket(bucket_name)
        print(f"Bucket: {bucket_name}")
        
        has_objects = False
        for obj in bucket.objects.all():
            print(" â””â”€â”€ ", obj.key)
            has_objects = True

        if not has_objects:
            print(" â””â”€â”€ <æ­¤ bucket ç‚ºç©º>")


    buckets = s3_client.list_buckets()["Buckets"]

    if buckets:
        for bucket in buckets:
            list_s3_bucket_structure(bucket["Name"])
    else:
        print("ç•¶å‰ç‚ºç©ºçš„ï¼šæ²’æœ‰ä»»ä½• S3 bucket")
    ```

    ![](images/img_49.png)

<br>

## æ­£å¼ä¸Šå‚³æ•¸æ“šåˆ° S3

_å›åˆ°é›²ç«¯ï¼›ç¢ºèªä»¥ä¸Šæ­¥é©Ÿå¯å®Œæˆæ–‡ä»¶çš„å¯«å…¥ï¼Œä»¥ä¸‹æ­£å¼å»ºç«‹é ç°½å URL ä¾†å¯«å…¥æ•¸æ“šé›†ã€‚_

<br>

1. å»ºç«‹å››å€‹é ç°½å URL ä¸¦å¯«å…¥åˆ° Bucket æ–‡ä»¶ä¸­ã€‚ 

    ```python
    import os

    # å®šç¾© Bucket åç¨±å’Œè·¯å¾‘
    prefix = 'lab41_local'
    train_file = 'train-pass1.csv'
    validate_file = 'validate-pass1.csv'
    test_file = 'test-pass1.csv'
    batch_in_file = 'batch-in.csv'

    def create_presigned_url(bucket, prefix, filename):
        # ç”Ÿæˆé ç°½å URLï¼Œæœ‰æ•ˆæœŸè¨­å®šç‚º 1 å°æ™‚
        presigned_url = s3_client.generate_presigned_url(
            'put_object',
            Params={
                'Bucket': bucket, 
                'Key': os.path.join(prefix, filename)
            },
            # è¨­ç½® URL æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
            ExpiresIn=3600  
        )
        return presigned_url

    # å‰µå»ºé ç°½å URL
    urls = {
        'train_file': create_presigned_url(
            bucket_name, prefix, train_file
        ),
        'validate_file': create_presigned_url(
            bucket_name, prefix, validate_file
        ),
        'test_file': create_presigned_url(
            bucket_name, prefix, test_file
        ),
        'batch_in_file': create_presigned_url(
            bucket_name, prefix, batch_in_file
        )
    }

    # å°‡é ç°½å URL å¯«å…¥åˆ° pre_url.txt
    s3_client.put_object(
        Bucket=bucket_name,
        Key='pre_url.txt',
        # å°‡ URL è½‰æ›ç‚ºå­—ä¸²æ ¼å¼
        Body=str(urls)  
    )

    print("é ç°½å URL å·²æˆåŠŸå¯«å…¥åˆ° pre_url.txt")
    ```

    ![](images/img_40.png)

<br>

## å›åˆ°æœ¬åœ°é›»è…¦ 

1. è®€å– S3 ä¸­çš„ `pre_url.txt` ä¸¦å–å…¶ä¸­çš„ `é ç°½å URL`ï¼›è§£æå‰çš„å®Œæ•´ JSON æ ¼å¼å…§å®¹å¦‚ä¸‹ã€‚

    ```python
    response = s3_client.get_object(
        Bucket=bucket_name, 
        Key=pre_url_file_key
    )
    response
    ```

    ![](images/img_41.png)

<br>

2. è§£æå…¶ä¸­çš„é ç°½åç¶²å€ï¼›å¯è®€å‡ºå››å€‹éµå€¼å°ã€‚

    ```python
    presigned_urls = eval(response['Body'].read().decode('utf-8'))
    presigned_urls
    ```

    ![](images/img_42.png)

<br>

3. ä½¿ç”¨ `é ç°½å URL` ä¸Šå‚³æ•¸æ“šï¼Œç´„ç•¥éœ€è¦ä¸€åˆ†é˜ï¼›ç‰¹åˆ¥æ³¨æ„ï¼Œé ç°½åä¸­çš„ `batch_in_file` è‡³æ­¤å°šæœªç”¨åˆ°ï¼›å¦å¤–ï¼Œåœ¨æ­¤é‚„é€²è¡Œäº† `å‹åˆ¥è½‰æ›` ä»¥ç¢ºä¿æ¨™ç±¤ç‚º `æ•´æ•¸é¡å‹`ï¼›æœ€é‡è¦çš„ï¼ŒåŒæ™‚å°‡ `label` ç§»åˆ°ç¬¬ä¸€æ¬„ï¼Œç¢ºä¿æ¨¡å‹è¨“ç·´æ™‚å¯è­˜åˆ¥åˆ°æ­£ç¢ºçš„æ¨™ç±¤ï¼Œå®Œæˆå¾Œä¸Šå‚³åˆ° S3ã€‚

    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer
    import io

    # è¨­ç½®å‘é‡åŒ–å™¨ï¼Œè¨­å®šæœ€å¤§ç‰¹å¾µæ•¸ç›®
    vectorizer = TfidfVectorizer(max_features=500)

    # å‘é‡åŒ–è¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦æ•¸æ“š
    train_matrix = vectorizer.fit_transform(train['text'])
    validate_matrix = vectorizer.transform(validate['text'])
    test_matrix = vectorizer.transform(test['text'])

    # å°‡å‘é‡åŒ–çš„æ•¸æ“šè½‰æ›ç‚º DataFrameï¼Œä¸¦é™„åŠ æ¨™ç±¤åˆ—
    train_df = pd.DataFrame(
        train_matrix.toarray(),
        columns=vectorizer.get_feature_names_out()
    )
    train_df['label'] = train['label'].values

    validate_df = pd.DataFrame(
        validate_matrix.toarray(), 
        columns=vectorizer.get_feature_names_out()
    )
    validate_df['label'] = validate['label'].values

    test_df = pd.DataFrame(
        test_matrix.toarray(), 
        columns=vectorizer.get_feature_names_out()
    )
    test_df['label'] = test['label'].values

    # ç¢ºèªæ¨™ç±¤åˆ—ç‚ºæ•´æ•¸é¡å‹
    train_df['label'] = train_df['label'].astype(int)
    validate_df['label'] = validate_df['label'].astype(int)
    test_df['label'] = test_df['label'].astype(int)

    # å°‡ label åˆ—ç§»å‹•åˆ°ç¬¬ä¸€æ¬„
    train_df = train_df[
        ['label'] + [col for col in train_df.columns if col != 'label']
    ]
    validate_df = validate_df[
        ['label'] + [col for col in validate_df.columns if col != 'label']
    ]
    test_df = test_df[
        ['label'] + [col for col in test_df.columns if col != 'label']
    ]

    # ä¸Šå‚³å‡½æ•¸ï¼šä½¿ç”¨é ç°½å URL è¦†è“‹ä¸Šå‚³æ–‡ä»¶
    def save_csv_to_url(dataframe, url):
        # å°‡ DataFrame å„²å­˜ç‚º CSV æ ¼å¼
        csv_buffer = io.StringIO()
        dataframe.to_csv(csv_buffer, index=False)

        # ä½¿ç”¨é ç°½å URL ä¸Šå‚³ CSV æ–‡ä»¶
        response = requests.put(url, data=csv_buffer.getvalue())
        
        if response.status_code == 200:
            print(f"æ–‡ä»¶æˆåŠŸä¸Šå‚³è‡³ S3ï¼š{url}")
        else:
            print(
                f"ä¸Šå‚³å¤±æ•—ï¼Œç‹€æ…‹ç¢¼ï¼š{response.status_code}, éŒ¯èª¤å…§å®¹ï¼š{response.text}"
            )

    # è¦†è“‹ä¸Šå‚³æ•¸æ“šé›†
    save_csv_to_url(train_df, presigned_urls['train_file'])
    save_csv_to_url(validate_df, presigned_urls['validate_file'])
    save_csv_to_url(test_df, presigned_urls['test_file'])
    ```

    ![](images/img_29.png)

<br>

4. ç¢ºèª label åˆ—åœ¨æ¯å€‹ DataFrame ä¸­çš„ä½ç½®ï¼Œé€™å°æ–¼å¾ŒçºŒçš„ `fit` æ­¥é©Ÿä¾†èªªå¾ˆé‡è¦ã€‚

    ```python
    print("è¨“ç·´é›† label æ¬„ä½ç½®:", list(train_df.columns).index('label'))
    print("é©—è­‰é›† label æ¬„ä½ç½®:", list(validate_df.columns).index('label'))
    print("æ¸¬è©¦é›† label æ¬„ä½ç½®:", list(test_df.columns).index('label'))
    ```

    ![](images/img_59.png)

<br>

## æŸ¥çœ‹ä¸Šå‚³ç¾æ³

1. å†æ¬¡é€éä»£ç¢¼æŸ¥è©¢ S3 ä¸­çš„ç‰©ä»¶ï¼Œé€™æ¬¡æœƒé¡¯ç¤ºç‰©ä»¶çš„å®¹é‡å¤§å°ã€‚

    ```python
    # æŸ¥è©¢ bucket ä¸­çš„ç‰©ä»¶
    response = s3_client.list_objects_v2(Bucket=bucket_name)

    # æª¢æŸ¥ bucket æ˜¯å¦æœ‰å…§å®¹
    if 'Contents' in response:
        print(f"Bucket '{bucket_name}' ä¸­çš„ç‰©ä»¶ï¼š")
        for obj in response['Contents']:
            print(f" - {obj['Key']} (å¤§å°: {obj['Size']} bytes)")
    else:
        print(f"Bucket '{bucket_name}' ç‚ºç©ºæˆ–ä¸å­˜åœ¨ä»»ä½•ç‰©ä»¶ã€‚")
    ```

    ![](images/img_22.png)

<br>

2. é€²å…¥ S3 æŸ¥çœ‹é€™å€‹å­è³‡æ–™å¤¾ `lab41_local`ï¼Œç¢ºå¯¦å·²å°‡ä¸‰å€‹æ•¸æ“šé›†ä¸Šå‚³ã€‚

    ![](images/img_23.png)

<br>

## è¨“ç·´ä¹‹å‰

_é€²å…¥é›²ç«¯ç’°å¢ƒï¼›å› ç‚ºåœ¨æ¥ä¸‹ä¾†çš„æ­¥é©Ÿä¸­éœ€è¦èª¿ç”¨ `get_execution_role()`ï¼Œæ‰€ä»¥å¿…é ˆåœ¨ Sagemaker ä¸­é‹è¡Œã€‚_

<br>

1. å–å¾— ARN ä¸¦å¯«å…¥ S3 Bucket æ–‡ä»¶ `arn_data.txt`ï¼Œé€™æ˜¯ç”¨æ–¼åœ¨æœ¬åœ°è®€å–ç”¨çš„ã€‚

    ```python
    import sagemaker

    # åˆå§‹åŒ– SageMaker å’Œ STS å®¢æˆ¶ç«¯
    sagemaker_session = sagemaker.Session()
    sts_client = boto3.client('sts')

    # ç²å–ç•¶å‰å¸³æˆ¶çš„è§’è‰² ARN
    role_arn = sagemaker.get_execution_role()

    # è¼¸å‡ºè§’è‰² ARN
    print("SageMaker Notebook ä¸­çš„è§’è‰² ARN:", role_arn)

    # å°‡è§’è‰² ARN å¯«å…¥åˆ° S3 ä¸­çš„æ–‡ä»¶
    bucket_name = bucket
    s3_file_key = 'arn_data.txt'

    # ä¸Šå‚³ ARN åˆ° S3
    s3_client = boto3.client('s3')
    s3_client.put_object(
        Bucket=bucket_name,
        Key=s3_file_key,
        Body=role_arn
    )

    print(
        f"è§’è‰² ARN å·²å¯«å…¥åˆ° S3 ä¸­çš„ {bucket_name}/{s3_file_key}"
    )
    ```

    ![](images/img_30.png)

<br>

2. å®Œæˆå¾Œï¼Œåœ¨ Bucket ä¸­å¯çœ‹åˆ° `arn_data.txt`ï¼Œé€™ä¾¿æ˜¯å„²å­˜äº† ARN è³‡è¨Šçš„æ–‡ä»¶ã€‚

    ![](images/img_43.png)

<br>

## å›åˆ°æœ¬åœ°é›»è…¦

1. åœ¨æœ¬åœ°è®€å– S3 ä¸­çš„ `arn_data.txt` æ–‡ä»¶ï¼Œä¸¦å°‡å…§å®¹å¯«å…¥è®Šæ•¸ `arn` ä¸­ã€‚

    ```python
    s3_file_key = 'arn_data.txt'

    # è®€å– S3 ä¸­çš„ arn_data.txt æ–‡ä»¶
    response = s3_client.get_object(
        Bucket=bucket_name, Key=s3_file_key
    )

    # å°‡æ–‡ä»¶å…§å®¹è®€å–åˆ°è®Šæ•¸ arn ä¸­
    role_arn = response['Body'].read().decode('utf-8')

    # è¼¸å‡ºç²å–çš„ ARN
    print("è®€å–çš„è§’è‰² ARN:", role_arn)
    ```

    ![](images/img_31.png)

<br>

2. æŸ¥çœ‹è§’è‰²çš„é™„åŠ æ”¿ç­–ã€‚

    ```python
    from botocore.exceptions import ClientError

    def list_role_policies(role_arn):
        # ç²å–è§’è‰²åç¨±
        role_name = role_arn.split('/')[-1]
        iam_client = boto3.client('iam')

        try:
            # ç²å–è§’è‰²çš„é™„åŠ æ”¿ç­–
            policies = iam_client.list_attached_role_policies(RoleName=role_name)
            print(f"è§’è‰² {role_name} é™„åŠ çš„æ”¿ç­–:")
            
            # å„²å­˜æ”¿ç­– ARN çš„åˆ—è¡¨
            policy_arns = []
            
            for policy in policies['AttachedPolicies']:
                print(
                    f"- {policy['PolicyName']} (ARN: {policy['PolicyArn']})"
                )
                # å°‡æ”¿ç­– ARN å„²å­˜åˆ°åˆ—è¡¨ä¸­
                policy_arns.append(policy['PolicyArn'])
            
            # è¿”å›æ”¿ç­– ARN åˆ—è¡¨
            return policy_arns

        except ClientError as e:
            print(f"ç²å–è§’è‰²æ”¿ç­–æ™‚å‡ºç¾éŒ¯èª¤: {e}")
            return None

    # èª¿ç”¨å‡½æ•¸ä¸¦ç²å–æ”¿ç­– ARN
    policy_arns = list_role_policies(role_arn)

    # è¼¸å‡ºæ”¿ç­– ARN
    if policy_arns:
        print("ç²å–åˆ°çš„æ”¿ç­– ARN:")
        for arn in policy_arns:
            print(arn)
    ```

    ![](images/img_44.png)

<br>

3. å°‡å…§å®¹å­˜å…¥ JSON æ–‡ä»¶ä¸­ä¾¿æ–¼è§€å¯Ÿï¼›å…¶ä¸­ç¬¬ä¸€å€‹æ”¿ç­–å…è¨±æ‰€æœ‰ SageMaker æ“ä½œã€å…è¨±æ‰€æœ‰ SageMaker Geospatial æ“ä½œã€æ¶µè“‹äº†å¤šç¨® S3 æ“ä½œåŒ…æ‹¬è®€å–å’Œå¯«å…¥ã€å…è¨±å°‡è§’è‰²å‚³éçµ¦ AWS æœå‹™ï¼›ä¸éç¬¬ä¸€å€‹æ”¿ç­–ä¸¦æ²’æœ‰æ˜ç¢ºåˆ—å‡ºå° Amazon Comprehend æˆ– Textract çš„æ¬Šé™ã€‚

    ```python
    import json

    policy_arn = policy_arns[0]
    policy = iam_client.get_policy(PolicyArn=policy_arn)
    policy_version = iam_client.get_policy_version(
        PolicyArn=policy_arn,
        VersionId=policy['Policy']['DefaultVersionId']
    )

    # å–å¾— policy æ–‡ä»¶å…§å®¹
    policy_document = policy_version['PolicyVersion']['Document']

    # å°‡ policy æ–‡ä»¶å…§å®¹ä¿å­˜åˆ° policy_1.json
    with open('policy_1.json', 'w') as file:
        json.dump(policy_document, file, indent=4)

    print(policy_document)
    ```

<br>

4. ç¬¬äºŒå€‹æ”¿ç­–å…è¨±æ‰€æœ‰ Amazon Comprehend æ“ä½œã€å…è¨± S3 è®€å–æ“ä½œã€å…è¨±ç²å–è§’è‰²å’Œå‚³éè§’è‰²çš„æ¬Šé™ï¼Œå¦å¤–ä¹Ÿå° Amazon Comprehend å’Œ Textract ç›´æ¥æˆæ¬Šã€‚

    ```python
    policy_arn = policy_arns[1]
    policy = iam_client.get_policy(PolicyArn=policy_arn)
    policy_version = iam_client.get_policy_version(
        PolicyArn=policy_arn,
        VersionId=policy['Policy']['DefaultVersionId']
    )

    # å–å¾— policy æ–‡ä»¶å…§å®¹
    policy_document = policy_version['PolicyVersion']['Document']

    # å°‡ policy æ–‡ä»¶å…§å®¹ä¿å­˜åˆ° policy_1.json
    with open('policy_2.json', 'w') as file:
        json.dump(policy_document, file, indent=4)

    print(policy_version['PolicyVersion']['Document'])
    ```

<br>

5. è§€å¯Ÿæœ¬åœ°è§’è‰²ï¼Œä¸¦ä¸å…·å‚™ç›¸é—œæ¬Šé™ã€‚

    ```python
    role_arn_local = sagemaker.get_execution_role()
    print(f'æœ¬åœ°çš„è§’è‰²ï¼š{role_arn_local}')
    list_role_policies(role_arn_local)
    ```

    ![](images/img_45.png)

<br>

## æº–å‚™è¨“ç·´

1. å®‰è£ `libomp`ï¼Œåœ¨æœ¬åœ°é‹è¡Œæ©Ÿå™¨å­¸ç¿’ç›¸é—œå°ˆæ¡ˆæ™‚ï¼Œé€™å€‹åº«æœƒåœ¨éœ€è¦å¤šç·šç¨‹ä¸¦è¡Œè¨ˆç®—çš„æƒ…æ³ä¸‹è¢«è‡ªå‹•èª¿ç”¨ï¼Œä¾‹å¦‚ä½¿ç”¨ `XGBoost`ã€`Scikit-Learn` æˆ–æ˜¯ `TensorFlow` ç­‰å¥—ä»¶æ™‚ã€‚

    ```bash
    brew install libomp
    ```

<br>

2. `libomp` æ˜¯ä¸€å€‹ç³»çµ±ç´šçš„åº«ï¼Œå¯é€éä»¥ä¸‹æŒ‡ä»¤æŸ¥çœ‹å®‰è£ç‹€æ³ã€‚

    ```bash
    brew info libomp
    ```

    ![](images/img_60.png)

<br>

3. å®‰è£å¥—ä»¶ `xgboost`ã€‚

    ```bash
    pip install xgboost
    ```

<br>

4. é©—è­‰å®‰è£ã€‚

    ```python
    import xgboost as xgb

    # è¼¸å‡º XGBoost çš„ç‰ˆæœ¬
    print(xgb.__version__)
    ```

    ![](images/img_53.png)

<br>

## è§€å¯Ÿ libomp é‹ä½œ

_å…ˆç•¥é_

<br>

## å»ºç«‹ XGBoost æ¨¡å‹ä¼°è¨ˆå™¨ `Estimator` 

_Gradient Boosting æ¼”ç®—æ³•_

<br>

1. å°å…¥åº«ã€‚ 

    ```python
    from sagemaker.image_uris import retrieve
    import xgboost as xgb
    ```

<br>

2. å»ºç«‹ä¼°è¨ˆå™¨æ‰€éœ€ä¸€èˆ¬åƒæ•¸ï¼›é¦–å…ˆè¨­å®šå®¹å™¨ï¼Œä¸¦å°‡å®¹å™¨ç‰ˆæœ¬æ”¹ç‚º `1.2-2`ã€‚

    ```python
    container = retrieve(
        'xgboost',
        boto3.Session().region_name, 
        # '1.0-1'
        '1.2-2'
    )
    ```

<br>

3. è¨­ç½® S3 è¼¸å‡ºè·¯å¾‘ï¼Œç‰¹åˆ¥æ³¨æ„ï¼Œé€™å€‹è¼¸å‡ºç›®éŒ„ç„¡éœ€è¨­ç½®é ç°½å URLã€‚ 

    ```python
    s3_output_location = f's3://{bucket_name}/{prefix}/output/'
    ```

<br>

4. å»ºç«‹ä¼°è¨ˆå™¨çš„è¶…åƒæ•¸ï¼Œå°æ–¼å®˜æ–¹ç¯„ä¾‹é€²è¡Œäº†ä¸€äº›ä¿®æ­£ï¼›ä¸»è¦æ˜¯ç§»é™¤ `silent` åƒæ•¸ï¼Œå› ç‚ºå¾ `XGBoost 1.0` ç‰ˆæœ¬èµ·ï¼Œ`silent` åƒæ•¸å·²è¢«ç§»é™¤ï¼Œä¸¦ç”± `verbosity` åƒæ•¸å–ä»£ï¼Œé€™æ˜¯ç”¨æ–¼è¨­ç½®æ—¥èªŒè¼¸å‡ºç´šåˆ¥ï¼Œè‹¥ä½¿ç”¨ `silent` æœƒå°è‡´ `KeyError`ï¼›å¦å¤–ï¼Œæ”¹è®Šè©•ä¼°æŒ‡æ¨™ï¼Œå°‡ `eval_metric` æ”¹ç‚º `logloss`ï¼Œé€™éƒ¨åˆ†åƒ…åƒ…æ˜¯æ¸¬è©¦ï¼Œä¹‹å¾Œå¯æ”¹å›ä¾†ã€‚

    ```python
    hyperparams = {
        "num_round": "42",
        "eval_metric": "logloss",
        #"eval_metric": "error",
        "objective": "binary:logistic",
        # "silent": 1,
        # 0: silent, 1: warning, 2: info, 3: debug
        "verbosity": 1,
        # è¨­å®šéš¨æ©Ÿç¨®å­
        "seed": 42
    }
    ```

<br>

5. å»ºç«‹ XGBoost ä¼°è¨ˆå™¨ã€‚

    ```python
    xgb_model = sagemaker.estimator.Estimator(
        container,
        role=role_arn,
        instance_count=1,
        instance_type='ml.m5.2xlarge',
        output_path=s3_output_location,
        hyperparameters=hyperparams,
        sagemaker_session=sagemaker.Session()
    )
    ```

<br>

## å»ºç«‹æ•¸æ“šè¼¸å…¥ç®¡é“

_å…ˆç¢ºèªæ–‡ä»¶å¯è¨ªå•ï¼Œç„¶å¾Œå»ºç«‹ç®¡é“_

<br>

1. æª¢æŸ¥æ–‡ä»¶ç¢ºå¯¦å¯è¨ªå•ã€‚

    ```python
    from botocore.exceptions import ClientError

    def check_s3_file_exists(bucket_name, key):
        s3_client = boto3.client('s3')
        try:
            s3_client.head_object(Bucket=bucket_name, Key=key)
            return True
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                print(f"æ–‡ä»¶ä¸å­˜åœ¨: s3://{bucket_name}/{key}")
                return False
            else:
                print(f"ç„¡æ³•è¨ªå•æ–‡ä»¶: s3://{bucket_name}/{key}, éŒ¯èª¤: {e}")
                return False

    # è¨­å®š S3 å­˜å„²æ¡¶å’Œè·¯å¾‘
    prefix = 'lab41_local'
    train_file = 'train-pass1.csv'
    validate_file = 'validate-pass1.csv'

    # æª¢æŸ¥è¨“ç·´å’Œé©—è­‰æ–‡ä»¶
    train_key = f"{prefix}/{train_file}"
    validate_key = f"{prefix}/{validate_file}"

    print("æª¢æŸ¥ S3 æ–‡ä»¶...")
    train_exists = check_s3_file_exists(bucket_name, train_key)
    validate_exists = check_s3_file_exists(bucket_name, validate_key)

    # çµæœ
    if train_exists:
        print(f"è¨“ç·´æ–‡ä»¶å¯è¨ªå•: s3://{bucket_name}/{train_key}")
    if validate_exists:
        print(f"é©—è­‰æ–‡ä»¶å¯è¨ªå•: s3://{bucket_name}/{validate_key}")
    ```

    ![](images/img_46.png)

<br>

2. è¨­å®šè¼¸å…¥ç®¡é“ `data_channels`ï¼›åŒ…å«å…©å€‹æ•¸æ“šç®¡é“ï¼Œåˆ†åˆ¥æ˜¯ `è¨“ç·´è³‡æ–™` èˆ‡ `é©—è­‰è³‡æ–™`ï¼›å®˜æ–¹ç¯„ä¾‹åˆ†åˆ¥å°‡å…©å€‹æ•¸æ“šæ–‡ä»¶æ”¾å…¥ `train` åŠ `validate` ä¹‹ä¸­ï¼Œé€™è£¡æš«æ™‚å…ˆæ”¾åœ¨ `prefix`ã€‚

    ```python
    train_channel = sagemaker.inputs.TrainingInput(
        # f's3://{bucket}/{prefix}/train/{train_file}',
        f's3://{bucket_name}/{prefix}/{train_file}',
        content_type='text/csv'
    )

    validate_channel = sagemaker.inputs.TrainingInput(
        # f's3://{bucket}/{prefix}/validate/{validate_file}',
        f's3://{bucket_name}/{prefix}/{validate_file}',
        content_type='text/csv'
    )
    # çµ„åˆç‚ºåƒæ•¸
    data_channels = {
        'train': train_channel, 
        'validation': validate_channel
    }
    ```

<br>

## è¨“ç·´æ¨¡å‹

1. èª¿ç”¨ `fit` é€²è¡Œæ¨¡å‹èª¿é©ï¼Œé€™å€‹æ­¥é©Ÿéœ€è¦ä¸€é»æ™‚é–“ã€‚

    ```python
    %%time

    xgb_model.fit(
        inputs=data_channels, 
        logs=False, 
        job_name='xgb-pass1-'+datetime.now().\
            strftime("%m-%d-%Y-%H-%M-%S")
    )
    ```

    ![](images/img_54.png)

<br>

2. åœ¨é€™æœ‰å€‹åœ°æ–¹å€¼å¾—æ³¨æ„ï¼Œé›–ç„¶ä¸¦æœªå»ºç«‹ `Output` çš„é ç°½å URLï¼Œä½†åœ¨ `fit` çš„æ™‚å€™ï¼Œå¯«å…¥ä¸¦ç„¡æ¬Šé™å•é¡Œã€‚

    ![](images/img_55.png)

<br>

3. é»æ“Šé€²å…¥ï¼Œå¯çœ‹åˆ°ä»¥ `job_name` å‘½åçš„å­ç›®éŒ„ã€‚

    ![](images/img_56.png)

<br>

4. æŸ¥è©¢çµæ§‹ã€‚

    ```python
    import boto3

    def list_s3_structure(bucket_name, prefix=''):
        s3_client = boto3.client('s3')
        result = s3_client.list_objects_v2(
            Bucket=bucket_name, Prefix=prefix, Delimiter='/'
        )
        
        # åˆ—å‡ºè³‡æ–™å¤¾
        if 'CommonPrefixes' in result:
            print("è³‡æ–™å¤¾:")
            for folder in result['CommonPrefixes']:
                print(f"- {folder['Prefix']}")
                # éæ­¸åˆ—å‡ºå­è³‡æ–™å¤¾
                list_s3_structure(bucket_name, prefix=folder['Prefix'])

        # åˆ—å‡ºæ–‡ä»¶
        if 'Contents' in result:
            print("æ–‡ä»¶:")
            for file in result['Contents']:
                print(f"- {file['Key']}")

    # èª¿ç”¨
    list_s3_structure(bucket_name)
    ```

<br>

## æ¥ä¸‹ä¾†

1. é¡¯ç¤ºä¾†è‡ªç›®å‰ XGBoost ä»»å‹™çš„æŒ‡æ¨™ã€‚

```python
sagemaker.analytics.TrainingJobAnalytics(
    xgb_model._current_job_name, 
    metric_names = ['train:error','validation:error']
).dataframe()
```

4. åˆå§‹çµæœä¼¼ä¹æ²’æœ‰å¹«åŠ©ã€‚ä½¿ç”¨ __test__ è³‡æ–™é›†è¨ˆç®—æ›´å¤šæŒ‡æ¨™ã€‚

```python
%%time

upload_s3_csv('batch-in.csv', 'batch-in', test_matrix, test, True)
batch_X_file='batch-in.csv'
batch_output = f's3://{bucket}/{prefix}/batch-out/'
batch_input = f's3://{bucket}/{prefix}/batch-in/{batch_X_file}'

xgb_transformer = xgb_model.transformer(
    instance_count=1,
    instance_type='ml.m5.2xlarge',
    strategy='MultiRecord',
    assemble_with='Line',
    output_path=batch_output
)

xgb_transformer.transform(
    data=batch_input,
    data_type='S3Prefix',
    content_type='text/csv',
    split_type='Line',
    job_name='xgboost-pass1'
)
xgb_transformer.wait(logs=False)
```

5. S3ã€‚
```python
s3 = boto3.client('s3')
obj = s3.get_object(
    Bucket=bucket, 
    Key=f'{prefix}/batch-out/batch-in.csv.out'
)
target_predicted = pd.read_csv(
    io.BytesIO(obj['Body'].read()),sep=',',
    names=['class']
)

def binary_convert(x):
    threshold = 0.5
    if x > threshold:
        return 1
    else:
        return 0

target_predicted_binary = target_predicted['class'].apply(binary_convert)
```

6. ä¹‹å¾Œèªªæ˜ã€‚

```python
plot_confusion_matrix(test['label'], target_predicted_binary)
```

7. ä¹‹å¾Œèªªæ˜ã€‚
```python
print_metrics(test['label'], target_predicted_binary)
```

## ç¬¬äºŒæ¬¡ä¼ é€’ï¼šæ ‡å‡†åŒ–æ–‡æœ¬


1. å¯åˆ é™¤æ‰€æœ‰éç´¢å¼•å­—ï¼Œä½†å¯èƒ½æƒ³ä¿ç•™å¯èƒ½å½±å“æƒ…ç»ªçš„éç´¢å¼•å­—ï¼Œä¾‹å¦‚ __not__ æˆ– __don't__ã€‚


```python
# Get a list of stopwords from the NLTK library
stop = stopwords.words('english')

def remove_stopwords(stopwords):
    # Implement this function
    excluding = ['against', 'not', 'don', 'don\'t','ain', 'are', 'aren\'t']
    ### BEGIN_SOLUTION
    excluding = ['against', 'not', 'don', 'don\'t','ain', 'are', 'aren\'t', 'could', 'couldn\'t',
             'did', 'didn\'t', 'does', 'doesn\'t', 'had', 'hadn\'t', 'has', 'hasn\'t', 
             'have', 'haven\'t', 'is', 'isn\'t', 'might', 'mightn\'t', 'must', 'mustn\'t',
             'need', 'needn\'t','should', 'shouldn\'t', 'was', 'wasn\'t', 'were', 
             'weren\'t', 'won\'t', 'would', 'wouldn\'t']
    ### END_SOLUTION
    return [word for word in stop if word not in excluding]

# New stopword list
stopwords = remove_stopwords(stop)

```

## æ·»åŠ æ¸…ç†æ­¥éª¤

1. æ›´æ–°ä»¥ä¸‹ `clean` å‡½æ•°ï¼Œä»¥å®Œæˆåˆ é™¤å‰å¯¼ç©ºæ ¼å’Œå°¾éšç©ºæ ¼ã€åˆ é™¤ä»»ä½• HTML æ ‡è®°ã€‚

```python
snow = SnowballStemmer('english')
def clean(sent):
    # Implement this function
    sent = sent.lower()
    sent = re.sub('\s+', ' ', sent)
    ### BEGIN_SOLUTION
    sent = sent.strip()
    sent = re.compile('<.*?>').sub('',sent)
    ### END_SOLUTION
    filtered_sentence = []
    
    for w in word_tokenize(sent):
        # You are applying custom filtering here. Feel free to try different things.
        # Check if it is not numeric, its length > 2, and it is not in stopwords
        if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  
            # Stem and add to filtered list
            filtered_sentence.append(snow.stem(w))
    final_string = " ".join(filtered_sentence) #final string of cleaned words
    return final_string
```

2. ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„å‡½æ•°åˆ›å»ºæ–°çš„æµ‹è¯•ã€éªŒè¯å’Œæµ‹è¯• DataFrameã€‚

```python
# Uncomment the next line and implement the function call to split_data
#train, validate, test = 

### BEGIN_SOLUTION
train, validate, test = split_data(df)
### END_SOLUTION

print(train.shape)
print(test.shape)
print(validate.shape)
```

3. ç®¡é“å·²æ›´æ–°ï¼Œä»¥åŒ…å«å¯¹ä¹‹å‰å®šä¹‰çš„æ¥è‡ª `CountVectorizer`çš„ `clean`å‡½æ•°çš„è°ƒç”¨ã€‚æ­¤å‡½æ•°éœ€è¦æ›´é•¿çš„è¿è¡Œæ—¶é—´ã€‚


```python
%%time

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

text_features = ['text']
model_target = 'label'

text_processor_0 = Pipeline([
    ('text_vect_0', CountVectorizer(preprocessor=clean, max_features=500))
])

data_preprocessor = ColumnTransformer([
    ('text_pre_0', text_processor_0, text_features[0])
])

print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)
train_matrix = data_preprocessor.fit_transform(train)
test_matrix = data_preprocessor.transform(test)
validate_matrix = data_preprocessor.transform(validate)
print('Datasets shapes after processing: ', train_matrix.shape, validate_matrix.shape, test_matrix.shape)
```

4. è®¾ç½®æ­¤ä¼ é€’çš„æ–‡ä»¶åã€‚


```python
prefix='lab41'
train_file='train_pass2.csv'
validate_file='validate_pass2.csv'
test_file='test_pass2.csv'
```

## å°†æ–‡ä»¶ä¸Šè½½åˆ° S3

1. ä½¿ç”¨ä¹‹å‰çš„ä»£ç å°†æ–°æ–‡ä»¶ä¸Šè½½åˆ° Amazon S3ã€‚

```python
upload_s3_csv(train_file, 'train', train_matrix, train)
upload_s3_csv(validate_file, 'validate', validate_matrix, validate)
upload_s3_csv(test_file, 'test', test_matrix, test, True)
```

## åˆ›å»ºä¼°ç®—å™¨å¹¶è®¾ç½®æ•°æ®é€šé“

1. ä½¿ç”¨ä¹‹å‰çš„ä»£ç è®¾ç½®ä¼°ç®—å™¨å’Œæ•°æ®é€šé“ã€‚

```python
%%time

container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')

hyperparams={
    "num_round":"42",
    "eval_metric": "error",
    "objective": "binary:logistic",
    "silent" : 1
}

xgb_model=sagemaker.estimator.Estimator(
    container,
    sagemaker.get_execution_role(),
    instance_count=1,
    instance_type='ml.m5.2xlarge',
    output_path=s3_output_location,
    hyperparameters = hyperparams,
    sagemaker_session=sagemaker.Session()
)

train_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/train/{train_file}',
    content_type='text/csv'
)

validate_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/validate/{validate_file}',
    content_type='text/csv'
)

data_channels = {
    'train': train_channel, 
    'validation': validate_channel
}

xgb_model.fit(
    inputs=data_channels, 
    logs=False, 
    job_name='xgb-pass2-'+datetime.now()\
        .strftime("%m-%d-%Y-%H-%M-%S")
)
```

2. å¾Œè£œã€‚

```python
sagemaker.analytics.TrainingJobAnalytics(
    xgb_model._current_job_name, 
    metric_names = ['train:error','validation:error']
).dataframe()
```

## åˆ›å»ºæ‰¹å¤„ç†è½¬æ¢å™¨ä»»åŠ¡

1. ä½¿ç”¨ä¹‹å‰çš„ä»£ç åˆ›å»ºä¸€ä¸ªè½¬æ¢å™¨ä»»åŠ¡ã€‚

```python
%%time

xgb_transformer = xgb_model.transformer(
    instance_count=1,
    instance_type='ml.m5.2xlarge',
    strategy='MultiRecord',
    assemble_with='Line',
    output_path=batch_output
)

xgb_transformer.transform(
    data=batch_input,
    data_type='S3Prefix',
    content_type='text/csv',
    split_type='Line')

xgb_transformer.wait(logs=False)
```

2. å¾Œè£œã€‚

```python
s3 = boto3.client('s3')
obj = s3.get_object(
    Bucket=bucket, 
    Key="{}/batch-out/{}".\
        format(prefix,'batch-in.csv.out')
)
target_predicted = pd.read_csv(
    io.BytesIO(obj['Body'].read()),
    sep=',',
    names=['class']
)

def binary_convert(x):
    threshold = 0.5
    if x > threshold:
        return 1
    else:
        return 0

target_predicted_binary = target_predicted['class'].apply(binary_convert)
```

3. å¾Œè£œã€‚

```python
plot_confusion_matrix(test['label'], target_predicted_binary)
```

4. å¾Œè£œã€‚

```python
print_metrics(test['label'], target_predicted_binary)
```

5. å°†åˆ›å»ºä¸€ä¸ªè¶…å‚æ•°ä¼˜åŒ–ä»»åŠ¡æ¥ä¼˜åŒ–æ¨¡å‹ã€‚

## åˆ›å»ºä¼°ç®—å™¨ç”¨äºä¼˜åŒ–

1. ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ªä¼°ç®—å™¨ç”¨äºä¼˜åŒ–ã€‚å–æ¶ˆæ³¨é‡Šå¹¶å®Œæˆä»¥ä¸‹ä¼°ç®—å™¨ä»£ç ã€‚

```python
# xgb = sagemaker.estimator.Estimator(....)
xgb = sagemaker.estimator.Estimator(
    container,
    role=sagemaker.get_execution_role(), 
    # make sure you have limit set for these instances
    instance_count= 1,
    instance_type='ml.m5.2xlarge', 
    output_path=f's3://{bucket}/{prefix}/output',
    sagemaker_session=sagemaker.Session()
)
```

2. å¾Œè£œã€‚

```python
xgb.set_hyperparameters(
    eval_metric='error',
    objective='binary:logistic',
    num_round=42,
    silent=1
)
```

## åˆ›å»ºè¶…å‚æ•°èŒƒå›´

1. ä½¿ç”¨ [XGBoost ä¼˜åŒ–æ–‡æ¡£](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html)ï¼Œå°†è¶…å‚æ•°èŒƒå›´æ·»åŠ åˆ°ä»¥ä¸‹å•å…ƒæ ¼ä¸­ã€‚


```python
from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner

hyperparameter_ranges = {'alpha': ContinuousParameter(0,1000)}

hyperparameter_ranges = {
    'alpha': ContinuousParameter(0, 1000),
    'min_child_weight': ContinuousParameter(0, 120),
    'subsample': ContinuousParameter(0.5, 1),
    'eta': ContinuousParameter(0.1, 0.5),  
    'num_round': IntegerParameter(1,4000)
}
```

## æŒ‡å®šç›®æ ‡æŒ‡æ ‡

1. é’ˆå¯¹äºŒå…ƒåˆ†ç±»é—®é¢˜å°† `objective_metric_name`å’Œ `objective_type`æ›´æ–°ä¸ºé€‚å½“çš„å€¼ã€‚

```python
objective_metric_name = '<INSERT_VALUE_HERE>'
objective_type = '<INSERT_VALUE_HERE>'

### BEGIN_SOLUTION
objective_metric_name = 'validation:error'
objective_type = 'Minimize'
### END_SOLUTION
```

2. åˆ›å»ºè¶…å‚æ•°ä¼˜åŒ–ä»»åŠ¡ã€‚

```python
tuner = HyperparameterTuner(
    xgb,
    objective_metric_name,
    hyperparameter_ranges,
    # Set this to 10 or above depending upon budget & available time.
    max_jobs=10,
    max_parallel_jobs=1,
    objective_type=objective_type,
    early_stopping_type='Auto',
)
```

3. è¿è¡Œä¼˜åŒ–ä»»åŠ¡ã€‚è¯·æ³¨æ„ï¼Œæ­¤ä»»åŠ¡å¯èƒ½éœ€è¦å¤§çº¦ 60 åˆ†é’Ÿçš„æ—¶é—´ã€‚

```python
%%time
tuner.fit(
    inputs=data_channels, 
    include_cls_metadata=False, 
    wait=False
)
```

4. å¦‚æœæƒ³åœ¨ç­‰å¾…æœŸé—´å°è¯•ç¬¬ 7 èŠ‚ï¼Œè¯·ä¸è¦è¿è¡Œä¸‹ä¸€ä¸ªå•å…ƒæ ¼ï¼Œè€Œæ˜¯è½¬åˆ°ç¬¬ 7 èŠ‚ã€‚

```python
tuner.wait()
```

5. ä¼˜åŒ–ä»»åŠ¡å®Œæˆåï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹æ¥è‡ªä¼˜åŒ–ä»»åŠ¡çš„æŒ‡æ ‡ã€‚

```python
from pprint import pprint
from sagemaker.analytics import HyperparameterTuningJobAnalytics

tuner_analytics = HyperparameterTuningJobAnalytics(
    tuner.latest_tuning_job.name,
    sagemaker_session=sagemaker.Session()
)

df_tuning_job_analytics = tuner_analytics.dataframe()

# Sort the tuning job analytics by the final metrics value
df_tuning_job_analytics.sort_values(
    by=['FinalObjectiveValue'],
    inplace=True,
    ascending=False if tuner.objective_type == "Maximize" else True
)

# Show detailed analytics for the top 20 models
df_tuning_job_analytics.head(20)
```
## ä½¿ç”¨æœ€ä½³è¶…å‚æ•°ä»»åŠ¡

1. ä¼˜åŒ–ä»»åŠ¡å®Œæˆåï¼Œæ‚¨å¯ä»¥ä» å¯¦ä½œHyperparameterTunerå¯¦ä½œ å¯¹è±¡ä¸­æ‰¾åˆ°æœ€ä½³ä¼˜åŒ–ä»»åŠ¡ã€‚

```python
attached_tuner = HyperparameterTuner.attach(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())
best_training_job = attached_tuner.best_training_job()
```

2. å¾Œè£œã€‚

```python
from sagemaker.estimator import Estimator
algo_estimator = Estimator.attach(best_training_job)

best_algo_model = algo_estimator.create_model(
    env={'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT':"text/csv"}
)
```

3. é€šè¿‡æ•°æ®å¤„ç†ç®¡é“è¿è¡Œæµ‹è¯•æ•°æ®ï¼Œä»¥æµ‹è¯•æ¨¡å‹ã€‚

```python
%%time
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

text_features = ['text']
model_target = 'label'

text_processor_0 = Pipeline([(
    'text_vect_0', 
    CountVectorizer(
        preprocessor=clean, max_features=500
    )
)])

data_preprocessor = ColumnTransformer([(
    'text_pre_0', 
    text_processor_0, 
    text_features[0]
)])

print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)
train_matrix = data_preprocessor.fit_transform(train)
test_matrix = data_preprocessor.transform(test)
validate_matrix = data_preprocessor.transform(validate)
print(
    'Datasets shapes after processing: ', 
    train_matrix.shape, 
    validate_matrix.shape, 
    test_matrix.shape
)
```

4. ä½¿ç”¨æ¥è‡ªè¶…å‚æ•°ä¼˜åŒ–ä»»åŠ¡çš„æœ€ä½³ç®—æ³•ï¼Œä»¥ä½¿ç”¨æ‰¹å¤„ç†è½¬æ¢çš„æµ‹è¯•æ•°æ®ã€‚

```python
%%time
upload_s3_csv('batch-in.csv', 'batch-in', test_matrix, test, True)

batch_output = f's3://{bucket}/{prefix}/batch-out/'
batch_input = f's3://{bucket}/{prefix}/batch-in/{batch_X_file}'

xgb_transformer = best_algo_model.transformer(
    instance_count=1,
    instance_type='ml.m5.2xlarge',
    strategy='MultiRecord',
    assemble_with='Line',
    output_path=batch_output
)
xgb_transformer.transform(
    data=batch_input,
    data_type='S3Prefix',
    content_type='text/csv',
    split_type='Line'
)
xgb_transformer.wait(logs=False)
```

5. å¤„ç†ç»“æœä»¥è®¡ç®—ç±»ã€‚

```python
s3 = boto3.client('s3')
obj = s3.get_object(
    Bucket=bucket, 
    Key=f'{prefix}/batch-out/batch-in.csv.out'
)
target_predicted = pd.read_csv(
    io.BytesIO(obj['Body'].read()),
    sep=',',
    names=['class']
)

def binary_convert(x):
    threshold = 0.5
    if x > threshold:
        return 1
    else:
        return 0

target_predicted_binary = target_predicted['class'].apply(binary_convert)

```

6. ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶æ‰“å°æŒ‡æ ‡ã€‚

```python
plot_confusion_matrix(test['label'], target_predicted_binary)
```

7. å¾Œè£œã€‚

```python
print_metrics(test['label'], target_predicted_binary)
```

## ä½¿ç”¨ BlazingText

_å°†ä½¿ç”¨ BlazingTextï¼Œè¿™æ˜¯ä¸€ç§å†…ç½®çš„ Amazon SageMaker ç®—æ³•ã€‚BlazingText å¯ä»¥åœ¨ä¸åšä¿®æ”¹çš„æƒ…å†µä¸‹æ‰§è¡Œåˆ†ç±»ã€‚å°†ä¸º BlazingText é‡æ–°æ ¼å¼åŒ–æ•°æ®ã€‚ç„¶åï¼Œå°†ä½¿ç”¨æ•°æ®è®­ç»ƒç®—æ³•å¹¶å°†ç»“æœä¸ä¹‹å‰çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚_


1. é¦–å…ˆï¼Œè·å–ç®—æ³•å®¹å™¨ã€‚


```python
import sagemaker
from sagemaker.image_uris import retrieve

container = retrieve(
    'blazingtext',
    boto3.Session().region_name,"latest"
)
```

2. ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é…ç½®å‰ç¼€ã€‚

```python
import io
    
prefix='lab41'
train_file='blazing_train.txt'
validate_file='blazing_validate.txt'
test_file='blazing_test.txt'
```

3. æé†’è‡ªå·±æ•°æ®æ˜¯ä»€ä¹ˆæ ·çš„ã€‚

```python
train.head()
```


4. BlazingText éœ€è¦é‡‡ç”¨ä»¥ä¸‹æ ¼å¼çš„æ•°æ®ï¼š

```bash
`__label__1` Caught this movie on the tube on a Sunday...
```

5. ä»¥ä¸‹ä¸¤ä¸ªå•å…ƒæ ¼å°† DataFrame è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼ï¼Œç„¶åå°†å®ƒä»¬ä¸Šè½½åˆ° Amazon S3ã€‚

```python
blazing_text_buffer = io.StringIO()
train.to_string(buf=blazing_text_buffer, columns=['label','text'], header=False, index=False, formatters=
                         {'label': '__label__{}'.format})
s3r = boto3.resource('s3')
s3r.Bucket(bucket).Object(os.path.join(prefix, 'blazing', train_file)).put(Body=blazing_text_buffer.getvalue())
```

6. å¾Œè£œã€‚

```python
blazing_text_buffer = io.StringIO()
validate.to_string(
    buf=blazing_text_buffer, 
    columns=['label','text'], 
    header=False, 
    index=False, 
    formatters={'label': '__label__{}'.format}
)
s3r.Bucket(bucket).Object(
    os.path.join(prefix, 'blazing', validate_file)
).put(
    Body=blazing_text_buffer.getvalue()
)
```

## BlazingText ä¼°ç®—å™¨

1. é€šè¿‡æŒ‡å®šç¼ºå¤±å€¼æ¥å–æ¶ˆæ³¨é‡Šå¹¶å®Œæˆä¼°ç®—å™¨ä»£ç ã€‚

```python
bt_model = sagemaker.estimator.Estimator(
    container,
    sagemaker.get_execution_role(), 
    instance_count=1, 
    instance_type='ml.c4.4xlarge',
    volume_size = 30,
    max_run = 360000,
    input_mode= 'File',
    output_path=s3_output_location,
    sagemaker_session=sagemaker.Session()
)

```

2. ä½¿ç”¨ä»¥ä¸‹è¶…å‚æ•°ã€‚

```python
bt_model.set_hyperparameters(
    mode="supervised",
    epochs=10,
    min_count=2,
    learning_rate=0.05,
    vector_dim=10,
    early_stopping=True,
    patience=4,
    min_epochs=5,
    word_ngrams=2
)
```

3. è®¾ç½®è®­ç»ƒé€šé“å’ŒéªŒè¯é€šé“ã€‚

```python
train_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/blazing/{train_file}',
    content_type='text/csv')

validate_channel = sagemaker.inputs.TrainingInput(
    f's3://{bucket}/{prefix}/blazing/{validate_file}',
    content_type='text/csv')

data_channels_3 = {
    'train': train_channel, 
    'validation': validate_channel
}
```

## å¼€å§‹è®­ç»ƒä»»åŠ¡

1. è¾“å…¥ä»¥ä¸‹ä»£ç å¼€å§‹è®­ç»ƒä»»åŠ¡ã€‚ï¼ˆè¿™ä¸ªæ­¥éª¤å¯èƒ½éœ€è¦èŠ±å‡ åˆ†é’Ÿçš„æ—¶é—´ã€‚ï¼‰

```python
%%time

bt_model.fit(inputs=data_channels_3, logs=False)
```

2. è®­ç»ƒä»»åŠ¡å®Œæˆåï¼Œè¯·æŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡ã€‚

```python
sagemaker.analytics.TrainingJobAnalytics(
    bt_model._current_job_name, 
    metric_names = ['train:accuracy','validation:accuracy']
).dataframe()
```

3.  å¾Œè£œã€‚

```python
pd.options.display.max_rows
pd.set_option('display.max_colwidth', None)
```

4. å¾Œè£œã€‚


```python
bt_test = test.copy()
bt_test.head()
```

5. å°†æ•°æ®é›†æ ¼å¼åŒ–ä¸º BlazingText æ‰€éœ€çš„æ ¼å¼ã€‚

```python
# bt_test['text'].str.strip()
bt_test.replace(r'\\n','', regex=True, inplace = True)
bt_test.rename(columns={'text':'source'}, inplace=True)
bt_test.drop(columns='label', inplace=True)
```

6. å¾Œè£œã€‚

```python
print(bt_test.head().to_json(orient="records", lines=True))
```

7. å°†æ–‡ä»¶ä¸Šè½½åˆ° Amazon S3ã€‚

```python
bt_file = 'bt_input.json'
blazing_text_buffer = io.StringIO()
bt_test.to_json(path_or_buf=blazing_text_buffer, orient="records", lines=True)
```

8. å¾Œè£œã€‚

```python
s3r.Bucket(bucket).Object(os.path.join(prefix, 'blazing', bt_file)).put(Body=blazing_text_buffer.getvalue())

```

9. å¾Œè£œã€‚


```python
batch_output = f's3://{bucket}/{prefix}/blazing/'
batch_input = f's3://{bucket}/{prefix}/blazing/{bt_file}'
```

10. å¯¹æµ‹è¯•æ•°æ®ä½¿ç”¨æ‰¹å¤„ç†è½¬æ¢å™¨ã€‚

```python
%%time
bt_transformer = bt_model.transformer(
    instance_count=1,
    instance_type='ml.m5.2xlarge',
   strategy='MultiRecord',
   assemble_with='Line',
   output_path=batch_output
)

bt_transformer.transform(
    data=batch_input,
    data_type='S3Prefix',
    content_type='application/jsonlines',
    split_type='Line'
)

bt_transformer.wait(logs=True)
```

11. æ£€ç´¢æ¥è‡ª Amazon S3 çš„ç»“æœã€‚

```python
obj = s3.get_object(
    Bucket=bucket, 
    Key=f'{prefix}/blazing/bt_input.json.out'
)
```

12. å¾Œè£œã€‚

```python
target_predicted = pd.read_json(
    io.BytesIO(obj['Body'].read()),
    lines=True
)
```

13. å¾Œè£œã€‚

```python
target_predicted.head()
```

14. é‡æ–°æ ¼å¼åŒ–ç»“æœï¼Œä»¥ä¾¿è®¡ç®—æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡ã€‚

```python
def binary_convert(label):
    label = label[0].replace('__label__','')
    return int(label)

target_predicted_binary = target_predicted['label'].apply(binary_convert)
```

15. å¾Œè£œã€‚

```python
plot_confusion_matrix(test['label'], target_predicted_binary)
```

16. å¾Œè£œã€‚

```python
print_metrics(test['label'], target_predicted_binary)
```

## ä½¿ç”¨Comprehend


åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨ Amazon Comprehend æ¥è®¡ç®—æƒ…ç»ªã€‚Amazon Comprehend ä¸ºæ‚¨æä¾›äº†æ­£é¢å’Œè´Ÿé¢çš„ç»“æœï¼Œè¿˜æ˜¾ç¤ºäº†ä¸­ç«‹å’Œå–œå¿§å‚åŠçš„ç»“æœã€‚Amazon Comprehend æ˜¯ä¸€é¡¹æ‰˜ç®¡çš„æœåŠ¡ï¼Œåœ¨ä½¿ç”¨å®ƒä¹‹å‰éœ€è¦è¾ƒå°‘çš„æ–‡æœ¬å¤„ç†ã€‚æ‚¨æ— éœ€å¤„ç†æœ¬èŠ‚ä¸­çš„ä»»ä½•æ–‡æœ¬ã€‚

æŸ¥çœ‹  `test` DataFrame ä¸­çš„æ•°æ®æ˜¯ä»€ä¹ˆæ ·çš„ã€‚


```python
test.head()
```

Amazon Comprehend çš„ä½¿ç”¨å¯ä»¥åƒ API è°ƒç”¨ä¸€æ ·ç®€å•ç›´æ¥ã€‚

ä»¥ä¸‹å•å…ƒæ ¼è¾“å‡ºäº†æ¥è‡ª Amazon Comprehend çš„å‰äº”ä¸ªç»“æœã€‚


```python
import boto3
import json

comprehend = boto3.client(service_name='comprehend')
for n in range(5):
    text = test.iloc[n]['text']
    response = comprehend.detect_sentiment(Text=text, LanguageCode='en')
    sentiment = response['Sentiment']
    print(f'{sentiment} - {text}')

```

æ‚¨å¯ä»¥å¯åŠ¨é¢„æµ‹ä»»åŠ¡æ¥å¤„ç†å¤šä¸ªé¡¹ç›®ã€‚å¿…é¡»å°†è¾“å…¥æ ¼å¼åŒ–ä¸ºæ¯è¡Œçš„å•ä¸ªè¾“å…¥ï¼Œç„¶åä¸Šè½½åˆ° Amazon S3ã€‚æ–‡æœ¬çš„æœ€å¤§å¤§å°ä¸º 5120ï¼Œå› æ­¤ `str.slice(0,5000)`å‡½æ•°ç”¨äºä¿®å‰ªé•¿æ–‡æœ¬ã€‚


```python
# Upload test file minus label to S3
def upload_comprehend_s3_csv(filename, folder, dataframe):
    csv_buffer = io.StringIO()
    
    dataframe.to_csv(csv_buffer, header=False, index=False )
    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())

comprehend_file = 'comprehend_input.csv'
upload_comprehend_s3_csv(comprehend_file, 'comprehend', test['text'].str.slice(0,5000))
test_url = f's3://{bucket}/{prefix}/comprehend/{comprehend_file}'
print(f'Uploaded input to {test_url}')
```

æ•°æ®ä¸Šè½½åˆ° Amazon S3 åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `start_sentiment_detection_jon`å‡½æ•°å¼€å§‹ä»»åŠ¡ã€‚



### æŒ‘æˆ˜ï¼šé…ç½® Amazon Comprehend ä»»åŠ¡å‚æ•°

åœ¨ä¸‹ä¸€ä¸ªå•å…ƒæ ¼ä¸­ï¼Œé…ç½® Amazon Comprehend ä»»åŠ¡å‚æ•°ã€‚
â€“ åœ¨__input_data_config__ä¸­ - 
  â€“å¯¦ä½œS3Uriå¯¦ä½œï¼šå°† *`<S3_INPUT_GOES_HERE> `* æ›¿æ¢ä¸ºä¹‹å‰å®šä¹‰çš„ `test_uri`
  â€“å¯¦ä½œInputFormatå¯¦ä½œï¼šå°† *`<INPUT_FORMAT_GOES_HERE> `* æ›¿æ¢ä¸º `ONE_DOC_PER_LINE`
â€“ åœ¨__output_data config__ ä¸­-  
  â€“å¯¦ä½œS3Uriå¯¦ä½œï¼šå°† *`<S3_OUTPUT_GOES_HERE> `* æ›¿æ¢ä¸º `s3_output_location`
  â€“å¯¦ä½œdata_access_role_arnå¯¦ä½œï¼šå°† *`arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW `* æ›¿æ¢ä¸º*Labè¯¦ç»†ä¿¡æ¯*æ–‡ä»¶ä¸­çš„ Amazon Resource Name (ARN)


```python
input_data_config={
    'S3Uri': 'S3_INPUT_GOES_HERE',
    'InputFormat': 'INPUT_FORMAT_GOES_HERE'
},

output_data_config={
    'S3Uri': 'S3_OUTPUT_GOES_HERE'
},
data_access_role_arn = 'arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW'

### BEGIN_SOLUTION
input_data_config={
    'S3Uri': test_url,
    'InputFormat': 'ONE_DOC_PER_LINE'
}
output_data_config={
    'S3Uri': s3_output_location
}
data_access_role_arn = 'arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW'
### END_SOLUTION
```

ç°åœ¨ï¼Œæ‚¨å·²å®šä¹‰äº†ä»»åŠ¡å‚æ•°ï¼Œå¯ä»¥å¼€å§‹æƒ…ç»ªæ£€æµ‹ä»»åŠ¡ã€‚


```python
response = comprehend.start_sentiment_detection_job(
    InputDataConfig=input_data_config,
    OutputDataConfig=output_data_config,
    DataAccessRoleArn=data_access_role_arn,
    JobName='movie_sentiment',
    LanguageCode='en'
)

print(response['JobStatus'])
```

ä»¥ä¸‹å•å…ƒæ ¼å°†å¾ªç¯è¿›è¡Œï¼Œç›´åˆ°ä»»åŠ¡ç»“æŸã€‚ï¼ˆè¿™ä¸ªæ­¥éª¤å¯èƒ½éœ€è¦èŠ±å‡ åˆ†é’Ÿçš„æ—¶é—´ã€‚ï¼‰


```python
%%time
import time
job_id = response['JobId']
while True:
    job_status=(comprehend.describe_sentiment_detection_job(JobId=job_id))
    if job_status['SentimentDetectionJobProperties']['JobStatus'] in ['COMPLETED','FAILED']:
        break            
    else:
        print('.', end='')
    time.sleep(15)
print((comprehend.describe_sentiment_detection_job(JobId=job_id))['SentimentDetectionJobProperties']['JobStatus'])
```

ä»»åŠ¡å®Œæˆåï¼Œæ‚¨å¯ä»¥é€šè¿‡è°ƒç”¨ `describe_sentiment_detection_job`å‡½æ•°è¿”å›ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚


```python
output=(comprehend.describe_sentiment_detection_job(JobId=job_id))
print(output)
```

åœ¨ å¯¦ä½œOutputDataConfigå¯¦ä½œ éƒ¨åˆ†ï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ° `S3Uri`ã€‚æå–è¯¥ URI å°†ä¸ºæ‚¨æä¾›å¿…é¡»ä» Amazon S3 ä¸‹è½½çš„æ–‡ä»¶ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ç»“æœæ¥è®¡ç®—æŒ‡æ ‡ï¼Œæ–¹å¼ä¸ä½¿ç”¨ç®—æ³•è®¡ç®—æ‰¹å¤„ç†è½¬æ¢ç»“æœçš„æ–¹å¼ç›¸åŒã€‚


```python
comprehend_output_file = output['SentimentDetectionJobProperties']['OutputDataConfig']['S3Uri']
comprehend_bucket, comprehend_key = comprehend_output_file.replace("s3://", "").split("/", 1)

s3r = boto3.resource('s3')
s3r.meta.client.download_file(comprehend_bucket, comprehend_key, 'output.tar.gz')

# Extract the tar file
import tarfile
tf = tarfile.open('output.tar.gz')
tf.extractall()
```

åº”å°†æå–çš„æ–‡ä»¶å‘½åä¸º __output__ã€‚é˜…è¯»æ­¤æ–‡ä»¶ä¸­çš„è¡Œã€‚


```python
import json
data = ''
with open ('output', "r") as myfile:
    data = myfile.readlines()
```

å°†è¿™äº›è¡Œæ·»åŠ åˆ°æ•°ç»„ä¸­ã€‚


```python
results = []
for line in data:
    json_data = json.loads(line)
    results.append([json_data['Line'],json_data['Sentiment']])
```

å°†æ•°ç»„è½¬æ¢ä¸º Pandas DataFrameã€‚


```python
c = pd.DataFrame.from_records(results, index='index', columns=['index','sentiment'])
c.head()
```

ç»“æœåŒ…å« å¯¦ä½œNEGATIVEå¯¦ä½œï¼ˆè´Ÿé¢ï¼‰ã€å¯¦ä½œPOSITIVEå¯¦ä½œï¼ˆæ­£é¢ï¼‰ã€å¯¦ä½œNEUTRALå¯¦ä½œï¼ˆä¸­ç«‹ï¼‰å’Œ å¯¦ä½œMIXEDå¯¦ä½œï¼ˆå–œå¿§å‚åŠï¼‰ï¼Œè€Œä¸æ˜¯æ•°å€¼ã€‚è¦å°†è¿™äº›ç»“æœä¸æµ‹è¯•æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œå¯å°†å®ƒä»¬æ˜ å°„åˆ°æ•°å€¼ï¼Œå¦‚ä»¥ä¸‹å•å…ƒæ ¼æ‰€ç¤ºã€‚è¿”å›ç»“æœä¸­çš„ç´¢å¼•ä¹Ÿæ˜¯æ— åºçš„ã€‚ `sort_index`å‡½æ•°åº”è§£å†³è¿™ä¸ªé—®é¢˜ã€‚


```python
class_mapper = {'NEGATIVE':0, 'POSITIVE':1, 'NEUTRAL':2, 'MIXED':3}
c['sentiment']=c['sentiment'].replace(class_mapper)
c = c.sort_index()
c.head()
```


```python
# Build list to compare for Amazon Comprehend
test_2 = test.reset_index()
test_3 = test_2.sort_index()
test_labels = test_3.iloc[:,2]
```

æ‚¨å¯ä»¥ä½¿ç”¨ `plot_confusion_matrix`å‡½æ•°æ˜¾ç¤ºæ··æ·†çŸ©é˜µã€‚ç”±äº Amazon Comprehend çš„ç»“æœè¿˜åŒ…å«__mixed__ å’Œ __neutral__ï¼Œå› æ­¤å›¾è¡¨ä¼šæœ‰æ‰€ä¸åŒã€‚


```python
plot_confusion_matrix(test_labels, c['sentiment'])
```

ç”¨äºæ‰“å°æŒ‡æ ‡çš„ç°æœ‰å‡½æ•°æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå› ä¸ºæ‚¨çš„æ•°æ®ç»´åº¦å¤ªå¤šã€‚ä»¥ä¸‹ä»£ç å•å…ƒæ ¼å°†è®¡ç®—ç›¸åŒçš„å€¼ã€‚


```python
cm = confusion_matrix(test_labels, c['sentiment'])

TN = cm[0,0]
FP = cm[0,1]
FN = cm[1,0]
TP = cm[1,1]

Sensitivity  = float(TP)/(TP+FN)*100
# Specificity or true negative rate
Specificity  = float(TN)/(TN+FP)*100
# Precision or positive predictive value
Precision = float(TP)/(TP+FP)*100
# Negative predictive value
NPV = float(TN)/(TN+FN)*100
# Fall out or false positive rate
FPR = float(FP)/(FP+TN)*100
# False negative rate
FNR = float(FN)/(TP+FN)*100
# False discovery rate
FDR = float(FP)/(TP+FP)*100
# Overall accuracy
ACC = float(TP+TN)/(TP+FP+FN+TN)*100

print(f"Sensitivity or TPR: {Sensitivity}%")    
print(f"Specificity or TNR: {Specificity}%") 
print(f"Precision: {Precision}%")   
print(f"Negative Predictive Value: {NPV}%")  
print( f"False Positive Rate: {FPR}%") 
print(f"False Negative Rate: {FNR}%")  
print(f"False Discovery Rate: {FDR}%" )
print(f"Accuracy: {ACC}%") 
```

# æ­å–œï¼

æ‚¨å·²ç»å®Œæˆäº†æœ¬Labå†…å®¹ï¼Œç°åœ¨å¯ä»¥æŒ‰ç…§LabæŒ‡å—ä¸­çš„è¯´æ˜ç»“æŸæœ¬Labå†…å®¹ã€‚

*Â©2023 Amazon Web Services, Inc. æˆ–å…¶è”å±å…¬å¸ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚æœªç» Amazon Web Services, Inc. äº‹å…ˆä¹¦é¢è®¸å¯ï¼Œä¸å¾—å¤åˆ¶æˆ–è½¬è½½æœ¬æ–‡çš„éƒ¨åˆ†æˆ–å…¨éƒ¨å†…å®¹ã€‚ç¦æ­¢å› å•†ä¸šç›®çš„å¤åˆ¶ã€å‡ºå€Ÿæˆ–å‡ºå”®æœ¬æ–‡ã€‚æ‰€æœ‰å•†æ ‡å‡ä¸ºå„è‡ªæ‰€æœ‰è€…çš„è´¢äº§ã€‚*

