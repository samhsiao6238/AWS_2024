{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 02.14.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验室 4.1：实施情绪分析\n",
    " \n",
    "\n",
    "\n",
    "在本实验室内容中，您将开发一种解决方案，用于对互联网电影数据库 (IMDB) 的数据集进行情绪分析。\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "- 评估用于情绪分析的自然语言处理 (NLP) 的机器学习 (ML) 算法\n",
    "- 创建情绪分析业务问题的解决方案。\n",
    "\n",
    "## 业务场景简介\n",
    "\n",
    "在本实验室内容中，您要在小型开发团队中扮演数据科学家的角色。您工作的组织维护着一个电影评论网站。已确定了一个关键的客户功能：根据正面和负面评论的数量为特定电影提供整体正面（笑脸）或负面（悲伤的脸）的评价。您将开发一个机器学习 (ML) 解决方案，使开发人员能为电影评论创建推理。您需要分析评论并指明评论是正面还是负面的。\n",
    "\n",
    "为了帮助完成此任务，您可以访问包含 50,000 个电影评论原始文本的数据集。这些评论已被标记为正面或负面。\n",
    "\n",
    "关于该数据集\n",
    "电影评论大型数据集收集了截然不同的电影评论。此数据支持以下论文相关工作：\n",
    "\n",
    "Andrew L. Maas、Raymond E. Daly、Peter T. Pham、Dan Huang、Andrew Y. Ng 和 Christopher Potts；“Learning Word Vectors for Sentiment Analysis”； 发表于 2011 年 6 月在美国俄勒冈州波特兰举行的第 49 届国际计算语言学学会年会 (ACL 2011)，http://ai.stanford.edu/~amaas/data/sentiment/。\n",
    "\n",
    "数据集包含单个文本字段，其中包含评论。数据集标记为正面 (1) 或负面 (0)。\n",
    "\n",
    "该数据集包含以下功能：\n",
    "\n",
    "文本：评论文本\n",
    "标签：评论是正面还是负面（1 还是 0）\n",
    "\n",
    "## 实验室步骤\n",
    "\n",
    "要完成本实验室内容，您需要按以下步骤操作：\n",
    "\n",
    "1. [安装程序包](#1.-Installing-packages)\n",
    "2. [读取数据集](#2.-Reading-the-dataset)\n",
    "3. [执行探索性数据分析](#3.-Performing-exploratory-data-analysis)\n",
    "4. [运行第一次传递：最低程度的处理](#4.-Running-the-first-pass:-Minimal-processing)\n",
    "5. [运行第二次传递：标准化文本](#5.-Running-the-second-pass:-Normalizing-the-text)\n",
    "6. [优化超参数](#6.-Tuning-hyperparameters)\n",
    "7. [使用 BlazingText](#7.-Using-BlazingText)\n",
    "8. [使用 Amazon Comprehend](#8.-Using-Amazon-Comprehend)\n",
    "\n",
    "## 提交作业\n",
    "\n",
    "1.在实验室控制台中，选择 **Submit**（提交）记录您的进度，在出现提示时，选择 **Yes**（是）。\n",
    "\n",
    "1.如果在几分钟后仍未显示结果，请返回到实验说明的顶部，并选择 **Grades**（成绩）。\n",
    "\n",
    "**提示**：您可以多次提交作业。您更改作业后，再次选择 **Submit**（提交）。您最后一次提交的作业将记为本实验室内容的作业。\n",
    "\n",
    "1.要查找有关您作业的详细反馈，请选择 **Details**（详细信息），然后选择 **View Submission Report**（查看提交报告）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.安装程序包\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "首先，更新并安装您将在笔记本中使用的程序包。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (24.3.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.35.54)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.35.54)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3) (0.10.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.54->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.54->boto3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.54->boto3) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.233.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.35.54)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.25.5)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.0)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.10)\n",
      "Requirement already satisfied: sagemaker-mlflow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.1.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.66.5)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.35.54)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.20.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.9.2)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (13.9.3)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (2024.8.30)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2024.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.9)\n",
      "Requirement already satisfied: pox>=0.3.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.17)\n",
      "Requirement already satisfied: mlflow>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-mlflow->sagemaker) (2.17.1)\n",
      "Requirement already satisfied: mlflow-skinny==2.17.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.17.1)\n",
      "Requirement already satisfied: Flask<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.13.3)\n",
      "Requirement already satisfied: graphene<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.4.1)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.7)\n",
      "Requirement already satisfied: matplotlib<4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.9.2)\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (17.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.5.2)\n",
      "Requirement already satisfied: scipy<2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.14.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.0.36)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (23.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (8.1.7)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.36.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.43)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.27.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.18.0)\n",
      "Requirement already satisfied: Mako in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.3.6)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.8.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (11.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.35.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.0.11)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.48b0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.7.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.1->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: seaborn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#Install/Upgrade dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade boto3\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install --upgrade nltk\n",
    "!pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__注意：__ 在首次进行本实验室内容之前，我们建议您通过选择__Kernel__ > __Start Kernel__来重新启动内核。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入将在笔记本中使用的程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import os, io, struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码单元格包含一些帮助程序函数，可绘制混淆矩阵并计算其他关键指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    df_confusion = pd.DataFrame(matrix)\n",
    "    colormap = sns.color_palette(\"BrBG\", 10)\n",
    "    sns.heatmap(df_confusion, annot=True, fmt='.2f', cbar=None, cmap=colormap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.show()\n",
    "    \n",
    "def print_metrics(test_labels, target_predicted_binary):\n",
    "    TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted_binary).ravel()\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    Sensitivity  = float(TP)/(TP+FN)*100\n",
    "    # Specificity or true negative rate\n",
    "    Specificity  = float(TN)/(TN+FP)*100\n",
    "    # Precision or positive predictive value\n",
    "    Precision = float(TP)/(TP+FP)*100\n",
    "    # Negative predictive value\n",
    "    NPV = float(TN)/(TN+FN)*100\n",
    "    # Fall out or false positive rate\n",
    "    FPR = float(FP)/(FP+TN)*100\n",
    "    # False negative rate\n",
    "    FNR = float(FN)/(TP+FN)*100\n",
    "    # False discovery rate\n",
    "    FDR = float(FP)/(TP+FP)*100\n",
    "    # Overall accuracy\n",
    "    ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "\n",
    "    print(f\"Sensitivity or TPR: {Sensitivity}%\")    \n",
    "    print(f\"Specificity or TNR: {Specificity}%\") \n",
    "    print(f\"Precision: {Precision}%\")   \n",
    "    print(f\"Negative Predictive Value: {NPV}%\")  \n",
    "    print( f\"False Positive Rate: {FPR}%\") \n",
    "    print(f\"False Negative Rate: {FNR}%\")  \n",
    "    print(f\"False Discovery Rate: {FDR}%\" )\n",
    "    print(f\"Accuracy: {ACC}%\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.读取数据集\n",
    "\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "在本节中，您将加载数据集。Amazon Sagemaker Studio 已下载了数据集。使用__pandas__ 库读取数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __ 加载训练数据：__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/imdb.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.执行探索性数据分析\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "在本节中，您将检查数据集。\n",
    "\n",
    "执行以下函数。提供第一个是为了让您学习格式。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：列出前八行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_eight_rows(df):\n",
    "    # Implement this function\n",
    "    return df.head(8)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  What I hoped for (or even expected) was the we...      0\n",
      "1  Garden State must rate amongst the most contri...      0\n",
      "2  There is a lot wrong with this film. I will no...      1\n",
      "3  To qualify my use of \"realistic\" in the summar...      1\n",
      "4  Dirty War is absolutely one of the best politi...      1\n",
      "5  Many other viewers are saying that this is not...      1\n",
      "6  I understand that Roger Corman loves to do thi...      0\n",
      "7  I love this show. I watched every episode last...      0\n"
     ]
    }
   ],
   "source": [
    "print(show_eight_rows(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：数据是什么形状？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_shape(df):\n",
    "    # Implement this function\n",
    "    ### BEGIN_SOLUTION\n",
    "    return df.shape\n",
    "    ### END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(show_data_shape(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：数据中有多少正面和负面实例？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_instances(df):\n",
    "    # Implement this function\n",
    "    ### BEGIN_SOLUTION\n",
    "    return df['label'].value_counts()\n",
    "    ### END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    25000\n",
      "1    25000\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(show_data_instances(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：数据是否有任何缺失值？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_missing_values(df):\n",
    "    # Implement this function\n",
    "    ### BEGIN_SOLUTION\n",
    "    return df.isna().sum()\n",
    "    ### END_SOLUTION\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(show_missing_values(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.运行第一次传递：最低程度的处理\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "在本节中，您将执行训练分类模型所需的最少的步骤。您将使用此训练模型来查看处理文本对结果的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，导入 Natural Langauge ToolKit (NLTK) 程序包和 Regular Expression (re) 程序包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：将数据拆分为几个数据集，分别用于训练、验证和测试\n",
    "\n",
    "在此任务中，您将拆分数据集，使数据集的 80％ 用于训练，用于验证和测试的各占 10%。\n",
    "\n",
    "要拆分数据集，请使用 __scikit-learn__ 中的  `train_test_split` 函数。有关此函数的更多信息，请参阅 [scikit-learn test_train_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) at https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html。\n",
    "\n",
    "指定 __df__ 作为数据集。将此数据集拆分为 __train__ 集和__test_and_validate__集。然后，将__test_and_validate__集拆分为 __test__ 集和 __validate__ 集。\n",
    "\n",
    "（* 可选 *）要获得可重复的结果，洗牌和 **random_state**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# uncomment the following lines and implement your solution\n",
    "def split_data(df):\n",
    "    # train, test_and_validate = train_test_split(....)\n",
    "    # test, validate = train_test_split(....)\n",
    "    ### BEGIN_SOLUTION\n",
    "    train, test_and_validate = train_test_split(df,\n",
    "                                            test_size=0.2,\n",
    "                                            shuffle=True,\n",
    "                                            random_state=324\n",
    "                                            )\n",
    "    test, validate = train_test_split(test_and_validate,\n",
    "                                                test_size=0.5,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=324)\n",
    "    ### END_SOLUTION\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过运行以下代码单元格检查是否正确拆分数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n",
      "(5000, 2)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "train, validate, test = split_data(df)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组装处理管道\n",
    "\n",
    "在此单元格中，为文本数据组装基本的处理管道。现在，您将修改此实施，以添加更多功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shapes before processing:  (40000, 2) (5000, 2) (5000, 2)\n",
      "Datasets shapes after processing:  (40000, 500) (5000, 500) (5000, 500)\n",
      "CPU times: user 7.54 s, sys: 114 ms, total: 7.65 s\n",
      "Wall time: 7.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "text_features = ['text']\n",
    "model_target = 'label'\n",
    "\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(max_features=500))\n",
    "])\n",
    "\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_pre_0', text_processor_0, text_features[0])\n",
    "])\n",
    "\n",
    "print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)\n",
    "train_matrix = data_preprocessor.fit_transform(train)\n",
    "test_matrix = data_preprocessor.transform(test)\n",
    "validate_matrix = data_preprocessor.transform(validate)\n",
    "print('Datasets shapes after processing: ', train_matrix.shape, validate_matrix.shape, test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要训练模型，必须以正确的格式将数据上载到 Amazon Simple Storage Service (Amazon S3)。XGBoost 使用逗号分隔值 (CSV) 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, X_train, y_train, is_test=False):\n",
    "    csv_buffer = io.StringIO()\n",
    "    features = [t.toarray().astype('float32').flatten().tolist() for t in X_train]\n",
    "    if is_test:\n",
    "        temp_list = features\n",
    "    else:\n",
    "        temp_list = np.insert(features, 0, y_train['label'], axis=1)\n",
    "    np.savetxt(csv_buffer, temp_list, delimiter=',' )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'c133864a3391494l8261467t1w637423426529-labbucket-hcjcbnnncwhe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置此传递的文件名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='lab41'\n",
    "train_file='train-pass1.csv'\n",
    "validate_file='validate-pass1.csv'\n",
    "test_file='test-pass1.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练数据集、验证数据集和测试数据集上载到 Amazon S3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_s3_csv(train_file, 'train', train_matrix, train)\n",
    "upload_s3_csv(validate_file, 'validate', validate_matrix, validate)\n",
    "upload_s3_csv(test_file, 'test', test_matrix, test, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：训练 XGBoost 模型\n",
    "\n",
    "取消注释并完成以下 SageMaker 函数，以创建一个 `Estimator`。使用以下参数：\n",
    "– **角色**：使用当前的 SageMaker 角色（__提示：__ 使用 `sagemaker.get_execution_role()`）\n",
    "– **实例数量**： `1`\n",
    "– **实例类型**： `ml.m5.xlarge`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "s3_output_location=f's3://{bucket}/{prefix}/output/'\n",
    "\n",
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"error\",\n",
    "             \"objective\": \"binary:logistic\",\n",
    "             \"silent\" : 1}\n",
    "\n",
    "# xgb_model=sagemaker.estimator.Estimator(container,\n",
    "#                                         role=<INSERT_ROLE_HERE>,\n",
    "#                                         instance_count=<INSERT_COUNT_HERE>,\n",
    "#                                         instance_type=<INSERT_INSTANCE_TYPE_HERE>,\n",
    "#                                         output_path=s3_output_location,\n",
    "#                                         hyperparameters=hyperparams,\n",
    "#                                         sagemaker_session=sagemaker.Session())\n",
    "### BEGIN_SOLUTION\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                        role=sagemaker.get_execution_role(),\n",
    "                                        instance_count=1,\n",
    "                                        instance_type='ml.m5.2xlarge',\n",
    "                                        output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置两个数据通道。一个数据通道用于训练模型的训练数据。另一个数据通道用于生成绩效指标的验证数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{bucket}/{prefix}/train/{train_file}',\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{bucket}/{prefix}/validate/{validate_file}',\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型。（这个步骤可能需要花几分钟的时间。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: xgb-pass1-11-05-2024-18-35-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-11-05 18:35:04 Starting - Starting the training job.....\n",
      "2024-11-05 18:35:35 Starting - Preparing the instances for training...\n",
      "2024-11-05 18:36:00 Downloading - Downloading input data...\n",
      "2024-11-05 18:36:19 Downloading - Downloading the training image.....\n",
      "2024-11-05 18:36:50 Training - Training image download completed. Training in progress.......\n",
      "2024-11-05 18:37:26 Uploading - Uploading generated training model.\n",
      "2024-11-05 18:37:34 Completed - Training job completed\n",
      "CPU times: user 136 ms, sys: 11.8 ms, total: 148 ms\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgb_model.fit(inputs=data_channels, logs=False, job_name='xgb-pass1-'+datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示来自当前 XGBoost 任务的指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:error</td>\n",
       "      <td>0.20855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>validation:error</td>\n",
       "      <td>0.23550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp       metric_name    value\n",
       "0        0.0       train:error  0.20855\n",
       "1        0.0  validation:error  0.23550"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(xgb_model._current_job_name, \n",
    "                                         metric_names = ['train:error','validation:error']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始结果似乎没有帮助。使用 __test__ 数据集计算更多指标。（这个步骤可能需要花几分钟的时间。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-11-05-18-37-38-633\n",
      "INFO:sagemaker:Creating transform job with name: xgboost-pass1\n"
     ]
    },
    {
     "ename": "ResourceInUse",
     "evalue": "An error occurred (ResourceInUse) when calling the CreateTransformJob operation: Job name must be unique within an AWS account and region, and a job with this name already exists (arn:aws:sagemaker:us-east-1:637423426529:transform-job/xgboost-pass1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceInUse\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:12\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/transformer.py:302\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, batch_data_capture_config, wait, logs)\u001b[0m\n\u001b[1;32m    292\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[1;32m    294\u001b[0m batch_data_capture_config \u001b[38;5;241m=\u001b[39m resolve_class_attribute_from_config(\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     batch_data_capture_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session,\n\u001b[1;32m    300\u001b[0m )\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_transform_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TransformJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_client_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_data_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_transform_job\u001b[38;5;241m.\u001b[39mwait(logs\u001b[38;5;241m=\u001b[39mlogs)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/transformer.py:636\u001b[0m, in \u001b[0;36m_TransformJob.start_new\u001b[0;34m(cls, transformer, data, data_type, content_type, compression_type, split_type, input_filter, output_filter, join_source, experiment_config, model_client_config, batch_data_capture_config)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Placeholder docstring\"\"\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m transform_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_transform_args(\n\u001b[1;32m    622\u001b[0m     transformer,\n\u001b[1;32m    623\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     batch_data_capture_config,\n\u001b[1;32m    634\u001b[0m )\n\u001b[0;32m--> 636\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtransform_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(transformer\u001b[38;5;241m.\u001b[39msagemaker_session, transformer\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:3886\u001b[0m, in \u001b[0;36mSession.transform\u001b[0;34m(self, job_name, model_name, strategy, max_concurrent_transforms, max_payload, input_config, output_config, resource_config, experiment_config, env, tags, data_processing, model_client_config, batch_data_capture_config)\u001b[0m\n\u001b[1;32m   3883\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   3884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_transform_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m-> 3886\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:6606\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   6589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   6590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   6591\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6594\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   6595\u001b[0m ):\n\u001b[1;32m   6596\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   6597\u001b[0m \n\u001b[1;32m   6598\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6604\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   6605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:3884\u001b[0m, in \u001b[0;36mSession.transform.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   3882\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating transform job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m   3883\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m-> 3884\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_transform_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mResourceInUse\u001b[0m: An error occurred (ResourceInUse) when calling the CreateTransformJob operation: Job name must be unique within an AWS account and region, and a job with this name already exists (arn:aws:sagemaker:us-east-1:637423426529:transform-job/xgboost-pass1)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "upload_s3_csv('batch-in.csv', 'batch-in', test_matrix, test, True)\n",
    "batch_X_file='batch-in.csv'\n",
    "batch_output = f's3://{bucket}/{prefix}/batch-out/'\n",
    "batch_input = f's3://{bucket}/{prefix}/batch-in/{batch_X_file}'\n",
    "\n",
    "xgb_transformer = xgb_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m5.2xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line',\n",
    "                         job_name='xgboost-pass1')\n",
    "xgb_transformer.wait(logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchKey",
     "evalue": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/batch-out/batch-in.csv.out\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m target_predicted \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(io\u001b[38;5;241m.\u001b[39mBytesIO(obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()),sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m,names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinary_convert\u001b[39m(x):\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=f'{prefix}/batch-out/batch-in.csv.out')\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['class'])\n",
    "\n",
    "def binary_convert(x):\n",
    "    threshold = 0.5\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted_binary = target_predicted['class'].apply(binary_convert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.第二次传递：标准化文本\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "在本节中，您将在重新训练模型之前对文本执行一些标准的预处理任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：删除可能影响情绪的非索引字\n",
    "\n",
    "您可以删除所有非索引字，但您可能想保留可能影响情绪的非索引字，例如 __not__ 或 __don't__。\n",
    "\n",
    "已提供了一些要排除的非索引字。更新函数，以删除可能影响情绪的其他词语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of stopwords from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(stopwords):\n",
    "    # Implement this function\n",
    "    excluding = ['against', 'not', 'don', 'don\\'t','ain', 'are', 'aren\\'t']\n",
    "    ### BEGIN_SOLUTION\n",
    "    excluding = ['against', 'not', 'don', 'don\\'t','ain', 'are', 'aren\\'t', 'could', 'couldn\\'t',\n",
    "             'did', 'didn\\'t', 'does', 'doesn\\'t', 'had', 'hadn\\'t', 'has', 'hasn\\'t', \n",
    "             'have', 'haven\\'t', 'is', 'isn\\'t', 'might', 'mightn\\'t', 'must', 'mustn\\'t',\n",
    "             'need', 'needn\\'t','should', 'shouldn\\'t', 'was', 'wasn\\'t', 'were', \n",
    "             'weren\\'t', 'won\\'t', 'would', 'wouldn\\'t']\n",
    "    ### END_SOLUTION\n",
    "    return [word for word in stop if word not in excluding]\n",
    "\n",
    "# New stopword list\n",
    "stopwords = remove_stopwords(stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：添加清理步骤\n",
    "\n",
    "更新以下 `clean`函数，以完成以下任务：\n",
    "– 删除前导空格和尾随空格\n",
    "– 删除任何 HTML 标记\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')\n",
    "def clean(sent):\n",
    "    # Implement this function\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub('\\s+', ' ', sent)\n",
    "    ### BEGIN_SOLUTION\n",
    "    sent = sent.strip()\n",
    "    sent = re.compile('<.*?>').sub('',sent)\n",
    "    ### END_SOLUTION\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in word_tokenize(sent):\n",
    "        # You are applying custom filtering here. Feel free to try different things.\n",
    "        # Check if it is not numeric, its length > 2, and it is not in stopwords\n",
    "        if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  \n",
    "            # Stem and add to filtered list\n",
    "            filtered_sentence.append(snow.stem(w))\n",
    "    final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用之前创建的函数创建新的测试、验证和测试 DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line and implement the function call to split_data\n",
    "#train, validate, test = \n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "train, validate, test = split_data(df)\n",
    "### END_SOLUTION\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "管道已更新，以包含对之前定义的来自 `CountVectorizer`的 `clean`函数的调用。此函数需要更长的运行时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "text_features = ['text']\n",
    "model_target = 'label'\n",
    "\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(preprocessor=clean, max_features=500))\n",
    "])\n",
    "\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_pre_0', text_processor_0, text_features[0])\n",
    "])\n",
    "\n",
    "print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)\n",
    "train_matrix = data_preprocessor.fit_transform(train)\n",
    "test_matrix = data_preprocessor.transform(test)\n",
    "validate_matrix = data_preprocessor.transform(validate)\n",
    "print('Datasets shapes after processing: ', train_matrix.shape, validate_matrix.shape, test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置此传递的文件名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='lab41'\n",
    "train_file='train_pass2.csv'\n",
    "validate_file='validate_pass2.csv'\n",
    "test_file='test_pass2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：将文件上载到 Amazon S3\n",
    "\n",
    "使用之前的代码将新文件上载到 Amazon S3。\n",
    "\n",
    "__提示：__复制以下代码并将其粘贴到以下代码单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN_SOLUTION\n",
    "upload_s3_csv(train_file, 'train', train_matrix, train)\n",
    "upload_s3_csv(validate_file, 'validate', validate_matrix, validate)\n",
    "upload_s3_csv(test_file, 'test', test_matrix, test, True)\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：创建估算器并设置数据通道\n",
    "\n",
    "使用之前的代码设置估算器和数据通道。\n",
    "\n",
    "__提示： __ 复制上一个单元格中的代码并将其粘贴到下面的单元格中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n",
    "\n",
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"error\",\n",
    "             \"objective\": \"binary:logistic\",\n",
    "             \"silent\" : 1}\n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                        sagemaker.get_execution_role(),\n",
    "                                        instance_count=1,\n",
    "                                        instance_type='ml.m5.2xlarge',\n",
    "                                        output_path=s3_output_location,\n",
    "                                        hyperparameters = hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{bucket}/{prefix}/train/{train_file}',\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{bucket}/{prefix}/validate/{validate_file}',\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "\n",
    "### END_SOLUTION\n",
    "\n",
    "xgb_model.fit(inputs=data_channels, logs=False, job_name='xgb-pass2-'+datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(xgb_model._current_job_name, \n",
    "                                         metric_names = ['train:error','validation:error']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：创建批处理转换器任务\n",
    "\n",
    "使用之前的代码创建一个转换器任务。（这个步骤可能需要花几分钟的时间。） \n",
    "\n",
    "__提示：__ 复制上一个示例中的代码并将其粘贴到以下单元格中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "xgb_transformer = xgb_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m5.2xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "### END_SOLUTION\n",
    "\n",
    "xgb_transformer.wait(logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['class'])\n",
    "\n",
    "def binary_convert(x):\n",
    "    threshold = 0.5\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted_binary = target_predicted['class'].apply(binary_convert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新模型比第一个模型更好还是更差？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.优化超参数\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "在本节中，您将创建一个超参数优化任务来优化模型。\n",
    "\n",
    "__注意__：优化超参数需要花大约一个小时的时间。如果您没有足够的时间，请前往第 7 节。您也可以在开始优化任务后跳至第 7 节，然后返回查看结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：创建估算器用于优化\n",
    "\n",
    "第一步是创建一个估算器用于优化。取消注释并完成以下估算器代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = sagemaker.estimator.Estimator(....)\n",
    "### BEGIN_SOLUTION\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker.get_execution_role(), \n",
    "                                    instance_count= 1, # make sure you have limit set for these instances\n",
    "                                    instance_type='ml.m5.2xlarge', \n",
    "                                    output_path=f's3://{bucket}/{prefix}/output',\n",
    "                                    sagemaker_session=sagemaker.Session())\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(eval_metric='error',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=42,\n",
    "                        silent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：创建超参数范围\n",
    "\n",
    "使用 [XGBoost 优化文档](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html)，将超参数范围添加到以下单元格中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0,1000)}\n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 1000),\n",
    "                         'min_child_weight': ContinuousParameter(0, 120),\n",
    "                         'subsample': ContinuousParameter(0.5, 1),\n",
    "                         'eta': ContinuousParameter(0.1, 0.5),  \n",
    "                         'num_round': IntegerParameter(1,4000)\n",
    "                         }\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：指定目标指标\n",
    "\n",
    "针对二元分类问题将 `objective_metric_name`和 `objective_type`更新为适当的值。有关更多信息，请参阅 [XGBoost 优化文档](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = '<INSERT_VALUE_HERE>'\n",
    "objective_type = '<INSERT_VALUE_HERE>'\n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "objective_metric_name = 'validation:error'\n",
    "objective_type = 'Minimize'\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建超参数优化任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10, # Set this to 10 or above depending upon budget & available time.\n",
    "                            max_parallel_jobs=1,\n",
    "                            objective_type=objective_type,\n",
    "                            early_stopping_type='Auto',\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行优化任务。请注意，此任务可能需要大约 60 分钟的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tuner.fit(inputs=data_channels, include_cls_metadata=False, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您想在等待期间尝试第 7 节，请不要运行下一个单元格，而是转到第 7 节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化任务完成后，您可以查看来自优化任务的指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())\n",
    "\n",
    "df_tuning_job_analytics = tuner_analytics.dataframe()\n",
    "\n",
    "# Sort the tuning job analytics by the final metrics value\n",
    "df_tuning_job_analytics.sort_values(\n",
    "    by=['FinalObjectiveValue'],\n",
    "    inplace=True,\n",
    "    ascending=False if tuner.objective_type == \"Maximize\" else True)\n",
    "\n",
    "# Show detailed analytics for the top 20 models\n",
    "df_tuning_job_analytics.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用最佳超参数任务\n",
    "\n",
    "优化任务完成后，您可以从 **HyperparameterTuner** 对象中找到最佳优化任务。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attached_tuner = HyperparameterTuner.attach(tuner.latest_tuning_job.name, sagemaker_session=sagemaker.Session())\n",
    "best_training_job = attached_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "algo_estimator = Estimator.attach(best_training_job)\n",
    "\n",
    "best_algo_model = algo_estimator.create_model(env={'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT':\"text/csv\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据处理管道运行测试数据，以测试模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "text_features = ['text']\n",
    "model_target = 'label'\n",
    "\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(preprocessor=clean, max_features=500))\n",
    "])\n",
    "\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('text_pre_0', text_processor_0, text_features[0])\n",
    "])\n",
    "\n",
    "print('Datasets shapes before processing: ', train.shape, validate.shape, test.shape)\n",
    "train_matrix = data_preprocessor.fit_transform(train)\n",
    "test_matrix = data_preprocessor.transform(test)\n",
    "validate_matrix = data_preprocessor.transform(validate)\n",
    "print('Datasets shapes after processing: ', train_matrix.shape, validate_matrix.shape, test_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用来自超参数优化任务的最佳算法，以使用批处理转换的测试数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "upload_s3_csv('batch-in.csv', 'batch-in', test_matrix, test, True)\n",
    "\n",
    "batch_output = f's3://{bucket}/{prefix}/batch-out/'\n",
    "batch_input = f's3://{bucket}/{prefix}/batch-in/{batch_X_file}'\n",
    "\n",
    "xgb_transformer = best_algo_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m5.2xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理结果以计算类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=f'{prefix}/batch-out/batch-in.csv.out')\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['class'])\n",
    "\n",
    "def binary_convert(x):\n",
    "    threshold = 0.5\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted_binary = target_predicted['class'].apply(binary_convert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制混淆矩阵并打印指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.使用 BlazingText\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "在本节中，您将使用 BlazingText，这是一种内置的 Amazon SageMaker 算法。BlazingText 可以在不做修改的情况下执行分类。您将为 BlazingText 重新格式化数据。然后，您将使用数据训练算法并将结果与之前的模型进行比较。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，获取算法容器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve('blazingtext',boto3.Session().region_name,\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为训练、验证和测试数据配置前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "    \n",
    "prefix='lab41'\n",
    "train_file='blazing_train.txt'\n",
    "validate_file='blazing_validate.txt'\n",
    "test_file='blazing_test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提醒自己数据是什么样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "BlazingText 需要采用以下格式的数据：\n",
    "\n",
    "\\__label__1 Caught this movie on the tube on a Sunday...\n",
    "\n",
    "以下两个单元格将 DataFrame 转换为正确的格式，然后将它们上载到 Amazon S3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blazing_text_buffer = io.StringIO()\n",
    "train.to_string(buf=blazing_text_buffer, columns=['label','text'], header=False, index=False, formatters=\n",
    "                         {'label': '__label__{}'.format})\n",
    "s3r = boto3.resource('s3')\n",
    "s3r.Bucket(bucket).Object(os.path.join(prefix, 'blazing', train_file)).put(Body=blazing_text_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blazing_text_buffer = io.StringIO()\n",
    "validate.to_string(buf=blazing_text_buffer, columns=['label','text'], header=False, index=False, formatters=\n",
    "                         {'label': '__label__{}'.format})\n",
    "s3r.Bucket(bucket).Object(os.path.join(prefix, 'blazing', validate_file)).put(Body=blazing_text_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：训练 BlazingText 估算器\n",
    "\n",
    "在下一个单元格中，通过指定缺失值来取消注释并完成估算器代码。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bt_model = sagemaker.estimator.Estimator(container,\n",
    "#                                         sagemaker.get_execution_role(), \n",
    "#                                         instance_count=, \n",
    "#                                         instance_type=,\n",
    "#                                         volume_size = 30,\n",
    "#                                         max_run = 360000,\n",
    "#                                         input_mode= 'File',\n",
    "#                                         output_path=,\n",
    "#                                         sagemaker_session=\n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         sagemaker.get_execution_role(), \n",
    "                                         instance_count=1, \n",
    "                                         instance_type='ml.c4.4xlarge',\n",
    "                                         volume_size = 30,\n",
    "                                         max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sagemaker.Session())\n",
    "\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用以下超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置训练通道和验证通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{bucket}/{prefix}/blazing/{train_file}',\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    f's3://{bucket}/{prefix}/blazing/{validate_file}',\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels_3 = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：开始训练任务\n",
    "\n",
    "输入以下代码开始训练任务。（这个步骤可能需要花几分钟的时间。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "bt_model.fit(inputs=data_channels_3, logs=False)\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练任务完成后，请查看训练指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(bt_model._current_job_name, \n",
    "                                         metric_names = ['train:accuracy','validation:accuracy']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复制测试数据，使其可以被格式化，以使用模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_test = test.copy()\n",
    "bt_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集格式化为 BlazingText 所需的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bt_test['text'].str.strip()\n",
    "bt_test.replace(r'\\\\n','', regex=True, inplace = True)\n",
    "bt_test.rename(columns={'text':'source'}, inplace=True)\n",
    "bt_test.drop(columns='label', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bt_test.head().to_json(orient=\"records\", lines=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文件上载到 Amazon S3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_file = 'bt_input.json'\n",
    "blazing_text_buffer = io.StringIO()\n",
    "bt_test.to_json(path_or_buf=blazing_text_buffer, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3r.Bucket(bucket).Object(os.path.join(prefix, 'blazing', bt_file)).put(Body=blazing_text_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = f's3://{bucket}/{prefix}/blazing/'\n",
    "batch_input = f's3://{bucket}/{prefix}/blazing/{bt_file}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对测试数据使用批处理转换器。（这个步骤可能需要花几分钟的时间。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bt_transformer = bt_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m5.2xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "bt_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='application/jsonlines',\n",
    "                         split_type='Line')\n",
    "\n",
    "bt_transformer.wait(logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检索来自 Amazon S3 的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = s3.get_object(Bucket=bucket, Key=f'{prefix}/blazing/bt_input.json.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = pd.read_json(io.BytesIO(obj['Body'].read()),lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新格式化结果，以便计算混淆矩阵和指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_convert(label):\n",
    "    label = label[0].replace('__label__','')\n",
    "    return int(label)\n",
    "\n",
    "target_predicted_binary = target_predicted['label'].apply(binary_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(test['label'], target_predicted_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与之前的模型相比，BlazingText 的表现如何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.使用 Amazon Comprehend\n",
    "（[回到顶部](#Lab-4.1:-Implementing-Sentiment-Analysis)）\n",
    "\n",
    "在本节中，您将使用 Amazon Comprehend 来计算情绪。Amazon Comprehend 为您提供了正面和负面的结果，还显示了中立和喜忧参半的结果。Amazon Comprehend 是一项托管的服务，在使用它之前需要较少的文本处理。您无需处理本节中的任何文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看  `test` DataFrame 中的数据是什么样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Comprehend 的使用可以像 API 调用一样简单直接。\n",
    "\n",
    "以下单元格输出了来自 Amazon Comprehend 的前五个结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "comprehend = boto3.client(service_name='comprehend')\n",
    "for n in range(5):\n",
    "    text = test.iloc[n]['text']\n",
    "    response = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "    sentiment = response['Sentiment']\n",
    "    print(f'{sentiment} - {text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以启动预测任务来处理多个项目。必须将输入格式化为每行的单个输入，然后上载到 Amazon S3。文本的最大大小为 5120，因此 `str.slice(0,5000)`函数用于修剪长文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test file minus label to S3\n",
    "def upload_comprehend_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    \n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "comprehend_file = 'comprehend_input.csv'\n",
    "upload_comprehend_s3_csv(comprehend_file, 'comprehend', test['text'].str.slice(0,5000))\n",
    "test_url = f's3://{bucket}/{prefix}/comprehend/{comprehend_file}'\n",
    "print(f'Uploaded input to {test_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据上载到 Amazon S3 后，您可以使用 `start_sentiment_detection_jon`函数开始任务。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 挑战：配置 Amazon Comprehend 任务参数\n",
    "\n",
    "在下一个单元格中，配置 Amazon Comprehend 任务参数。\n",
    "– 在__input_data_config__中 - \n",
    "  –**S3Uri**：将 *`<S3_INPUT_GOES_HERE> `* 替换为之前定义的 `test_uri`\n",
    "  –**InputFormat**：将 *`<INPUT_FORMAT_GOES_HERE> `* 替换为 `ONE_DOC_PER_LINE`\n",
    "– 在__output_data config__ 中-  \n",
    "  –**S3Uri**：将 *`<S3_OUTPUT_GOES_HERE> `* 替换为 `s3_output_location`\n",
    "  –**data_access_role_arn**：将 *`arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW `* 替换为*实验室详细信息*文件中的 Amazon Resource Name (ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_config={\n",
    "    'S3Uri': 'S3_INPUT_GOES_HERE',\n",
    "    'InputFormat': 'INPUT_FORMAT_GOES_HERE'\n",
    "},\n",
    "\n",
    "output_data_config={\n",
    "    'S3Uri': 'S3_OUTPUT_GOES_HERE'\n",
    "},\n",
    "data_access_role_arn = 'arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW'\n",
    "\n",
    "### BEGIN_SOLUTION\n",
    "input_data_config={\n",
    "    'S3Uri': test_url,\n",
    "    'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "}\n",
    "output_data_config={\n",
    "    'S3Uri': s3_output_location\n",
    "}\n",
    "data_access_role_arn = 'arn:aws:iam::637423426529:role/service-role/c133864a3391494l8261467t1w-ComprehendDataAccessRole-qUxYBBIu9EvW'\n",
    "### END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，您已定义了任务参数，可以开始情绪检测任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.start_sentiment_detection_job(\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    DataAccessRoleArn=data_access_role_arn,\n",
    "    JobName='movie_sentiment',\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "print(response['JobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下单元格将循环进行，直到任务结束。（这个步骤可能需要花几分钟的时间。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "job_id = response['JobId']\n",
    "while True:\n",
    "    job_status=(comprehend.describe_sentiment_detection_job(JobId=job_id))\n",
    "    if job_status['SentimentDetectionJobProperties']['JobStatus'] in ['COMPLETED','FAILED']:\n",
    "        break            \n",
    "    else:\n",
    "        print('.', end='')\n",
    "    time.sleep(15)\n",
    "print((comprehend.describe_sentiment_detection_job(JobId=job_id))['SentimentDetectionJobProperties']['JobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务完成后，您可以通过调用 `describe_sentiment_detection_job`函数返回任务的详细信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=(comprehend.describe_sentiment_detection_job(JobId=job_id))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 **OutputDataConfig** 部分，您应该会看到 `S3Uri`。提取该 URI 将为您提供必须从 Amazon S3 下载的文件。您可以使用结果来计算指标，方式与使用算法计算批处理转换结果的方式相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_output_file = output['SentimentDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "comprehend_bucket, comprehend_key = comprehend_output_file.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\n",
    "s3r = boto3.resource('s3')\n",
    "s3r.meta.client.download_file(comprehend_bucket, comprehend_key, 'output.tar.gz')\n",
    "\n",
    "# Extract the tar file\n",
    "import tarfile\n",
    "tf = tarfile.open('output.tar.gz')\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应将提取的文件命名为 __output__。阅读此文件中的行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = ''\n",
    "with open ('output', \"r\") as myfile:\n",
    "    data = myfile.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这些行添加到数组中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for line in data:\n",
    "    json_data = json.loads(line)\n",
    "    results.append([json_data['Line'],json_data['Sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数组转换为 Pandas DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.DataFrame.from_records(results, index='index', columns=['index','sentiment'])\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果包含 **NEGATIVE**（负面）、**POSITIVE**（正面）、**NEUTRAL**（中立）和 **MIXED**（喜忧参半），而不是数值。要将这些结果与测试数据进行比较，可将它们映射到数值，如以下单元格所示。返回结果中的索引也是无序的。 `sort_index`函数应解决这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapper = {'NEGATIVE':0, 'POSITIVE':1, 'NEUTRAL':2, 'MIXED':3}\n",
    "c['sentiment']=c['sentiment'].replace(class_mapper)\n",
    "c = c.sort_index()\n",
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list to compare for Amazon Comprehend\n",
    "test_2 = test.reset_index()\n",
    "test_3 = test_2.sort_index()\n",
    "test_labels = test_3.iloc[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以使用 `plot_confusion_matrix`函数显示混淆矩阵。由于 Amazon Comprehend 的结果还包含__mixed__ 和 __neutral__，因此图表会有所不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, c['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于打印指标的现有函数无法正常工作，因为您的数据维度太多。以下代码单元格将计算相同的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, c['sentiment'])\n",
    "\n",
    "TN = cm[0,0]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "TP = cm[1,1]\n",
    "\n",
    "Sensitivity  = float(TP)/(TP+FN)*100\n",
    "# Specificity or true negative rate\n",
    "Specificity  = float(TN)/(TN+FP)*100\n",
    "# Precision or positive predictive value\n",
    "Precision = float(TP)/(TP+FP)*100\n",
    "# Negative predictive value\n",
    "NPV = float(TN)/(TN+FN)*100\n",
    "# Fall out or false positive rate\n",
    "FPR = float(FP)/(FP+TN)*100\n",
    "# False negative rate\n",
    "FNR = float(FN)/(TP+FN)*100\n",
    "# False discovery rate\n",
    "FDR = float(FP)/(TP+FP)*100\n",
    "# Overall accuracy\n",
    "ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "\n",
    "print(f\"Sensitivity or TPR: {Sensitivity}%\")    \n",
    "print(f\"Specificity or TNR: {Specificity}%\") \n",
    "print(f\"Precision: {Precision}%\")   \n",
    "print(f\"Negative Predictive Value: {NPV}%\")  \n",
    "print( f\"False Positive Rate: {FPR}%\") \n",
    "print(f\"False Negative Rate: {FNR}%\")  \n",
    "print(f\"False Discovery Rate: {FDR}%\" )\n",
    "print(f\"Accuracy: {ACC}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 恭喜！\n",
    "\n",
    "您已经完成了本实验室内容，现在可以按照实验室指南中的说明结束本实验室内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*©2023 Amazon Web Services, Inc. 或其联属公司。保留所有权利。未经 Amazon Web Services, Inc. 事先书面许可，不得复制或转载本文的部分或全部内容。禁止因商业目的复制、出借或出售本文。所有商标均为各自所有者的财产。*\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "metadata": {
   "interpreter": {
    "hash": "12bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
