{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验 3.5 - 学员笔记本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "本实验是模块 3 引导式实验的延续。\n",
    "\n",
    "在本实验中，您将部署经过训练的模型并根据该模型进行预测。然后，您将删除终端节点，并对测试数据集执行批量转换。\n",
    "\n",
    "\n",
    "## 业务场景简介\n",
    "\n",
    "您在一家医疗保健服务提供商工作，并希望改善骨科患者的异常检测。\n",
    "\n",
    "您的任务是利用机器学习 (ML) 解决此问题。您可以使用包含六个生物力学特征且目标为*正常*或*异常*的数据集。您可以使用此数据集训练 ML 模型，以预测患者是否会出现异常。\n",
    "\n",
    "\n",
    "## 关于该数据集\n",
    "\n",
    "该生物医学数据集由 Henrique da Mota 博士在法国里昂 Médico-Chirurgical de Réadaptation des Massues 中心的整形外科应用研究组 (GARO) 实习期间创建。这些数据分到两个不同但相关的分类任务中。\n",
    "\n",
    "第一项任务是将患者归类为以下三类之一： \n",
    "\n",
    "- *正常*（100 名患者）\n",
    "- *椎间盘疝*（60 名患者）\n",
    "- *脊椎滑脱*（150 名患者）\n",
    "\n",
    "对于第二个任务，则是将*椎间盘疝*和*脊椎滑脱*合并为一个类别，标记为*异常*。因此，在第二个任务中，患者属于以下两个类别之一：*正常*（100 名患者）或*异常*（210 名患者）。\n",
    "\n",
    "\n",
    "## 属性信息\n",
    "\n",
    "数据集中的每名患者都由六个生物力学属性表示，这些属性（顺序如下）是根据骨盆和腰椎的形状和方向得出的： \n",
    "\n",
    "- 骨盆入射角\n",
    "- 骨盆倾斜角\n",
    "- 腰椎前凸角\n",
    "- 骶骨倾斜角\n",
    "- 骨盆半径\n",
    "- 脊椎滑脱等级\n",
    "\n",
    "以下约定用于分类标签： \n",
    "- 椎间盘疝 (DH)\n",
    "- 脊椎滑脱 (SL)\n",
    "- 正常 (NO) \n",
    "- 异常 (AB)\n",
    "\n",
    "有关此数据集的更多信息，请参阅[脊柱数据集网页](http://archive.ics.uci.edu/ml/datasets/Vertebral+Column)。\n",
    "\n",
    "\n",
    "## 数据集属性\n",
    "\n",
    "该数据集来自：\n",
    "Dua, D. 和 Graff, C.（2019 年）。UCI 机器学习存储库 (http://archive.ics.uci.edu/ml)。加州尔湾市：加利福尼亚大学信息与计算机科学学院。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验设置\n",
    "\n",
    "由于此解决方案分散在模块中的多个实验中，因此您需要执行以下单元格中的内容，以便加载数据并训练要部署的模型。\n",
    "\n",
    "**注意：设置过程最多可能需要 5 分钟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据\n",
    "\n",
    "通过执行以下单元格中的内容，将导入数据并让数据可供使用。\n",
    "\n",
    "**注意**：以下单元格中的内容代表以前的实验中的关键步骤。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c130335a3301602l7984601t1w760730171266-labbucket-o4qqi2tcy0re'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/samhsiao/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import warnings, requests, zipfile, io\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     37\u001b[0m     dataframe\u001b[38;5;241m.\u001b[39mto_csv(csv_buffer, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m     s3_resource\u001b[38;5;241m.\u001b[39mBucket(bucket)\u001b[38;5;241m.\u001b[39mObject(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(prefix, folder, filename))\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m     39\u001b[0m         Body\u001b[38;5;241m=\u001b[39mcsv_buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[43mupload_s3_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m upload_s3_csv(test_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, test)\n\u001b[1;32m     45\u001b[0m upload_s3_csv(validate_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m\"\u001b[39m, validate)\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mupload_s3_csv\u001b[0;34m(filename, folder, dataframe)\u001b[0m\n\u001b[1;32m     36\u001b[0m csv_buffer \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO()\n\u001b[1;32m     37\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mto_csv(csv_buffer, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43ms3_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mObject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PythonVenv/envAWS/lib/python3.10/site-packages/boto3/resources/factory.py:581\u001b[0m, in \u001b[0;36mResourceFactory._create_action.<locals>.do_action\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 581\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# Clear cached data. It will be reloaded the next\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# time that an attribute is accessed.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# TODO: Make this configurable in the future?\u001b[39;00m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PythonVenv/envAWS/lib/python3.10/site-packages/boto3/resources/action.py:88\u001b[0m, in \u001b[0;36mServiceAction.__call__\u001b[0;34m(self, parent, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m     81\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     parent\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mservice_name,\n\u001b[1;32m     84\u001b[0m     operation_name,\n\u001b[1;32m     85\u001b[0m     params,\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, response)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_handler(parent, params, response)\n",
      "File \u001b[0;32m~/Documents/PythonVenv/envAWS/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PythonVenv/envAWS/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records."
     ]
    }
   ],
   "source": [
    "f_zip = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00212/vertebral_column_data.zip\"\n",
    "r = requests.get(f_zip, stream=True)\n",
    "Vertebral_zip = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "Vertebral_zip.extractall()\n",
    "\n",
    "data = arff.loadarff(\"column_2C_weka.arff\")\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "class_mapper = {b\"Abnormal\": 1, b\"Normal\": 0}\n",
    "df[\"class\"] = df[\"class\"].replace(class_mapper)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "\n",
    "train, test_and_validate = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"class\"]\n",
    ")\n",
    "test, validate = train_test_split(\n",
    "    test_and_validate,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=test_and_validate[\"class\"],\n",
    ")\n",
    "\n",
    "prefix = \"lab3\"\n",
    "\n",
    "train_file = \"vertebral_train.csv\"\n",
    "test_file = \"vertebral_test.csv\"\n",
    "validate_file = \"vertebral_validate.csv\"\n",
    "\n",
    "s3_resource = boto3.Session().resource(\"s3\")\n",
    "\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False)\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(\n",
    "        Body=csv_buffer.getvalue()\n",
    "    )\n",
    "\n",
    "\n",
    "upload_s3_csv(train_file, \"train\", train)\n",
    "upload_s3_csv(test_file, \"test\", test)\n",
    "upload_s3_csv(validate_file, \"validate\", validate)\n",
    "\n",
    "container = retrieve(\"xgboost\", boto3.Session().region_name, \"1.0-1\")\n",
    "\n",
    "hyperparams = {\"num_round\": \"42\", \"eval_metric\": \"auc\", \"objective\": \"binary:logistic\"}\n",
    "\n",
    "s3_output_location = \"s3://{}/{}/output/\".format(bucket, prefix)\n",
    "xgb_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters=hyperparams,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")\n",
    "\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket, prefix, train_file), content_type=\"text/csv\"\n",
    ")\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket, prefix, validate_file),\n",
    "    content_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": train_channel, \"validation\": validate_channel}\n",
    "\n",
    "xgb_model.fit(inputs=data_channels, logs=False)\n",
    "\n",
    "print(\"ready for hosting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 1：托管模型\n",
    "\n",
    "现在，您已经拥有经过训练的模型，您可以使用 Amazon SageMaker 托管服务来托管它。\n",
    "\n",
    "第一步是部署模型。由于您有模型对象 *xgb_model*，因此可以使用 **deploy** 方法。在本实验中，将使用单个 ml.m4.xlarge 实例。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                serializer = sagemaker.serializers.CSVSerializer(),\n",
    "                instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2：执行预测\n",
    "\n",
    "现在，您已部署模型，您可以运行一些预测。\n",
    "\n",
    "首先，检查测试数据并重新熟悉它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您有 31 个实例，7 个属性。前五个实例是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您不需要将目标值 (class) 包括在内。该预测器可以接受采用逗号分隔值 (CSV) 格式的数据。因此，您可以使用以下代码获得*不包含 class 列的*第 1 行：\n",
    "\n",
    "`test.iloc[:1,1:]` \n",
    "\n",
    "**iloc** 函数接受参数 [*rows*,*cols*]\n",
    "\n",
    "要仅获得第 1 行，请使用 `0:1`。如果要获得第 2 行，则可以使用 `1:2`。\n",
    "\n",
    "要获得第一列 (*col 0*) *以外*的所有列，请使用 `1:`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = test.iloc[0:1,1:]\n",
    "row.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以将其转换为逗号分隔值 (CSV) 文件，并将其存储在字符串缓冲区中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X_csv_buffer = io.StringIO()\n",
    "row.to_csv(batch_X_csv_buffer, header=False, index=False)\n",
    "test_row = batch_X_csv_buffer.getvalue()\n",
    "print(test_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，您可以使用这些数据执行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.predict(test_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您得到的结果不是 *0* 或 *1*，而是*概率得分*。您可以对概率分数应用条件逻辑，以确定结果应显示为 0 还是 1。您将在进行批量预测时使用此操作。\n",
    "\n",
    "现在，将结果与测试数据进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：预测准确吗？\n",
    "\n",
    "**挑战任务**：更新之前的代码以发送数据集的第二行。这些预测正确吗？ 对其他几行尝试执行此任务。\n",
    "\n",
    "一次发送一行可能很麻烦。您可以编写函数来批量提交这些值，但 SageMaker 本身已具备批处理功能。接下来，您将了解该功能。但在这之前，您需要终止模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 3：终止已部署的模型\n",
    "\n",
    "要删除终端节点，请在预测器上使用 **delete_endpoint** 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 4：执行批次转换\n",
    "\n",
    "在进行训练、测试、特征工程循环期间，您希望在模型上测试保留或测试集。然后，您可以使用这些结果来计算指标。您可以像之前那样部署终端节点，但是之后必须记住删除该终端节点。但是，有一种更高效的方法。\n",
    "\n",
    "您可以使用模型的转换器方法来获取转换器对象。然后，您可以使用此对象的转换器方法对整个测试数据集执行预测。SageMaker 将： \n",
    "\n",
    "- 启动具有该模型的实例\n",
    "- 对所有输入值执行预测\n",
    "- 将这些值写入 Amazon Simple Storage Service (Amazon S3) \n",
    "- 最后，终止实例\n",
    "\n",
    "首先，将数据转换为 CSV 文件，以便转换器对象可以将其作为输入。这次，您将使用 **iloc** 以获取所有行以及除第一列外的所有列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X = test.iloc[:,1:];\n",
    "batch_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，将数据写入 CSV 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X_file='batch-in.csv'\n",
    "upload_s3_csv(batch_X_file, 'batch-in', batch_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，在执行转换之前，需要配置转换器的输入文件、输出位置和实例类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgb_transformer = xgb_model.transformer(instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换完成后，您可以从 Amazon S3 下载结果并将其与输入进行比较。\n",
    "\n",
    "首先，从 Amazon S3 下载输出并将其加载到 pandas DataFrame 中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),',',names=['class'])\n",
    "target_predicted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以使用函数将概率转换为 *0* 或 *1*。\n",
    "\n",
    "第一个表输出将是*预测值*，第二个表输出将是*原始测试数据*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_convert(x):\n",
    "    threshold = 0.65\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted['binary'] = target_predicted['class'].apply(binary_convert)\n",
    "\n",
    "print(target_predicted.head(10))\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：**binary_convert** 函数中的 *threshold* 设置为 *0.65*。\n",
    "\n",
    "**挑战任务**：尝试更改阈值。它会影响结果吗？\n",
    "\n",
    "**注意**：初始模型可能并不好。您将在下一个实验中生成一些指标，然后在最后一个实验中优化模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 恭喜！\n",
    "\n",
    "您已经完成了本实验，现在可以按照实验指南中的说明结束本实验。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envAWS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
