{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题：预测航班延误\n",
    "\n",
    "该笔记本的目标是：\n",
    "- 处理下载的 .zip 文件并利用其创建数据集\n",
    "- 执行探索性数据分析 (EDA)\n",
    "- 建立基准模型\n",
    "- 将简单模型转化为集成模型\n",
    "- 执行超级参数优化\n",
    "- 确认特征重要性\n",
    "\n",
    "\n",
    "## 业务场景简介\n",
    "\n",
    "您在一家行程预订网站工作，该网站希望能改善航班延误的旅客的客户体验。该公司希望开发一种功能，让乘客在预订美国国内最繁忙机场的到港或离港航班时，能够知道航班是否会因天气原因而延误。\n",
    "\n",
    "您的任务是利用机器学习 (ML) 来确定航班是否会因天气原因而延误，从而在一定程度上解决这一问题。您可以访问一个有关大型航空公司国内航班的准点率的数据集。您可以使用这一数据来训练 ML 模型，以便预测最繁忙机场的航班是否会延误。\n",
    "\n",
    "\n",
    "## 关于该数据集\n",
    "\n",
    "该数据集包含经过认证的美国航空公司报告的预定和实际的出发与到达时间。对于预定行程的国内旅客产生的收入，这些航空公司至少占据了 1%。数据由美国交通统计局 (BTS) 下属的航空公司信息办公室收集。数据集包含 2013 到 2018 年间航班的日期、时刻、出发地、目的地、航空公司、航程以及航班延误状态。\n",
    "\n",
    "\n",
    "### 特征\n",
    "\n",
    "有关数据集内的特征的更多信息，请参阅 [准点延误数据集特征](https://www.transtats.bts.gov/Fields.asp)。\n",
    "\n",
    "### 数据集属性  \n",
    "网站：https://www.transtats.bts.gov/\n",
    "\n",
    "本实验使用的数据集由美国交通统计局 (BTS) 下属的航空公司信息办公室编制，取自航空公司准点率数据库，网站为 https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&amp;DB_URL=Mode_ID=1&amp;Mode_Desc=Aviation&amp;Subject_ID2=0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 1：问题定义和数据收集\n",
    "\n",
    "在开始这个项目时，先用几句话来总结在这个场景中要解决的业务问题和要实现的业务目标。您可以在以下各部分写下想法。同时说明您想让自己的团队衡量的业务指标。填写这些信息后，编写 ML 问题陈述。最后，添加一两条备注，说明此活动所对应的机器学习的类型。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中总结这些详细信息。</span>\n",
    "\n",
    "### 1.确定 ML 是否是适合在此场景中进行部署的解决方案以及原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.定义业务问题、成功指标和期望的 ML 输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.确定您要解决的 ML 问题的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.分析您要使用的数据是否合适。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置\n",
    "\n",
    "您已经决定要关注的部分，现在您可以设置此实验，以便开始解决问题。\n",
    "\n",
    "**注意**：该笔记本是在具有 25 GB 存储的 `ml.m4.xlarge` 笔记本实例上创建和测试的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib2 import Path\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "instance_type='ml.m4.xlarge'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 2：数据预处理和可视化  \n",
    "在数据预处理阶段，您将探索数据并将其可视化，以便更好地了解数据。首先，导入必要的库并将数据读入 pandas DataFrame。导入数据后，浏览数据集。查看数据集的形状，探索要使用的列及其类型（数值型、分类型）。对各项特征执行基本统计，以便了解特征的均值和范围；深入分析目标列并确定其分布。\n",
    "\n",
    "\n",
    "### 要考虑的特定问题\n",
    "\n",
    "在本实验的这一整个部分中，请考虑以下问题：\n",
    "\n",
    "1.您可以从对特征执行的基本统计中推断出什么？ \n",
    "2.您可以从目标类的分布中推断出什么？\n",
    "3.您从数据探索中还可以推断出什么？\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中总结这些问题和其他类似问题的答案。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先将公有 Amazon Simple Storage Service (Amazon S3) 存储桶中的数据集引入这个笔记本环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_1.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_1.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_12.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_12.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_2.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_2.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_11.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_11.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_3.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_3.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_4.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_4.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_5.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_5.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_10.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_10.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_8.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_8.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_7.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_7.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_9.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_9.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_6.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_6.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_1.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_1.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_10.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_10.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_12.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_12.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_4.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_4.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_2.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_2.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_11.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_11.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_3.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_3.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_5.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_5.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_6.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_6.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_7.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_7.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_8.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_8.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_1.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_1.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_9.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_9.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_2.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_2.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_11.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_11.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_10.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_10.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_12.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_12.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_3.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_3.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_4.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_4.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_6.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_6.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_5.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_5.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_7.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_7.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_8.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_8.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_9.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_9.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_1.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_1.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_11.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_11.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_2.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_2.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_10.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_10.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_12.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_12.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_3.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_3.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_4.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_4.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_6.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_6.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_7.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_7.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_5.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_5.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_8.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_8.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_9.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_9.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_1.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_1.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_10.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_10.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_12.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_12.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_11.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_11.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_2.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_2.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_3.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_3.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_9.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_9.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_8.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_8.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_7.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_7.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_4.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_4.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_6.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_6.zip\n",
      "download: s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_5.zip to ../project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_5.zip\n"
     ]
    }
   ],
   "source": [
    "# download the files\n",
    "\n",
    "zip_path = '/home/ec2-user/SageMaker/project/data/FlightDelays/'\n",
    "base_path = '/home/ec2-user/SageMaker/project/data/FlightDelays/'\n",
    "csv_base_path = '/home/ec2-user/SageMaker/project/data/csvFlightDelays/'\n",
    "\n",
    "!mkdir -p {zip_path}\n",
    "!mkdir -p {csv_base_path}\n",
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data/ {zip_path} --recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_files = [str(file) for file in list(Path(base_path).iterdir()) if '.zip' in str(file)]\n",
    "len(zip_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 .zip 文件中提取逗号分隔值 (CSV) 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_2.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_8.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_1.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_12.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_1.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_1.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_6.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_9.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_7.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_8.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_6.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_3.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_4.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_7.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_4.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_10.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_3.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_7.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_10.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_9.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_11.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_5.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_5.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_10.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_12.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_2.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_4.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_12.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_3.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_8.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_10.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_6.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_5.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_11.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_5.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_12.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_8.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_2.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_9.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_3.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_9.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_2.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_7.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_4.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_7.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_9.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_6.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_3.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_11.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_1.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_6.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2016_11.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_4.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_10.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_2.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_11.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2018_8.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2015_1.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_5.zip \n",
      "Extracting /home/ec2-user/SageMaker/project/data/FlightDelays/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2017_12.zip \n",
      "Files Extracted\n"
     ]
    }
   ],
   "source": [
    "def zip2csv(zipFile_name , file_path):\n",
    "    \"\"\"\n",
    "    Extract csv from zip files\n",
    "    zipFile_name: name of the zip file\n",
    "    file_path : name of the folder to store csv\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with ZipFile(zipFile_name, 'r') as z: \n",
    "            print(f'Extracting {zipFile_name} ') \n",
    "            z.extractall(path=file_path) \n",
    "    except:\n",
    "        print(f'zip2csv failed for {zipFile_name}')\n",
    "\n",
    "for file in zip_files:\n",
    "    zip2csv(file, csv_base_path)\n",
    "\n",
    "print(\"Files Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files = [str(file) for file in list(Path(csv_base_path).iterdir()) if '.csv' in str(file)]\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在加载 CSV 文件前，请先阅读提取的文件夹中的 HTML 文件。该 HTML 文件介绍了相关背景以及有关数据集内包含的特征的更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"../project/data/csvFlightDelays/readme.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc4cabfbb20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=os.path.relpath(f\"{csv_base_path}readme.html\"), width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载示例 CSV 文件\n",
    "\n",
    "在合并所有 CSV 文件之前，请检查单个 CSV 文件中的数据。使用 pandas 时，请先读取 `On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2018_9.csv` 文件。您可以使用内置的 Python `read_csv` 函数 ([pandas.read_csv documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html))。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(f\"{csv_base_path}On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2018_9.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集内的行数和列数，并打印列名。\n",
    "\n",
    "**提示**：要查看 DataFrame 的行和列，请使用 `<DataFrame>.shape` function. To view the column names, use the `` 函数。要查看列名，请使用 `<DataFrame>.columns` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3942653770.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_shape = # **ENTER YOUR CODE HERE**\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df_shape = # **ENTER YOUR CODE HERE**\n",
    "print(f'Rows and columns in one CSV file is {df_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集的前 10 行。 \n",
    "\n",
    "**提示**：要打印 `x` 行，请使用 pandas 内置函数 `head(x)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集里的所有列。要查看列名，使用 `<DataFrame>.columns`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in <CODE>:# **ENTER YOUR CODE HERE**\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印数据集里包含 *Del* 一词的所有列。这可以让您了解包含*延误数据*的列有多少。\n",
    "\n",
    "**提示**：您可以使用 Python 的列表推导功能来列出符合特定 `if` 语句条件的值。\n",
    "\n",
    "例如：`[x for x in [1,2,3,4,5] if x > 2]`  \n",
    "\n",
    "**提示**：您可以使用 `in` 关键字（[关键字文档中的 Python 部分](https://www.w3schools.com/python/ref_keyword_in.asp)）来确认值是否位于列表中。\n",
    "\n",
    "例如：`5 in [1,2,3,4,5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下列问题也可以帮助您了解有关数据集的更多信息。\n",
    "\n",
    "**问题**   \n",
    "\n",
    "1.数据集有多少个行和列？   \n",
    "2.数据集里包含多少个年份？   \n",
    "3.数据集的日期范围是多少？   \n",
    "4.数据集里包含哪些航空公司？   \n",
    "5.数据集里包含哪些出发地和目的地机场？\n",
    "\n",
    "**提示**\n",
    "- 要显示 DataFrame 的维度，请使用 `df_temp.shape`。\n",
    "- 要引用特定列，请使用 `df_temp.columnName`（例如，`df_temp.CarrierDelay`）。\n",
    "- 要获取列的唯一值，请使用 `df_temp.column.unique()`（例如`df_temp.Year.unique()`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", <CODE>)\n",
    "print(\"The months covered in this dataset are: \", <CODE>)\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：出发地和目的地机场的总数分别是多少？\n",
    "\n",
    "**提示**：要使用 **Origin** 和 **Dest** 列查找每个机场的值，可以在 pandas 中使用 `values_count` 函数（[pandas.Series.value_counts 文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame({'Origin':<CODE>, 'Destination':<CODE>})\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：根据数据集里的航班数量，打印出前 15 个出发地和目的地机场。\n",
    "\n",
    "**提示**：您可以在 pandas 中使用 `sort_values` 函数（[pandas.DataFrame.sort_values 文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sort_values(by=<CODE>,ascending=False).head(15) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**掌握了有关某个航班的所有信息后，您能否预测其是否会延误？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ArrDel15** 列是指示变量，当航班延误超过 15 分钟时，其值为 *1*，否则为 *0*。\n",
    "\n",
    "您可以将该列用作分类问题的目标列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设您要从旧金山到洛杉矶出差。您想更好地管理在洛杉矶的预订。因此，想要了解在给定一组特征的情况下，您的航班是否会延误。在登机之前，您需要了解这个数据集里的多少特征？\n",
    "\n",
    "`DepDelay`、`ArrDelay`、`CarrierDelay`、`WeatherDelay`、`NASDelay`、`SecurityDelay`、`LateAircraftDelay` 和 `DivArrDelay` 等列包含有关延误的信息。但是，这种延误可能发生在出发地，也可能发生在目的地。如果航班因天气骤变而导致延误了 10 分钟才到达，则这种数据对您预订洛杉矶的酒店没有帮助。\n",
    "\n",
    "因此，要简化问题陈述，请考虑用以下各列来预测到达延误：<br>\n",
    "\n",
    "`Year`、`Quarter`、`Month`、`DayofMonth`、`DayOfWeek`、`FlightDate`、`Reporting_Airline`、`Origin`、`OriginState`、`Dest`、`DestState`、`CRSDepTime`、`DepDelayMinutes`、`DepartureDelayGroups`、`Cancelled`、`Diverted`、`Distance`、`DistanceGroup`、`ArrDelay`、`ArrDelayMinutes`、`ArrDel15`、`AirTime`\n",
    "\n",
    "您也可以将出发地和目的地机场筛选为：\n",
    "- 热门机场：ATL、ORD、DFW、DEN、CLT、LAX、IAH、PHX、SFO\n",
    "- 排名前五的航空公司：UA、OO、WN、AA、DL\n",
    "\n",
    "这些信息应有助于减少将要合并的 CSV 文件中的数据大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合并所有 CSV 文件\n",
    " \n",
    "首先，创建一个空 DataFrame，用于复制每个文件中的 DataFrame。然后，对于 `csv_files` 列表中的每个文件：\n",
    "\n",
    "1.将 CSV 文件读入 DataFrame \n",
    "2.根据 `filter_cols` 变量筛选列\n",
    "\n",
    "```\n",
    "        columns = ['col1', 'col2']\n",
    "        df_filter = df[columns]\n",
    "```\n",
    "\n",
    "3.每个 `subset_cols` 中仅保留 `subset_vals`。要检查 `val` 是否在 DataFrame 列中，请使用 pandas 的 `isin` 函数（[pandas.DataFram.isin 文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html)）。然后选择包含它的行。\n",
    "\n",
    "```\n",
    "        df_eg[df_eg['col1'].isin('5')]\n",
    "```\n",
    "\n",
    "4.将该 DataFrame 与空 DataFrame 合并 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(csv_files, filter_cols, subset_cols, subset_vals, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    Combine csv files into one Data Frame\n",
    "    csv_files: list of csv file paths\n",
    "    filter_cols: list of columns to filter\n",
    "    subset_cols: list of columns to subset rows\n",
    "    subset_vals: list of list of values to subset rows\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for file in csv_files:\n",
    "        df_temp = pd.read_csv(file)\n",
    "        df_temp = df_temp[filter_cols]\n",
    "        for col, val in zip(subset_cols,subset_vals):\n",
    "            df_temp = df_temp[df_temp[col].isin(val)]      \n",
    "        \n",
    "        df = pd.concat([df, df_temp], axis=0)\n",
    "      \n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f'Combined csv stored at {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols is the list of columns to predict Arrival Delay \n",
    "cols = ['Year','Quarter','Month','DayofMonth','DayOfWeek','FlightDate',\n",
    "        'Reporting_Airline','Origin','OriginState','Dest','DestState',\n",
    "        'CRSDepTime','Cancelled','Diverted','Distance','DistanceGroup',\n",
    "        'ArrDelay','ArrDelayMinutes','ArrDel15','AirTime']\n",
    "\n",
    "subset_cols = ['Origin', 'Dest', 'Reporting_Airline']\n",
    "\n",
    "# subset_vals is a list collection of the top origin and destination airports and top 5 airlines\n",
    "subset_vals = [['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'], \n",
    "               ['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'], \n",
    "               ['UA', 'OO', 'WN', 'AA', 'DL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上一个函数将所有不同文件合并到一个可以轻松读取的文件中。\n",
    "\n",
    "**注意**：这一过程需要 5 到 7 分钟才能完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "combined_csv_filename = f\"{base_path}combined_files.csv\"\n",
    "combine_csv(csv_files, cols, subset_cols, subset_vals, combined_csv_filename)\n",
    "print(f'CSVs merged in {round((time.time() - start)/60,2)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集\n",
    "\n",
    "加载已合并的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(combined_csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印前五个记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下列问题也可以帮助您了解有关数据集的更多信息。\n",
    "\n",
    "**问题**   \n",
    "\n",
    "1.数据集有多少个行和列？   \n",
    "2.数据集里包含多少个年份？   \n",
    "3.数据集的日期范围是多少？   \n",
    "4.数据集里包含哪些航空公司？   \n",
    "5.数据集里包含哪些出发地和目的地机场？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", list(<CODE>))\n",
    "print(\"The months covered in this dataset are: \", sorted(list(<CODE>)))\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义目标列：**is_delay**（*1* 表示到达时间延误 15 分钟以上，*0* 表示所有其他情况）。要将 **ArrDel15** 列重命名为 **is_delay**，使用 `rename` 方法。\n",
    "\n",
    "**提示**：您可以在 pandas 中使用 `rename` 函数（[pandas.DataFrame.rename 文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)）。\n",
    "\n",
    "例如：\n",
    "```\n",
    "data.rename(columns={'col1':'column1'}, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns=<CODE>, inplace=True) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查找各列的空值。您可以使用 `isnull()` 函数（[pandas.isnull 文档](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.isnull.html)）。\n",
    "\n",
    "**提示**：`isnull()` 检测特定值是否为空值。它会在其位置返回一个布尔值（*True* 或 *False*）。要对列数求和，使用 `sum(axis=0)` 函数（例如，`df.isnull().sum(axis=0)`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 1658130 个行中，有 22540 个行缺少航班延误详情和飞行时间，占 1.3%。您可以删除或替换这些行。该文档没有提及有关丢失行的任何信息。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove null columns\n",
    "data = data[~data.is_delay.isnull()]\n",
    "data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 CRSDepTime 转化为 24 小时制时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DepHourofDay'] = (data['CRSDepTime']//100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ML 问题陈述**\n",
    "- 掌握了一组特征后，您能否预测出某个航班是否会延误 15 分钟以上？\n",
    "- 目标变量只会有 *0* 和 *1* 两个值，因此您可以使用分类算法。\n",
    "\n",
    "在开始建模前，最好先查看特征的分布和相关性等信息。\n",
    "- 这可以让您了解数据中的任何非线性或模式。\n",
    "    - 线性模型：添加幂、指数或交互特征\n",
    "    - 尝试非线性模型\n",
    "- 数据不平衡 \n",
    "    - 选择不会给模型性能带来偏差的指标（准确率与曲线下面积（或称 AUC）之比）\n",
    "    - 使用加权或自定义损失函数\n",
    "- 缺少数据\n",
    "    - 基于简单的统计数据进行替换：均值、中位数、众数（数值变量）和频繁类（分类变量）\n",
    "    - 基于聚类进行替换（用 k 最近邻算法（或称 KNN）预测列值）\n",
    "    - 删除列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据探索\n",
    "\n",
    "检查类别*延误*与*不延误*。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.groupby('is_delay').size()/len(data) ).plot(kind='bar')# Enter your code here\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：您能从柱状图中推断出有关*延误*率和*不延误*率的什么信息？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以下两个单元格的内容并回答问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_columns = ['Month', 'DepHourofDay', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest']\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20,20), squeeze=False)\n",
    "# fig.autofmt_xdate(rotation=90)\n",
    "\n",
    "for idx, column in enumerate(viz_columns):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    temp = data.groupby(column)['is_delay'].value_counts(normalize=True).rename('percentage').\\\n",
    "    mul(100).reset_index().sort_values(column)\n",
    "    sns.barplot(x=column, y=\"percentage\", hue=\"is_delay\", data=temp, ax=ax)\n",
    "    plt.ylabel('% delay/no-delay')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"is_delay\", y=\"Distance\", data=data, fit_reg=False, hue='is_delay', legend=False)\n",
    "plt.legend(loc='center')\n",
    "plt.xlabel('is_delay')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**\n",
    "\n",
    "使用先前图表中的数据，回答以下问题：\n",
    "\n",
    "- 哪个月的延误次数最多？\n",
    "- 一天之中什么时候的延误次数最多？\n",
    "- 星期几的延误次数最多？\n",
    "- 哪家航空公司的延误次数最多？\n",
    "- 哪个出发地和目的地机场的延误次数最多？\n",
    "- 航程是不是与延误有关的一项因素？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征\n",
    "\n",
    "查看所有列及其具体类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "筛选出所需的列：\n",
    "- *Date* 是多余的数据，因为已经有 *Year*、*Quarter*、*Month*、*DayofMonth* 和 *DayOfWeek* 可以描述日期。\n",
    "- 使用 *Origin* 和 *Dest* 代码，而不是 *OriginState* 和 *DestState*。\n",
    "- 因为您只是在对航班是否延误进行分类，所以不需要 *TotalDelayMinutes*、*DepDelayMinutes* 和 *ArrDelayMinutes*。\n",
    "\n",
    "将 *DepHourofDay* 视为分类变量，因为它与目标没有数量关系。\n",
    "- 如果需要对此变量进行独热编码，则会多出 23 列。\n",
    "- 处理分类变量的其他方法包括进行哈希编码、进行正则化均值编码以及将值拆分到存储桶中等等。\n",
    "- 在这种情况下，只需要拆分为多个存储桶即可。\n",
    "\n",
    "要将列类型更改为类别，使用 `astype` 函数（[pandas.DataFrame.astype 文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = data.copy()\n",
    "data = data[[ 'is_delay', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay']]\n",
    "categorical_columns  = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'DepHourofDay']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使用独热编码，针对选择的分类数据列使用 Pandas 函数 `get_dummies`。然后可以使用 Pandas 函数 `concat` 将生成的这些特征合并到原始数据集中。要对分类变量进行编码，您也可以使用关键字 `drop_first=True` 来进行*虚拟编码*。有关虚拟编码的更多信息，请参阅[虚拟编码（统计数据）](https://en.wikiversity.org/wiki/Dummy_variable_(statistics))。\n",
    "\n",
    "例如：\n",
    "```\n",
    "pd.get_dummies(df[['column1','columns2']], drop_first=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(<CODE>, drop_first=True) # Enter your code here\n",
    "data = pd.concat([<CODE>, <CODE>], axis = 1)\n",
    "data.drop(categorical_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认数据集和新列的长度。\n",
    "\n",
    "**提示**：使用 `shape` 和 `columns` 属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您现在可以训练模型了。在拆分数据之前，将 **is_delay** 列重命名为 *target*。\n",
    "\n",
    "**提示**：您可以在 pandas 中使用 `rename` 函数（[pandas.DataFrame.rename 文档](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> 步骤 2 结束 </span>\n",
    "\n",
    "将项目文件保存到本地计算机。按照以下步骤进行操作：\n",
    "\n",
    "1.在左侧的文件资源管理器中，右键单击您要处理的笔记本。\n",
    "\n",
    "2.选择 **Download**（下载），然后将文件保存到本地。 \n",
    "\n",
    "此操作会将当前的笔记本下载到计算机上默认的下载文件夹中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 3：模型训练和评估\n",
    "\n",
    "在将数据集从 DataFrame 转换为可供机器学习算法使用的格式之前，您必须完成一些预备步骤。对于 Amazon SageMaker，您必须执行以下步骤：\n",
    "\n",
    "1.使用 `sklearn.model_selection.train_test_split` 将数据分为 `train_data`、`validation_data` 和 `test_data`。 \n",
    "\n",
    "2.将数据集转换为适合 Amazon SageMaker 训练作业使用的文件格式。可以是 CSV 文件或记录 protobuf。有关更多信息，请参阅 [适用于训练的常见数据格式](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html)。 \n",
    "\n",
    "3.将数据上传到 S3 存储桶。如果您从未创建过存储桶，请参阅 [创建存储桶](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html)。 \n",
    "\n",
    "使用以下单元格完成这些步骤。根据需要插入和删除单元格。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在您的项目演示中，写下您在此阶段所做的关键决策。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练测试拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(data):\n",
    "    train, test_and_validate = train_test_split(data, test_size=0.2, random_state=42, stratify=data['target'])\n",
    "    test, validate = train_test_split(test_and_validate, test_size=0.5, random_state=42, stratify=test_and_validate['target'])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_data(data)\n",
    "print(train['target'].value_counts())\n",
    "print(test['target'].value_counts())\n",
    "print(validate['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**示例答案**\n",
    "```\n",
    "0.0 1033570\n",
    "1.0 274902\n",
    "名称：target，dtype：int64\n",
    "0.0 129076\n",
    "1.0 34483\n",
    "名称：target，dtype：int64\n",
    "0.0 129612\n",
    "1.0 33947\n",
    "名称：target，dtype：int64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基准分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet\n",
    "import boto3\n",
    "\n",
    "# Instantiate the LinearLearner estimator object with 1 ml.m4.xlarge\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               instance_count=<CODE>,\n",
    "                                               instance_type=<CODE>,\n",
    "                                               predictor_type=<CODE>,\n",
    "                                               binary_classifier_model_selection_criteria=<CODE>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例代码\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels))\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                              instance_count=1,\n",
    "                                              instance_type='ml.m4.xlarge',\n",
    "                                              predictor_type='binary_classifier',\n",
    "                                              binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "                                              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性学习器接受 protobuf 或 CSV 内容类型的训练数据。它还接受 protobuf、CSV 或 JavaScript 对象表示法 (JSON) 内容类型的推理请求。训练数据有特征和 ground-truth 标签，而推理请求中的数据只有特征。\n",
    "\n",
    "在生产管道中，AWS 建议将数据转换为 Amazon SageMaker protobuf 格式并将其存储在 Amazon S3 中。为了快速启动并运行，AWS 提供 `record_set` 操作，让您 能够在数据集小到足以装入本地内存时来进行转换和上传。该方法接受您已有的 NumPy 数组，因此您将在此步骤中使用它。`RecordSet` 对象将跟踪数据的 Amazon S3 临时位置。使用 `estimator.record_set` 函数创建训练、验证和测试记录。然后，使用 `estimator.fit` 函数开始训练作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create train, validate, and test records\n",
    "train_records = classifier_estimator.record_set(train.values[:, 1:].astype(np.float32), train.values[:, 0].astype(np.float32), channel='train')\n",
    "val_records = classifier_estimator.record_set(validate.values[:, 1:].astype(np.float32), validate.values[:, 0].astype(np.float32), channel='validation')\n",
    "test_records = classifier_estimator.record_set(test.values[:, 1:].astype(np.float32), test.values[:, 0].astype(np.float32), channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，使用您上传的数据集训练模型。\n",
    "\n",
    "### 示例代码\n",
    "```\n",
    "linear.fit([train_records,val_records,test_records])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the classifier\n",
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "在本节中，您将评估自己训练的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，检查训练作业的指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(classifier_estimator._current_job_name, \n",
    "                                         metric_names = ['test:objective_loss', \n",
    "                                                         'test:binary_f_beta',\n",
    "                                                         'test:precision',\n",
    "                                                         'test:recall']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，设置一些函数，用于将测试数据加载到 Amazon S3 中，并使用批量预测函数执行预测。使用批批量预测将有助于降低成本，因为实例仅在对提供的测试数据执行预测时才运行。\n",
    "\n",
    "**注意**：将 `<LabBucketName>` 替换为在实验设置过程中创建的实验存储桶的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "#bucket='<LabBucketName>'\n",
    "prefix='flight-linear'\n",
    "train_file='flight_train.csv'\n",
    "test_file='flight_test.csv'\n",
    "validate_file='flight_validate.csv'\n",
    "whole_file='flight.csv'\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_linear_predict(test_data, estimator):\n",
    "    batch_X = test_data.iloc[:,1:];\n",
    "    batch_X_file='batch-in.csv'\n",
    "    upload_s3_csv(batch_X_file, 'batch-in', batch_X)\n",
    "\n",
    "    batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "    batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "    classifier_transformer = estimator.transformer(instance_count=1,\n",
    "                                           instance_type='ml.m4.xlarge',\n",
    "                                           strategy='MultiRecord',\n",
    "                                           assemble_with='Line',\n",
    "                                           output_path=batch_output)\n",
    "\n",
    "    classifier_transformer.transform(data=batch_input,\n",
    "                             data_type='S3Prefix',\n",
    "                             content_type='text/csv',\n",
    "                             split_type='Line')\n",
    "    \n",
    "    classifier_transformer.wait()\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "    target_predicted_df = pd.read_json(io.BytesIO(obj['Body'].read()),orient=\"records\",lines=True)\n",
    "    return test_data.iloc[:,0], target_predicted_df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "要对测试数据集运行预测，对测试数据集运行 `batch_linear_predict` 函数（先前已定义）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, target_predicted = batch_linear_predict(test, classifier_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要查看混淆矩阵的图以及各个评分指标，请创建几个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(test_labels, target_predicted):\n",
    "    matrix = confusion_matrix(test_labels, target_predicted)\n",
    "    df_confusion = pd.DataFrame(matrix)\n",
    "    colormap = sns.color_palette(\"BrBG\", 10)\n",
    "    sns.heatmap(df_confusion, annot=True, fmt='.2f', cbar=None, cmap=colormap)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def plot_roc(test_labels, target_predicted):\n",
    "    TN, FP, FN, TP = confusion_matrix(test_labels, target_predicted).ravel()\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    Sensitivity  = float(TP)/(TP+FN)*100\n",
    "    # Specificity or true negative rate\n",
    "    Specificity  = float(TN)/(TN+FP)*100\n",
    "    # Precision or positive predictive value\n",
    "    Precision = float(TP)/(TP+FP)*100\n",
    "    # Negative predictive value\n",
    "    NPV = float(TN)/(TN+FN)*100\n",
    "    # Fall out or false positive rate\n",
    "    FPR = float(FP)/(FP+TN)*100\n",
    "    # False negative rate\n",
    "    FNR = float(FN)/(TP+FN)*100\n",
    "    # False discovery rate\n",
    "    FDR = float(FP)/(TP+FP)*100\n",
    "    # Overall accuracy\n",
    "    ACC = float(TP+TN)/(TP+FP+FN+TN)*100\n",
    "\n",
    "    print(\"Sensitivity or TPR: \", Sensitivity, \"%\") \n",
    "    print( \"Specificity or TNR: \",Specificity, \"%\") \n",
    "    print(\"Precision: \",Precision, \"%\") \n",
    "    print(\"Negative Predictive Value: \",NPV, \"%\") \n",
    "    print( \"False Positive Rate: \",FPR,\"%\")\n",
    "    print(\"False Negative Rate: \",FNR, \"%\") \n",
    "    print(\"False Discovery Rate: \",FDR, \"%\" )\n",
    "    print(\"Accuracy: \",ACC, \"%\") \n",
    "\n",
    "    test_labels = test.iloc[:,0];\n",
    "    print(\"Validation AUC\", metrics.roc_auc_score(test_labels, target_predicted) )\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(test_labels, target_predicted)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # create the axis of thresholds (scores)\n",
    "    ax2 = plt.gca().twinx()\n",
    "    ax2.plot(fpr, thresholds, markeredgecolor='r',linestyle='dashed', color='r')\n",
    "    ax2.set_ylabel('Threshold',color='r')\n",
    "    ax2.set_ylim([thresholds[-1],thresholds[0]])\n",
    "    ax2.set_xlim([fpr[0],fpr[-1]])\n",
    "\n",
    "    print(plt.figure())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要绘制混淆矩阵，对批处理作业的 `test_labels` 和 `target_predicted` 数据调用 `plot_confusion_matrix` 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要打印统计数据并绘制受试者操作特征 (ROC) 曲线，对批处理作业的 `test_labels` 和 `target_predicted` 数据调用 `plot_roc` 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要考虑的关键问题：\n",
    "\n",
    "1.您的模型在测试集上的表现与训练集上的表现相比如何？ 您可以从这种比较中推断出什么？ \n",
    "2.准确率、查准率和查全率等指标结果之间有没有明显差异？ 如果是，为什么您会看到这些差异？ \n",
    "3.考虑到您的业务状况和目标，您此时最需要考虑的指标是什么？ 原因何在？\n",
    "4.您认为最重要指标的结果是否足以满足您的业务需求？ 如果不满足，您在下一次迭代中可能会更改哪些内容？ （这将在接下来的特征工程部分中进行。）\n",
    "\n",
    "用下面的单元格来回答这些问题和其他问题。根据需要插入和删除单元格。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中，写下这些问题的答案以及您可能会在本节中回答的其他类似问题的答案。记录关键细节和您所做的决策。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**问题**：您可以从混淆矩阵中总结出什么？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> 步骤 3 结束 </span>\n",
    "\n",
    "将项目文件保存到本地计算机。按照以下步骤进行操作：\n",
    "\n",
    "1.在左侧的文件资源管理器中，右键单击您要处理的笔记本。\n",
    "\n",
    "2.选择 **Download**（下载），然后将文件保存到本地。 \n",
    "\n",
    "此操作会将当前的笔记本下载到计算机上默认的下载文件夹中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代 II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 步骤 4：特征设计\n",
    "\n",
    "现在，您已经完成了一次训练和评估模型的迭代。鉴于模型第一次获得的结果可能不足以解决您的业务问题，您可以对数据进行哪些更改以改进模型的表现？\n",
    "\n",
    "### 要考虑的关键问题：\n",
    "\n",
    "1.两个主要类（*延误*与*不延误*）的平衡可能会对模型效果有怎样的影响？\n",
    "2.是否有任何相关联的特征？\n",
    "3.在此阶段，您是否可以执行可能会对模型性能产生积极影响的特征约简技术？ \n",
    "4.您能否想办法添加更多数据或数据集？\n",
    "5.执行一些特征设计之后，模型效果与第一次迭代相比如何？\n",
    "\n",
    "使用以下单元格中的内容来执行您认为可以改善模型性能的特定工程技术（以前面的问题为指导）。根据需要插入和删除单元格。\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：在项目演示中，记录您的关键决策以及在这个部分中使用的方法，并包含您在再次评估模型后获得的任何新性能指标。</span>\n",
    "\n",
    "在开始之前，思考一下为什么精确率和召回率在 80% 左右，而准确率为 99%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加更多特征：\n",
    "\n",
    "1.节假日\n",
    "2.天气"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014 到 2018 年的节假日列表是已知数据，因此您可以创建指示变量 **is_holiday** 来标记这些节假日。\n",
    "\n",
    "我们假设节假日的航班延误比平时更多。请添加一个涵盖 2014 到 2018 年的节假日的布尔变量 `is_holiday`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://www.calendarpedia.com/holidays/federal-holidays-2014.html\n",
    "\n",
    "holidays_14 = ['2014-01-01',  '2014-01-20', '2014-02-17', '2014-05-26', '2014-07-04', '2014-09-01', '2014-10-13', '2014-11-11', '2014-11-27', '2014-12-25' ] \n",
    "holidays_15 = ['2015-01-01',  '2015-01-19', '2015-02-16', '2015-05-25', '2015-06-03', '2015-07-04', '2015-09-07', '2015-10-12', '2015-11-11', '2015-11-26', '2015-12-25'] \n",
    "holidays_16 = ['2016-01-01',  '2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04', '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26']\n",
    "holidays_17 = ['2017-01-02', '2017-01-16', '2017-02-20', '2017-05-29' , '2017-07-04', '2017-09-04' ,'2017-10-09', '2017-11-10', '2017-11-23', '2017-12-25']\n",
    "holidays_18 = ['2018-01-01', '2018-01-15', '2018-02-19', '2018-05-28' , '2018-07-04', '2018-09-03' ,'2018-10-08', '2018-11-12','2018-11-22', '2018-12-25']\n",
    "holidays = holidays_14+ holidays_15+ holidays_16 + holidays_17+ holidays_18\n",
    "\n",
    "### Add indicator variable for holidays\n",
    "data_orig['is_holiday'] = # Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "天气数据获取自 https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&amp;stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&amp;dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&amp;startDate=2014-01-01&amp;endDate=2018-12-31。\n",
    "<br>\n",
    "\n",
    "该数据集包含多个城市的风速、降水量、降雪量和温度信息，按其机场代码列出。\n",
    "\n",
    "**问题**：雨、雪或大风造成的恶劣天气是否会导致航班延误？ 您现在将对此进行检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMLFO-1/flight_delay_project/data2/daily-summaries.csv /home/ec2-user/SageMaker/project/data/\n",
    "#!wget 'https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&startDate=2014-01-01&endDate=2018-12-31' -O /home/ec2-user/SageMaker/project/data/daily-summaries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将与机场代码对应的天气数据导入数据集中。使用以下气象站和机场进行分析。创建一个名为 *airport* 的新列，用于将气象站映射到机场名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('/home/ec2-user/SageMaker/project/data/daily-summaries.csv')\n",
    "station = ['USW00023174','USW00012960','USW00003017','USW00094846','USW00013874','USW00023234','USW00003927','USW00023183','USW00013881'] \n",
    "airports = ['LAX', 'IAH', 'DEN', 'ORD', 'ATL', 'SFO', 'DFW', 'PHX', 'CLT']\n",
    "\n",
    "### Map weather stations to airport code\n",
    "station_map = {s:a for s,a in zip(station, airports)}\n",
    "weather['airport'] = weather['STATION'].map(station_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 **DATE** 列中创建另一个名为 *MONTH* 的列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['MONTH'] = weather['DATE'].apply(lambda x: x.split('-')[1])\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例输出\n",
    "```\n",
    "  STATION     DATE      AWND PRCP SNOW SNWD TAVG TMAX  TMIN airport MONTH\n",
    "0 USW00023174 2014-01-01 16   0   NaN  NaN 131.0 178.0 78.0  LAX    01\n",
    "1 USW00023174 2014-01-02 22   0   NaN  NaN 159.0 256.0 100.0 LAX    01\n",
    "2 USW00023174 2014-01-03 17   0   NaN  NaN 140.0 178.0 83.0  LAX    01\n",
    "3 USW00023174 2014-01-04 18   0   NaN  NaN 136.0 183.0 100.0 LAX    01\n",
    "4 USW00023174 2014-01-05 18   0   NaN  NaN 151.0 244.0 83.0  LAX    01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `fillna()` 来分析并处理 **SNOW** 和 **SNWD** 列缺少的值。使用 `isna()` 函数来检查所有列缺少的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.SNOW.fillna(0, inplace=True)\n",
    "weather.SNWD.fillna(0, inplace=True)\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：打印 *TAVG*、*TMAX* 和 *TMIN* 缺少值的行的索引。\n",
    "\n",
    "**提示**：要查找缺少的行，请使用 `isna()` 函数。然后，在 *idx* 变量上使用该列表以获取索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([i for i in range(len(weather))])\n",
    "TAVG_idx = idx[weather.TAVG.isna()] \n",
    "TMAX_idx = # Enter your code here \n",
    "TMIN_idx = # Enter your code here \n",
    "TAVG_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例输出\n",
    "\n",
    "```\n",
    "array([ 3956,  3957,  3958,  3959,  3960,  3961,  3962,  3963,  3964,\n",
    "        3965,  3966,  3967,  3968,  3969,  3970,  3971,  3972,  3973,\n",
    "        3974,  3975,  3976,  3977,  3978,  3979,  3980,  3981,  3982,\n",
    "        3983,  3984,  3985,  4017,  4018,  4019,  4020,  4021,  4022,\n",
    "        4023,  4024,  4025,  4026,  4027,  4028,  4029,  4030,  4031,\n",
    "        4032,  4033,  4034,  4035,  4036,  4037,  4038,  4039,  4040,\n",
    "        4041,  4042,  4043,  4044,  4045,  4046,  4047, 13420])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以用特定气象站或机场的平均值来替换缺少的 *TAVG*、*TMAX* 和 *TMIN* 值。由于 *TAVG_idx* 缺少连续的行，因此无法用上个值进行替换。请使用均值进行替换。使用 `groupby` 函数来聚合采用均值的变量。\n",
    "\n",
    "**提示**：按 `MONTH` 和 `STATION` 分组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_impute = weather.groupby([<CODE>]).agg({'TAVG':'mean','TMAX':'mean', 'TMIN':'mean' }).reset_index()# Enter your code here\n",
    "weather_impute.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并均值数据与天气数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather = pd.merge(weather, weather_impute,  how='left', left_on=['MONTH','STATION'], right_on = ['MONTH','STATION'])\\\n",
    ".rename(columns = {'TAVG_y':'TAVG_AVG',\n",
    "                   'TMAX_y':'TMAX_AVG', \n",
    "                   'TMIN_y':'TMIN_AVG',\n",
    "                   'TAVG_x':'TAVG',\n",
    "                   'TMAX_x':'TMAX', \n",
    "                   'TMIN_x':'TMIN'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次检查缺少的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.TAVG[TAVG_idx] = weather.TAVG_AVG[TAVG_idx]\n",
    "weather.TMAX[TMAX_idx] = weather.TMAX_AVG[TMAX_idx]\n",
    "weather.TMIN[TMIN_idx] = weather.TMIN_AVG[TMIN_idx]\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据集中删除 `STATION,MONTH,TAVG_AVG,TMAX_AVG,TMIN_AVG,TMAX,TMIN,SNWD`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['STATION','MONTH','TAVG_AVG', 'TMAX_AVG', 'TMIN_AVG', 'TMAX' ,'TMIN', 'SNWD'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将出发地和目的地天气状况添加到数据集内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add origin weather conditions\n",
    "data_orig = pd.merge(data_orig, weather,  how='left', left_on=['FlightDate','Origin'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_O','PRCP':'PRCP_O', 'TAVG':'TAVG_O', 'SNOW': 'SNOW_O'})\\\n",
    ".drop(columns=['DATE','airport'])\n",
    "\n",
    "### Add destination weather conditions\n",
    "data_orig = pd.merge(data_orig, weather,  how='left', left_on=['FlightDate','Dest'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_D','PRCP':'PRCP_D', 'TAVG':'TAVG_D', 'SNOW': 'SNOW_D'})\\\n",
    ".drop(columns=['DATE','airport'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：在合并数据后最好先检查空值或缺失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用独热编码将分类数据转化为数值数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_orig.copy()\n",
    "data = data[['is_delay', 'Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay','is_holiday', 'AWND_O', 'PRCP_O',\n",
    "       'TAVG_O', 'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D']]\n",
    "\n",
    "\n",
    "categorical_columns  = ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', \n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'is_holiday']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(data[['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest', 'is_holiday']], drop_first=True)\n",
    "data = pd.concat([data, data_dummies], axis = 1)\n",
    "data.drop(categorical_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查新列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例输出\n",
    "\n",
    "```\n",
    "Index(['Distance', 'DepHourofDay', 'is_delay', 'AWND_O', 'PRCP_O', 'TAVG_O',\n",
    "       'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D', 'Year_2015',\n",
    "       'Year_2016', 'Year_2017', 'Year_2018', 'Quarter_2', 'Quarter_3',\n",
    "       'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6',\n",
    "       'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "       'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5',\n",
    "       'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9',\n",
    "       'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13',\n",
    "       'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17',\n",
    "       'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21',\n",
    "       'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25',\n",
    "       'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29',\n",
    "       'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3',\n",
    "       'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "       'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "       'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "       'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "       'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD',\n",
    "       'Dest_PHX', 'Dest_SFO', 'is_holiday_1'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次将 **is_delay** 列重命名为 **target**。使用与之前相同的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次创建训练集。\n",
    "\n",
    "**提示**：使用您先前定义（和使用）的函数 `split_data`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新基准分类器\n",
    "\n",
    "现在我们来看一下这些新特征是否提高了模型的预测能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LinearLearner estimator object\n",
    "classifier_estimator2 = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例代码\n",
    "\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels)) \n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               instance_count=1,\n",
    "                                               instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                               binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = classifier_estimator2.record_set(train.values[:, 1:].astype(np.float32), train.values[:, 0].astype(np.float32), channel='train')\n",
    "val_records = classifier_estimator2.record_set(validate.values[:, 1:].astype(np.float32), validate.values[:, 0].astype(np.float32), channel='validation')\n",
    "test_records = classifier_estimator2.record_set(test.values[:, 1:].astype(np.float32), test.values[:, 0].astype(np.float32), channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用刚刚创建的三个数据集训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用新训练的模型执行批量预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制混淆矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制 ROC 曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型的效果只有少许提高。我们使用 Amazon SageMaker 来尝试基于树的集成模型 *XGBoost*。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试 XGBoost 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以下步骤：  \n",
    "\n",
    "1.使用训练集变量并将其另存为 CSV 文件 train.csv、validation.csv 和 test.csv。\n",
    "2.将存储桶名称存储在变量中。Amazon S3 存储桶名称位于实验说明左侧。 \n",
    "a. `bucket = <LabBucketName>`  \n",
    "b. `prefix = 'flight-xgb'`  \n",
    "3.使用适用于 Python 的 AWS 开发工具包 (Boto3) 将模型上传到存储桶。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c130335a3301608l7992428t1w61683623926-flightbucket-i7m3o2pzzyh8'\n",
    "prefix='flight-xgb'\n",
    "train_file='flight_train.csv'\n",
    "test_file='flight_test.csv'\n",
    "validate_file='flight_validate.csv'\n",
    "whole_file='flight.csv'\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用函数 `sagemaker.s3_input` 为训练和验证数据集创建 `record_set`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role = sagemaker.get_execution_role(), \n",
    "                                    instance_count=1, \n",
    "                                    instance_type=instance_type,\n",
    "                                    output_path=s3_output_location,\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        eval_metric = \"auc\", \n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将批批量转换器用于新模型，然后使用测试数据集评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X = test.iloc[:,1:];\n",
    "batch_X_file='batch-in.csv'\n",
    "upload_s3_csv(batch_X_file, 'batch-in', batch_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgb_transformer = xgb.transformer(instance_count=1,\n",
    "                                       instance_type=instance_type,\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取预测目标和测试标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),',',names=['target'])\n",
    "test_labels = test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据定义的阈值计算预测值。\n",
    "\n",
    "**注意**：预测目标为分数，必须将其转换为二元分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_predicted.head())\n",
    "\n",
    "def binary_convert(x):\n",
    "    threshold = 0.55\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted['target'] = target_predicted['target'].apply(binary_convert)\n",
    "\n",
    "test_labels = test.iloc[:,0]\n",
    "\n",
    "print(target_predicted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为 `target_predicted` 和 `test_labels` 绘制混淆矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制 ROC 图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试不同阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：根据模型在测试集上的表现，您可以得出什么结论？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数优化 (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "### You can spin up multiple instances to do hyperparameter optimization in parallel\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker.get_execution_role(), \n",
    "                                    instance_count= 1, # make sure you have a limit set for these instances\n",
    "                                    instance_type=instance_type, \n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric='auc',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                        rate_drop=0.3,\n",
    "                        tweedie_variance_power=1.4)\n",
    "\n",
    "hyperparameter_ranges = {'alpha': ContinuousParameter(0, 1000, scaling_type='Linear'),\n",
    "                         'eta': ContinuousParameter(0.1, 0.5, scaling_type='Linear'),\n",
    "                         'min_child_weight': ContinuousParameter(3, 10, scaling_type='Linear'),\n",
    "                         'subsample': ContinuousParameter(0.5, 1),\n",
    "                         'num_round': IntegerParameter(10,150)}\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10, # Set this to 10 or above depending upon budget and available time.\n",
    "                            max_parallel_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit(inputs=data_channels)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-exclamation-triangle\" style=\"color:red\"></i> 等待训练作业完成。这可能需要 25 到 30 分钟。\n",
    "\n",
    "**要监控超参数优化作业，您需要进行以下操作**：  \n",
    "\n",
    "1.在 AWS 管理控制台的 **Services**（服务）菜单中，选择 **Amazon SageMaker**。 \n",
    "2.单击 **Training > Hyperparameter tuning jobs**（训练 >超参数优化作业）。\n",
    "3.您可以查看每个超参数优化作业的状态、目标指标值和日志。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查作业是否成功完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参数优化作业将产生效果最佳的模型。您可以从优化作业中获取有关该模型的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_client = boto3.Session().client('sagemaker')\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "print(f'tuning job name:{tuning_job_name}')\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "best_training_job = tuning_job_result['BestTrainingJob']\n",
    "best_training_job_name = best_training_job['TrainingJobName']\n",
    "print(f\"best training job: {best_training_job_name}\")\n",
    "\n",
    "best_estimator = tuner.best_estimator()\n",
    "\n",
    "tuner_df = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()\n",
    "tuner_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用估算器 `best_estimator` 并使用数据对其进行训练。\n",
    "\n",
    "**提示**：请参阅之前的 XGBoost 估算器拟合函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将批批量转换器用于新模型，然后使用测试数据集评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = \"s3://{}/{}/batch-out/\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/batch-in/{}\".format(bucket,prefix,batch_X_file)\n",
    "\n",
    "xgb_transformer = best_estimator.transformer(instance_count=1,\n",
    "                                       instance_type=instance_type,\n",
    "                                       strategy='MultiRecord',\n",
    "                                       assemble_with='Line',\n",
    "                                       output_path=batch_output)\n",
    "\n",
    "xgb_transformer.transform(data=batch_input,\n",
    "                         data_type='S3Prefix',\n",
    "                         content_type='text/csv',\n",
    "                         split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/batch-out/{}\".format(prefix,'batch-in.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),',',names=['target'])\n",
    "test_labels = test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取预测目标和测试标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_predicted.head())\n",
    "\n",
    "def binary_convert(x):\n",
    "    threshold = 0.55\n",
    "    if x > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "target_predicted['target'] = target_predicted['target'].apply(binary_convert)\n",
    "\n",
    "test_labels = test.iloc[:,0]\n",
    "\n",
    "print(target_predicted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为 `target_predicted` 和 `test_labels` 绘制混淆矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制 ROC 图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题**：尝试不同的超参数和超参数范围。这些更改是否改善了模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "现在，您已经至少对模型进行了几次训练和评估。现在到了结束该项目并反思以下问题的时候：\n",
    "\n",
    "- 您学到了什么？ \n",
    "- 接下来您可能会采取哪些类型的步骤（假设您有更多时间）\n",
    "\n",
    "使用下面的单元格回答其中一些问题和其他相关问题：\n",
    "\n",
    "1.您的模型性能是否能实现您的业务目标？ 如果不符合，而且您还有时间进行优化，那么您会采取哪些不同的做法？\n",
    "2.在您对数据集、特征和超参数做出更改之后，模型有多大程度的改进？ 在整个项目中，您采用了哪些方法对模型进行改进？效果最佳的是哪个？\n",
    "3.在整个项目中，您遇到的最大挑战是什么？\n",
    "4.关于管道的各个方面，有哪些没有解决的问题对您来说是毫无意义的？\n",
    "5.在进行这个项目的过程中，关于机器学习，您学到的三件最重要的事情是什么？\n",
    "\n",
    "#### <span style=\"color: blue;\">项目演示：确保在项目演示中同样总结这些问题的答案。现在，将项目演示期间的所有笔记整理在一起，准备向全班展示您的成果。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your answers here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
